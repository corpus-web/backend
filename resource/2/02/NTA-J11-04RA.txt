<title>TENDL: Complete Nuclear Data Library for Innovative Nuclear Science and Technology</title>
<Affiliation>1.Nuclear Data Section, International Atomic Energy Agency, P.O. Box 100, 1400 Vienna, Austria
2.Department of Physics and Astronomy, Uppsala University, Uppsala, Sweden
3Laboratory for Reactor Physics Systems Behaviour, Paul Scherrer Institut, Villigen, Switzerland
4.NRG, Westerduinweg 3, 1755 LE Petten, Netherlands
5.Taras Shevchenko National University of Kyiv, Kyiv, Ukraine
6.Nuclear Energy Agency, OECD, 92100 Boulogne-Billancourt, France
7.United Kingdom Atomic Energy Authority, Culham Science Centre, Abingdon OX14 3DB, United Kingdom
</Affiliation>
<author>
A.J. Koning,1, 2, ∗ D. Rochman,3 J.-Ch. Sublet,1 N. Dzysiuk,4, 5 M. Fleming,6, 7 and S. van der Marck4</author>
<year>2018</year>
<Jounral>NUCLEAR DATA SHEETS</Journal>
<Publishing_house> Elsevier Inc.</Publishing_house>
<Text_Collector>田苗，BFSU</Text_Collector>
<DOI>10.1016/j.nds.2019.01.002</DOI>
<URL>https://doi.org/10.1016/j.nds.2019.01.002</URL>
The TENDL library is now established as one of the major nuclear data libraries in the world, striving for completeness and quality of nuclear data ﬁles for all isotopes, evaluation methods, pro- cessing and applied performance. To reach this status, some basic principles have been applied which sets it apart from other libraries: reproducible dedicated evaluations when diﬀerential data are available, through determination of nuclear models implemented in TALYS and their parameters, completeness (with or without experimental data), format and processing standardization, automa- tion of production and reproducibility. In this paper, we will outline how such an approach has become a reality, and recall some of the past successes since the ﬁrst TENDL release in 2008. Next, we will demonstrate the performance of the latest TENDL releases for diﬀerent application ﬁelds, as well as new approaches for uncertainty quantiﬁcation based on Bayesian inference methods and possible diﬀerential and integral adjustments. Also, current limitations of the library performances due to modelling and needs for new and more precise experimental data will be outlined.
I.INTRODUCTION
The ﬁeld which we now call ‘nuclear data’ was already present in the 1950’s and 1960’s when the development of nuclear technology required a systematic use of basic nuclear scientiﬁc information. Information from nuclear reaction and structure experiments were stored in numer- ical databases for preservation of knowledge and foremost for use in applications. These databases were fed into the ﬁrst prototypes of computer codes which, subject to se- vere memory and speed limitations, managed to simulate the behavior of several nuclear devices at a basic but es- sential level.
Jumping straight to 2019, we can certainly say that the nuclear data ﬁeld has matured. Large experimen-  tal databases like EXFOR [1] for reactions and XUNDL for structure reach an impressive degree of completeness of compilation with respect to historical and new mea- surements. Nuclear simulation codes have the latest nu- clear models implemented and challenge the current day computer power, and the amount of evaluated, ENSDF, ENDF data in libraries and processed nuclear data li- braries is entering the Gb range. After its trendsetting role in the 1960’s, the nuclear data ﬁeld has been passed by other organizations, companies and industries when it comes to IT-related development. This is probably due to the decreasing popularity of, and ﬁnancial input in, nu- clear data evaluation. This lagging behind relates among others to the most used data format, ENDF-6, which in- deed still stems from the 1960’s and must look totally archaic to newcomers. Fortunately, there are now initia- tives such as the Generalized Nuclear Data (GND) format
[2]to modernize this situation. The TENDL approach outlined in this paper relies on a ﬁrm control over infor- mation and knowledge in the whole nuclear data chain. Information management throws light on data and infor- mation, while knowledge management does upon knowl- edge, understanding and wisdom. Thereby, it is impor- tant to not only store and use information but also knowl- edge and to make optimal use of what we and our pre- decessors have measured, invented, evaluated and devel- oped, without reinventing the wheel every time.
The information management side of nuclear data is relatively well-developed. (Well-)established databases are available, or processed into, ready to use forms. In the nuclear data world, one would categorize the EX- FOR data compilation as information, while its inter- pretation, or evaluation (which so far has not yet been recorded in a systematic way), would rank as knowledge. Similarly, on the theoretical side, various nuclear models and their parameters have been accepted by the commu- nity as the current state-of-the-art until eventually some better model comes along. That means that the use of these models should basically be trivial; after the research stage comes the implementation and production stage, and these two phases should be strictly separated. The TALYS code [3] aims to fulﬁll this goal: several years after the research stage of theoretical nuclear reaction mod-elling, a particular model should be so well established and accepted that their should be no discussion about its implementation in software. Again knowledge has been transformed into (the production of) information in the sense of validated, frozen software. Mixing research, de- velopment and production usually leads to failure, which is why for example a new version of TALYS is only re- leased once per two years.
The knowledge management side of nuclear data is quite.   Each component of the nuclear data chain has  its own experts who know best the details, and which available information is more right or more wrong. This knowledge is should actually be quantiﬁed, in the sense that judgment about particular experimental data sets and the performance of nuclear models in certain energy and mass ranges, together with their parameters would be safely stored in ‘expert databases’, after which the standard automated ﬂow of software and data can pro- duce the next version of the data library. In short, the key to eﬃcient, high-quality nuclear data evaluation is quantifying knowledge of the experts.
The aim of the system built around TALYS is to im- plement this information and knowledge ﬂow as much as possible. The TENDL library is merely one output   of the system built around TALYS. This should not be read as a case of false modesty; the rapidly increasing use of TENDL worldwide is something to be proud of, but retrospectively we conclude that it is nothing more than pouring the output of this software and knowledge sys- tem into a speciﬁc format, albeit a complicated one. Also it should be noted here that parts of TENDL still come from other existing libraries, since these parts clearly out- perform current TALYS capabilities. For example, this is the case for the major actinides which are adopted from the ENDF/B-VIII data library [4]. The production of ENDF-6 formatted TENDL ﬁles opens up many possi- bilities for simulation of integral processes, as shown in this paper. Although the emphasis in this paper will be on the TENDL library, the systematic and automated approach built around TALYS gives rise to possibilities which can not easily be found elsewhere, for example
Uncertainty propagation with Total  Monte  Carlo  [5]: varying experimental data, resonance and nu- clear model parameters inside their uncertainty margins at the beginning of the entire chain of soft- ware and databases for  nuclear  technology  results in exact probability  distributions  and  correlations  of technologically relevant parameters, without the necessity to drag along (processed) covariance ma- trices throughout the chain. Instead, one has a sta- tistical ensemble of ENDF-6 and processed libraries
Complete veriﬁcation and validation of the EXFOR database [6] to a certain global depth of detail.
Large scale comparisons of diﬀerent nuclear models by comparing the relative ratio of the results for a wide range of nuclides.
Complete libraries with astrophysical reaction rates [7], medical isotope production data,
Worldwide use of TALYS and its satellite software, for basic nuclear science and applications, etc.
and it is to be expected that the availability of such a system gives rise to ’sudden’ new ideas (e.g., visualiza- tion possibilities, useful computer Apps, etc.), analogous to the new concepts in the above list which emerged un-
planned and only a few years apart. Each of the topics above probably deserves, or is already outlined in, a sep- arate publication. In this paper we will focus on the TENDL nuclear data library, including the physics be- hind it, its uncertainty quantiﬁcation, and applied use.
A.General Philosophy of TENDL
The ﬁrst versions of the various nuclear data libraries in the world contained evaluations which had only speciﬁed parts of the libraries ﬁlled with data, often those reac- tion channels for which experimental data existed and/or those parts which were deemed important for applications (otherwise diﬀerential measurements would probably not exist). Not so long ago, a detailed look into a nuclear data library gave the impression that the generally adopted point of view was “If it has not been measured, it does not exist”, since only speciﬁc parts of a nuclear data library were ﬁlled. Thanks to the development of nuclear mod- els, in combination with demands from users which could not aﬀord gaps in their nuclear data tables, this point of view is slowly but surely being abandoned by all major nuclear data libraries. TENDL takes a rather extreme point of view here: every nuclear reaction process which is expected to take place in reality should be present in a nuclear data library, measured or not measured. In short, that means, all projectiles, all nuclides, all reaction channels and all energies. This entails a heavy reliance on nuclear modeling with predictive power for all reac- tion channels and secondary distributions, and a globally reliable approach for uncertainty quantiﬁcation. Where possible, and preferably, simulated results should be over- ruled by high-quality experimental data, either directly, or by TALYS calculations with parameters that have been adjusted to match the experimental data. As detailed in this paper, this approach of completeness, with as much quality as possible, is applied to both the resonance range and the fast energy range.
B.Current TENDL Use Worldwide
In Fig. 1, the worldwide use of the TENDL libraries in the past decade is depicted, while Fig. 2 shows a, some- what arbitrary, distribution over ﬁelds of application.
A welcome observation is that in more and more nu- clear analyses, both with regards to diﬀerential data and applications, TENDL is now compared with other world data libraries with respect  to  its  performance. This  means that TENDL is beyond the stage of  only  being  used when ”there is nothing else available”,  although it     is perhaps hard to believe that a complete nuclear data library can also deliver quality on an individual nuclide basis. In this paper, we will show why use of TENDL for individual nuclide-by-nuclide analyses is justiﬁed. Nev- ertheless, Fig. 2 shows that the use of TENDL is domi- nated by non-criticality analyses for applications as non- proliferation,   activation,   decay   heat,   radiation damage,
etc. which require neutron data that is generally not available in other libraries, and applications with charged
particles and photons for which there is often nothing else available.
FIG. 1. Time-dependence of TENDL citations, from the ﬁrst library release in 2008.
FIG. 2. (Color online) Segmentation of TENDL publications
by application area, over the years 2008-2017.
C.Conceptual Diﬀerences with Other World Libraries
The production process of TENDL is diﬀerent from all other nuclear data libraries in the world. In both ap- proaches the evaluation eﬀort is shared between people. Only for TENDL this happens at a level deeper than that of ﬁlling the ENDF library with data, namely with fun- damental input ﬁles for the evaluation: Databases rela- tive to EXFOR, containing pointers to include or exclude experimental data, a ’best’ TALYS input ﬁle for each target nuclide, a speciﬁc chosen resonance parameter set and a ﬁle with speciﬁc, better data from other world li- braries. The entire library is produced at once,  while  for other nuclear data libraries diﬀerent evaluators de- liver isotopic ENDF-6 data ﬁles at diﬀerent times, using diﬀerent ENDF-6 procedures, at one central place. This leads to a rather fundamental question: Can an evaluated isotopic ﬁle in TENDL perform better than that of other world libraries, knowing that in TENDL the evaluation eﬀort has to be more equally shared over all nuclides, while in the approach of other world libraries one invests more expert knowledge in that particular isotope but one does not make use of this eﬃcient system which produces a very complete and consistent ﬁle? The current paper aims to give some answers to that. Important to remem- ber is that for all other world libraries an ’evaluation’ is equal to an ENDF-6 formatted ﬁle, since knowledge has
gone straight into the latter which is not reproducible anymore. Hence, we like to say that e.g., ENDF/B-VIII has 556 evaluations (or isotopic data ﬁles), while TENDL-
2017 has 2813 isotopic data ﬁles.
D.This Paper
This paper is organized as follows. In Section II, we will start with an outline of the low-energy reaction part of the system and describe the treatment of the resolved and unresolved resonance ranges.  In Section III, the fast
and high energies will be covered, and this part is dom- inated by the TALYS code. A brief review of the used nuclear models and parameters will be given. Section IV contains a description of the methods used to assign un- certainties to the nuclear data. In Section V the entire software system, ’T6’, to produce the TENDL libraries is described, together with an explanation of the method to come to best results from a diﬀerential data point of view, and a description of all TENDL sublibraries. Section VI deals with nuclear data adjustment to both diﬀerential and integral data. In Section VII the processing codes and routes for the TENDL libraries are described, while the subsequent benchmarking is outlined in Section VIII. Section IX gives an overview of the most important appli- cations of TENDL that we are aware of. Finally, Section X is devoted to the future of TENDL and conclusions.
II.RESOLVED AND UNRESOLVED RESONANCES
The resolved and unresolved resonance range (later called RRR and URR, respectively) is of major impor- tance for the correct simulation of all systems heavily relying on thermal reactions. In the following, we will describe the speciﬁc approach taken in the TENDL li- brary.
Our goal is to provide the users with a set of resonance parameters for all isotopes included in TENDL (neutron sublibrary).   As  easily  understood,  the  origin  of these
parameters cannot be the same for e.g., 73As or 75As which are both given in TENDL (one is short lived  with
80 days half-life, the other one is stable). This is why we can distinguish a few categories:
1.isotopes with measured resonance parameters and measured “integral” values. Here, integral values can be thermal cross sections, capture integrals or
Maxwellian averaged cross sections,
2.isotopes with “integral” values only,
3.isotopes without any experimental information.
Examples for the two mentioned isotopes are presented in Fig. 3.  As seen on these ﬁgures, both isotopes seem to
possess “evaluated” resolved resonances. If looked more into  details,   the  75As  isotopes  have   very  similar  reso-
nances between the two presented libraries. Given that 75As is the only stable isotope of Arsenic, these reso- nances are certainly derived from transmission and cap- ture measurements at time-of-ﬂight facilities. In the case of 73As, one can see that the resonances are very diﬀer- ent between the two libraries.  In the case of ENDF/B-
VIII.0 [4], they even follow a regular pattern, derived from a ﬁxed D0 (averaged spacing between resonances). This indicates that these resonances are not measured, but created to provide a resonance aspect to the cross section, possibly scaled to reproduced a thermal value. In the case of TENDL-2017, even if the resonances ap- pear to be real, they are also derived from systematics and statistical patterns. The  way  to obtain them will   be presented below. These two examples are representa- tive of the global approach often taken in a library, and systematically in TENDL.
Additionally, the reality is a bit more complicated, since for some isotopes, only a few resonances are re- ported, and it is known that an important amount is miss- ing. Additionally, TENDL is also provided with covari- ance information. In the resonance range, the evaluation of the covariances entails some particularities compared to the fast neutron range, as the cross sections strongly vary in small energy steps. These aspects will be pre- sented in more details in the following sections, hopefully providing TENDL users with the necessary information to build up conﬁdence in the presented data.
A.
Source of Resonance Parameters
As presented above, the origin of the resonance pa- rameters in the TENDL libraries vary, depending on the degree of knowledge for the given isotope. In any case,  a resolved resonance region is always given. In the pro- duction of TENDL, the resonance range is handled by a speciﬁc code called TARES.
1.A Resonance Compiler Code: TARES
TARES is a program to obtain cross section informa- tion in the resonance range, solely used for the production of resonance parameters in the context of the TENDL li- brary. Its primary goal was to be connected with the soft- ware TALYS [8] to complete the low energy region. As TARES is used in the production of the TENDL library, it is part of the “T6” package, which will be described later. Since then, TARES is used in the production of the TENDL libraries and in the production of the so-called “random ENDF ﬁles”, used to propagate nuclear data uncertainties following the “Total Monte Carlo” (TMC) approach [5]. It does not only perform original resonance analysis, but TARES is also using other codes such as SAMMY [9] for the ﬁtting of pointwise cross sections, CALENDF [10] for the generation of ladder of resonances and PREPRO [11] for the reconstruction of pointwise res- onances. One of the particularity of TARES is to success- fully linked all these codes for all isotopes of interest in TENDL.
It can be used from Z = 3 to Z = 118 to
produce resonance parameters in the  ENDF  for-  mat (called MF2 or ﬁle 2). Mainly three formats   are used: multi-level Breit Wigner (for actinides), Reich-Moore (for some non actinides) and LRF7  (for the majority of non actinides),
produce resonance parameter uncertainties in the ENDF format (called MF32),
produce cross section uncertainties in the re-  solved resonance range in the ENDF format (called MF33),
produce random resonance parameters based on pa- rameter uncertainties,
reconstruct pointwise cross sections based on the resonance parameters in tabulated x-y format,
reconstruct pointwise cross sections based on the resonance parameters in ENDF format (called MF3),
produce grouped cross sections based on the reso- nance parameters in tabulated x-y format,
apply the retroactive method to update the reso- nance parameter covariances (using SAMMY),
FIG. 3. (Color online) Resonance range for 73As (top) and 75As (bottom) from TENDL-2017 and ENDF/B-VIII.0.
and produce angular distributions in the resonance range (also using SAMMY, or more precisely SAM- RML).
Details of the application of TARES for TENDL will be developed in the following sections.
2.Stable and Long-lived Isotopes
In the case of stable and long-lived isotopes, the reso- nance parameters are taken either from existing libraries, or from compilations such as the Atlas of Neutron Reso- nances [12]. In these cases, a complete evaluation in the sense of performing a SAMMY [9] or REFIT [13] analysis is not done. Such approach is justiﬁed for isotopes heav- ier than A	19, including actinides. These parameters, together with a speciﬁc formalism approximation will de- ﬁne what enters in the so-called “ﬁle 2” (also called MF2) of the ENDF ﬁle. For the formalism, only two are used for the resolved resonance range (Reich-Moore or Multi- level Breit-Wigner). For the unresolved resonance range, the same procedure is followed: if a URR domain exists in another library, it is adopted in TENDL. If none are given, we either also do not provide a URR, or we propose one coming from TALYS calculations (see next section). In some cases, the calculated thermal cross section from the resonance contribution does not correspond to the measured thermal value. In this case, the missing cross sections are added by using negative resonances. In TENDL, we are using an automatic procedure to calcu- late the resonance parameters of the negative resonances in order to match a speciﬁc thermal value. Very good re- sults for the elastic, total and capture cross sections can be obtained, but it is not always successful in the case of actinides, where manual adjustments need to be done. Still, once the resonance parameters are found, they are stored in a simple table, ready for the next use of TARES. In the case of light elements, the MF2 is often not used, but the pointwise cross sections are given in the “ﬁle 3”, of MF3. In TENDL, we do not have for the time being the capability to perform our own evaluations of these im- portant isotopes. A very pragmatic solution is followed: simply adopting a full evaluated ENDF ﬁle from other li- braries. This is for instance the case for 1H, 16O. This be- ing said, for light isotopes not provided in other libraries, we believe that it is (still) worth providing some values in the shape of MF2, based on resonance compilations, or on the High Fidelity Resonances approach (HFR, see
next section). This is the case for instance for 16N.
In parallel, the estimation of the uncertainties and cor- relations are also done taking into account the experimen- tal, or compiled uncertainties for thermal cross sections and resonance integrals.
3.
Isotopes with Signiﬁcant Missing Resonances
For some isotopes, some relatively large ranges of en- ergy are not covered by measured resonances, even at low energy. This is for instance the case of 106Cd, or 108Cd. For such isotopes, two possibilities exist: either ignore the missing resonances (resulting in a strong decrease of the cross section), or ﬁlling the gap with statistical reso- nances. The origin of these statistical resonances is pre- sented in the HFR section, but an example for 106Cd(n,γ) is presented in Fig. 4. One can see the impact of the missing resonances for a speciﬁc energy range. One of the drawbacks of creating statistical resonances is to pro- vide the impression that there is no need of additional measurements. It is therefore important to understand the origin of such resonances and to ﬂag them as being calculated and not measured.
4.Shorter-lived Isotopes
Continuing towards the path of missing more and more information for the thermal and resonance range, it is very common that resonances of short-lived isotopes are not known. In many cases, their thermal cross sections are also not known. In general, such isotopes are not im- portant for reactor applications, but can become relevant for short-term processes (such as the calculation of decay heat up to minutes after the shutdown of a reactor, or in environments with very high neutron ﬂux [14]). The way to deal with such isotopes in TENDL is explained in the HFR section, with the guiding idea that all isotopes have a resolved resonance range in TENDL.
B.High-Fidelity Resonances with CALENDF
The global methodology of the High Fidelity Res- onance approach (or HFR)  is  presented  in  details  in Ref. [15]. An overview will be presented here with the speciﬁc link to TENDL.
As mentioned before, every single isotope in TENDL has a resolved resonance range. For missing resonances, these RRR are populated by statistical resonances with the HFR method. As in the case of TALYS, there no manual intervention required in the case of TARES and the HFR. TARES already contains the necessary infor- mation to provide statistical resonances for each single isotope. This procedure is used each time there is no  (or missing) resonances, and each time a random MF2 is needed. A schematic view of the whole process is given in Fig. 2 of Ref. [15].
1. Prior to the HFR: Smooth, Radiator and the SRA
Filling the resonance range with calculated resonances is not new. The ﬁrst solution consists in extending the
FIG. 4. (Color online) Example of the “ﬁlling” of missing resonances for 106Cd(n,γ): comparison between TENDL-2017 and ENDF/B-VIII.0.
optical model calculation from a few hundreds of keV down to 0.01 eV, thus obtaining smooth cross sections. Obviously no resonance structure can be obtained, but such  calculations  could  be adjusted  in  order  to repro-
duce a measured thermal value (e.g., by adjusting optical model and level density parameters). An example of such
method is presented in Fig. 5 for the 90Sr(n,γ) cross sec- tion.  In this example, one can clearly see the drawback of this method: there is a non-physical discontinuity be- tween the resonance and fast-neutron ranges, due to the diﬀerent adjustments made in these two energy ranges.
Another simple approach to mimic resonance behavior is to create one resonance at every D0, D0 being the s- wave average level spacing (see for instance Ref. [16]). In this case, resonances appear on a regular basis and were said to follow a “radiator behaviour”. This method was applied in the ﬁrst releases of the TENDL libraries. A short description is given here, as it helps to understand the evolution towards the HFR.
For radiator-like resonances, the following assumptions
were made for the six resonance parameters (l, j, E, Γn, Γγ and possibly Γf ). The l value is assumed to be zero, and the spin of resonances j is the spin of the target nucleus plus 0.5. The four other parameters are changing with the resonance numbers i. The resonance energy Ei is equal to
@1
with D0 the s-wave average level spacing. For the neutron width Γn, according to the extreme compound, or black, nucleus model [17] the strength function is constant for all nuclei, and for s-wave neutrons is given by
@2
where   Γ	is the average s-wave reduced neutron width,is the average s-wave level spacing, k0 is the wave
number for a 1 eV neutron while K is the wave number inside the nucleus. For a potential well depth of 42 MeV, the black nucleus value of the strength function is 1 10−4 (Note that the strength function is a dimensionless quan- tity). It is then assumed that
@3
with the penetrability VA = 1 for s-waves (l = 0). We can then extract the neutron widths for the ith resonance being:@4
FIG. 5. (Color online) Example of smooth and resonance cross sections for 90Sr(n,γ): comparison between TENDL-2017 and ENDF/B-VIII.0.
with S0 the strength function for s-wave  resonances.  As an example, the case of 205Pb is presented in Fig. 6 with  20 hypothetical resonance levels added to the bound level to simulate the elastic and capture cross  sections  up  to 100 keV [16]. It was assumed a s-wave strength function  S0 of 10−4, an average level spacing D0 of 5 keV, a radia- tive width Γγ of 10 eV and a scattering radius of 8.7 fm. The ﬁssion width is simply related to the capture width with the following formula [12]:
@5
In this approach, the shape of the resonances is also not physical as it can be seen in Fig. 6. A reﬁnement can be found in the Single Resonance Approach (SRA), as orig-inally presented in Refs. [18, 19]. The idea was to use a unique resonance plus a 1/v background to reproduce a known thermal cross section. One of the advantages is to be able to reproduce thermal values as well as resonance integrals when no other information are available. If no experimental values are known, systematics are applied as also presented in Ref. [16]. An example of such capture cross section is presented in Fig. 7 for 90Sr(n,γ). One can see that the SRA represents an improvement for the RRR representation compared to a smooth cross section. Still,
FIG. 6. (Color online) Example of radiator-like resonances for
205Pb, see Ref. [16] for details.
a strong and non physical abrupt step appears between the resonance treatment and the fast neutron range. Ad- ditionally, the SRA does not address how to provide an
unresolved resonance range, if needed.
2. Next Step: the HFR
The HFR approach had as an original goal to create    a link between the resolved resonance range and the fast neutron range. In the absence of experimental data in the resonance range, the idea is to extend the optical model calculation virtually down to 0 eV, to extract energy- dependent average parameters and to use these parame- ters to produce statistical resonances based on the ladder approach. Details are given below.
Such approach can be applied to isotopes without any experimental information, to isotopes with known ther- mal cross sections only, and also to isotopes with known
resonances  up  to  a  certain  energy.  In  the  last  case,   the HFR would simply mean to “reconstruct” the URR into  resolved  resonances  (see  a  good  example  for 238U
in Ref. [20]). A set of parameters are needed with their energy-dependent variations:
•scattering radius r,
•average level spacing D0,
•average reduced neutron width Γ0 ,
•average radiation width Γγ ,
•and if needed the average ﬁssion width Γf .
As explained, these parameters are obtained by extending the necessary model calculations at low energy (optical mode. This is realized by using TALYS and the so-called “urr.dat” output ﬁle which contains the necessary infor-
mation on a speciﬁc energy grid. In the TENDL libraries, the default optical model is the Koning-Delaroche [21], either local or global, depending on the isotopes (with some exceptions such as the stable Hf isotopes and some actinides). Therefore, the above parameters are simple derived from this optical model. An outline of the stan- dard options of the optical model potential, level densities and photon strength function is given in the next Section on TALYS.
Once these parameters are available,  they are given as input for the generation of random ladders of reso-
nances, using the statistical properties of the unresolved resonance range. As presented in Ref. [16], “ladders can be generated at an energy E by randomly selecting a start- ing resonance energy for one (l, j) sequence, and also ran-
domly selecting a set of widths for that resonance using the appropriate average widths and χ2 distribution func- tions. The next higher resonance energy can be  selected by sampling from the Wigner distribution for resonance spacings, and a new set of widths for that resonance can be chosen. The process is continued until a long ladder of resonances for that (l, j) is obtained. The process for the other (l, j) sequences is then repeated, each such sequence
being uncorrelated in positions from the others.” In prac- tice, the CALENDF code [10] is used inside TARES, ap- plying  a  Gaussian  Orthogonal  Ensemble  random matrix
for each (l,j) couple to generate random resonance ener- gies [22]. The advantage of using CALENDF compared to NJOY for the resonance generation is that CALENDF follows the Wigner law and to include correlations be- tween successive resonances. Energy “segments” are de- termined in CALENDF with a few tens of s-wave res- onances in each of them. Based on stratiﬁed random numbers, the widths of the resonances are also obtained. One needs to realize that the resonances created with this method are not real but their parameters are derived from speciﬁc distributions, in agreement with the optical model used at higher energy. If the seed of the random number generator is changing,  diﬀerent resonances  will
be obtained for a speciﬁc isotope.
Examples of these statistical resonances can be found in the recent TENDL libraries and the case of 90Sr(n,γ) is also presented in Fig. 5. The impact of the HFR com- pared to the normal Hauser-Feshbach (HF) calculations have been extensively presented in Ref. [14], where it is demonstrated that the deviation between HF and HFR calculations is increasing when going further away from the valley of stability. In Ref. [14], a speciﬁc choice of level density model and gamma-strength function was made (Back-shifted Fermi gas model and Hartree-Fock BCS tables). Many other selections can be used to generate statistical resonances. In Fig. 8,  the ratios of HFR/HF in terms of Maxwellian Average Cross Section (MACS) are presented for a diﬀerent choice: using Hartree-Fock- Bogolyubov tables for the gamma-strength functions and Microscopic level densities (Skyrme force) from Hilaire’s combinatorial tables. The results presented here are very similar to those of Ref. [14]: the HFR leads to higher resonant captures compared to the HF model when the nuclear level densities are increasing. This indicates the importance of using cross sections having a resonance be- haviour compared to smooth cross sections. In TENDL, the HFR is applied for all, either completely or partially missing from experiment, resolved and unresolved reso- nance ranges, for all nuclides.
C.Retroactive SAMMY Fitting
The retroactive method helps to analyse existing reso- nance parameters, without using the original experimen- tal data (for instance from transmission measurements). It is therefore an approximate method and cannot replace a complete data study, as it is usually done for resonance analysis. With a certain number of assumptions, one can use the SAMMY ﬁtting program [9] to obtain covariance matrices for the resonance parameters, or for the recon- structed cross sections. With the knowledge on the ther- mal cross sections (and uncertainties) and possibly other integral data (resonance integral, MACS), one can make sure that the reconstructed cross sections and uncertain-FIG. 7. (Color online) Same as Fig. 5 but with EAF-2010 for the Single Resonance Approach.
ties will match the thermal points. Such procedure is used in TARES for the production of the resonance pa- rameters and their covariances.
Recently (since TENDL-2017), some features of SAMMY are also used to produce a set of cross section co- variances (MF33) instead of resonance parameter covari- ances. Such covariance matrices for group cross sections in the resonance range are also merged with the MF33 for the fast neutron range. Therefore,  only MF33 from  0 to 200 MeV is now provided in TENDL. In Ref. [23]  it was shown that it is better for some applications to use the MF32 covariance ﬁle regarding self-shielding ef- fects, but from a user perspective the processing of MF33 can be performed without speciﬁc knowledge (such solu- tion is also accepted in the ENDF/B-VIII.0 library [4]). Examples for the use of the retroactive method can be
found in the SAMMY manual (e.g., sample case t097k). The retroactive method is applied as follows in the case of TENDL:
Existing resonance parameters are provided to SAMMY for cross section reconstruction,
artiﬁcial uncertainties are given to the recon- structed cross sections (relatively small value, in the order of percent),
•a new ﬁtting is performed, thus providing new res-
onance parameters (almost equal to the previous ones) with an energy-energy correlation matrix and a resonance parameter correlation matrix,
the uncertainties for the reconstructed group cross sections can artiﬁcially be adjusted to match ther- mal values.
Following these steps, new resonance parameter ENDF ﬁles can be obtained: for the parameters (in diﬀerent for- malism, multi-level Breit Wigner, Reich Moore or limited LRF7), and the uncertainties (for parameters as well as for group cross sections).
This procedure is applied for the majority of isotopes in TENDL, but not for the main ones such as 235,238U or
239Pu.
III.TALYS
The nuclear model code TALYS[3] is designed to anal- yse and predict nuclear reactions. The physical models implemented in TALYS have been validated for the simu- lation of nuclear reactions that involve neutrons, photons, protons, deuterons, tritons, 3He- and alpha-particles, in the keV - 200 MeV energy range and for target nuclides of mass 19 and heavier. Formally, the ranges are even more
FIG. 8. (Color online) (N, Z) plane for the ratios of the HFR over the HF MACS at 30keV for about 3300 nuclei between Li and Bi lying between the valley of β-stability and the neu- tron drip line. A diﬀerent choice of level density and gamma- strength function was made compared to a similar ﬁgure pre- sented in Ref. [14]. Colors indicates the order of magnitude of the HFR/HF ratio for each isotope.extensive than this: Using external resonance informa- tion the code now produces meaningful results already  in the meV range, and the code can even be used for targets as light as 6Li and up to incident energies of 1 GeV. Note however that the optical and statistical mod- els implemented in TALYS are known to break down for nuclides with masses below about 20. Also, above sev- eral hundreds of MeV the omission of pion production the absence of a sound model for gama-ray transitions at thermal energies, and explicit treatment of break-up reactions such as (d,p) to discrete states,  and the lat-  est models for quantum-mechanical pre-equilibrium emis- sion. The code aims at quality and completeness at the same time: Sophisticated nuclear reaction models have been implemented,  using a vast  range of  microscopical nuclear structure ingredients, but at the same time al- lows to add enough phenomenology to always produce desired answers.
TALYS has been extensively validated over the past 20 years in ﬁelds ranging from fundamental nuclear reac- tion studies to data evaluation for nuclear reactors, med- ical isotope production and astrophysics, among others. This validation was not only performed by the authors but also by thousands of users who often do not hesitate to report both positive and negative feedback, including bugs, to the authors which has made this open source code very robust. Figure 9 shows the number of papers per year that make use of TALYS. Obviously, at the time of printing the current paper, the numbers for 2018 are not deﬁnite yet, but we can state that currently on aver- age about 2 papers per day are released that use TALYS in some way, and that the total number of papers citing TALYS is now beyond 5000. An extensive description of the various applications of TALYS can be found in [8]. Fig. 10 gives a breakdown per application.
FIG. 9. Time-dependence of TALYS citations, from the ﬁrst beta release in 2001.
FIG. 10. (Color online) Segmentation of TALYS publications by application area, over the years 2001-2017.
Since all models used for the nuclear reaction mech- anisms implemented  in  TALYS  are  limited, it  can  not be expected that TALYS can ab initio reproduce high-quality experimental data, and therefore adjustable model parameters are needed. More than 400 diﬀer-   ent keywords are available, though not all for adjustable model parameters but also for model and output options
etc., to let TALYS do what the user desires. And even with that, reproduction of experimental data may not al- ways be successful, if the nuclear model is “too wrong”. This holds especially for actinides and the modeling of the ﬁssion channel, for which it is still diﬃcult to use TALYS as a routine production tool. An advantage of TALYS is that it can generate data for any reaction channel regard- less of whether it has been measured or not. The quality of the ﬁnal nuclear reaction simulation is then represented by the ability of TALYS to match, interpolate between, and extrapolate beyond the existing experimental data.
TALYS can be used for many purposes, not only nu- clear data evaluation. For completeness, we here list the TALYS output that is required for the production of the TENDL libraries. As output, for a single projectile + tar- get combination and a range of incident energies, TALYS produces cross sections (assuming incident neutrons here) for:
As detailed elsewhere in this paper, the only output that TALYS does not (yet) provide, and which makes TENDL a truly complete general purpose library, is resonance in- formation (provided by TARES), average number of neu- trons from ﬁssion (provided by TAFIS), prompt ﬁssion neutron spectrum (provided by TANES), and complete covariance information (provided by TASMAN).
A.Brief Summary of Nuclear Models
We will not repeat the more extensive description of all nuclear models that are implemented in TALYS, and refer to Ref. [8] for that. Rather we constrain ourselves to describing the essential models and parameters that are used for a typical TENDL ﬁle.
total (n,tot), elastic (n,el) and non-elastic (n,non) reactions,
•capture channel (n,γ),
single  particle  production  channels  (n,n∗),   (n,p), (n,d), (n,t), (n,h), (n,α),
discrete level inelastic reactions, (n,n∗ ), (n,n∗ ), etc. and  the  continuum  (n,n∗cont),  and  similarly  for  all other ejectiles,
•multi-particle production (n,2n), (n,np), etc,
total ﬁssion (n,f) and its subdivision into ﬁrst, sec- ond, etc. chance partial ﬁssion,
residual production (n,x)Z El, mostly relevant at higher energies, as far as not yet covered by the exclusive channels mentioned above,
production of the ground state and isomers, if present,  e.g.  (n,n∗)g,  (n,n∗)m,  and  similarly  for  all other channels and residual products,
total particle production (n,xn), (n,xp), etc, mostly relevant at higher energies.
As for secondary distributions, TALYS produces:
•the elastic scattering angular distribution,
the angular distributions for inelastic scattering per discrete level, and similarly for the other ejectiles,
double-diﬀerential emission spectra for all outgoing particles,
•recoil distributions for the residual nuclides,
•particle production yields,
discrete and continuum gamma-ray distributions for all open channels.
1.
Optical Model
The most important quantities calculated by the opti- cal model are
•the shape elastic angular distribution,
•the reaction cross section,
•the total cross section,
inelastic cross sections and angular distributions us- ing either a coupled-channels (deformed nuclei) or DWBA (spherical nuclei) approach,
transmission coeﬃcients, for compound nucleus de- cay and pre-equilibrium emission,
and a good optical model is  expected  to  reliably  pre- dict these quantities for energies and nuclides for which  no  measurements  exist. All  optical  model  calculations in TALYS are performed by ECIS-06 [24], which is im- plemented as a subroutine. Unless  otherwise  speciﬁed, the  optical  model  potential  is  that  of Koning-Delaroche
[21]for non-actinides and Soukhovitskii et al. [25] for ac- tinides. As detailed in [21], local parameterizations, i.e., per target nuclide, are used as preference, and if they
do not exist due to absence of experimental data the global parameterization is used. In addition, by default the dispersive (unpublished) variant of the potential is taken with the parameters as stored in the optical model database of TALYS.
For every nuclide at or near the valley of stability, a coupling scheme has been stored in the TALYS database, so that upon request automatically a coupled-channels calculation can be invoked.  If a speciﬁc deformed poten-
tial is not available in the database, then for deformed non-ﬁssile nuclides, e.g., in the rare earth region, we take the local or global spherical potential of Koning-
Delaroche and subtract 15% from the imaginary surface potential parameter.
The other direct reaction methods  used,  depending on the target nucleus, such as DWBA, weak-coupling and various rotational and vibrational coupled-channels schemes are discussed at length in [21].
For deuterons, tritons, and Helium-3 particles, we use a simpliﬁcation of the folding approach of Watanabe [26], see also Ref. [27]. We take the nucleon OMPs described above, either local or global, as the basis for these com- plex particle potentials.   A change compared to some of
the recent TENDL versions is that for alpha particles we now use the potential of Avrigeanu et al. [28], which pro- vides a globally good description of alpha particles for
both projectiles as ejectiles.
2.Level Density
For most nuclides, the level density is taken to be of the Backshifted Fermi Gas form, although for some nuclides we use a combined Constant Temperature + Fermi gas form, as detailed in Ref. [29]. There is a level density pre- scription for the target nucleus, compound nucleus and all residual nuclides that can be reached in the reaction, and for each of these nuclides this involves typically 3 or 4 parameters: the level density parameter a, the spin cutoﬀ parameter σ2, and the pairing shift P (or E0  and T in the constant temperature model). For deformed nu-clides, extra parameters for collective enhancement are involved. For nuclides that may ﬁssion, the above num-ber of parameters is multiplied by the number of ﬁssion barriers, since on top of each barrier a discrete and con- tinuum level scheme is built. Although models for using microscopic level density tables are available, the  cur- rent TENDL versions are still based on phenomenologi- cal models, since there is more experience with parameter adjustment for these models, as changes from nucleus to nucleus are smoother for global phenomenological mod- els. Since these microscopical level densities, which we will call  ρmicro, have  not  been  adjusted  to  experimen- tal data, we add adjustment ﬂexibility through a scaling
function, i.e.,
@6
where by  default c = 0 and δ = 0 (i.e., unaltered values
from  the  tables).    The  “pairing  shift”  δ  simply implies
obtaining the level density from the table at a diﬀerent energy. The constant c plays a role similar to that of the level density parameter a of phenomenological models. Adjusting c and δ together gives adjustment ﬂexibility at both low and higher energies.
3.Gamma-ray Strength Function
The gamma-ray strength function is often taken as that of Kopecky-Uhl [30], of which the most important pa-rameters are the height, width,  and energy of the Gi- ant Dipole Resonance. Like the particle transmission co- eﬃcients that emerge from the optical model, gamma- ray transmission coeﬃcients enter the Hauser-Feshbach model for the calculation of the competition of photons with other particles. For E1-transitions, GDR param- eters for various individual nuclides exist in the RIPL database, and these are stored in the nuclear structure
database of TALYS. Some nuclides have a split GDR, i.e., a second set of Lorentzian parameters.  For these cases,
the incoherent sum of two strength functions is taken. For all transitions other than E1, systematic formulae compiled by Kopecky [31], for the resonance parameters are used.
At suﬃciently low incident neutron energies, the av- erage radiative capture width Γγ is due entirely to the s-wave interaction, and this width at the neutron sepa- ration energy is used to normalize the gamma-ray trans- mission coeﬃcients. These Γγ values are, when available, read from our nuclear structure database. For nuclides for which no experimental value is available, values come from interpolation, in mass, between available experimen- tal values.
Similar to level densities, TALYS also provides micro- scopic options for E1 radiation, with gamma-ray strength functions calculated according to the QRPA model, see Ref. [32]. Here, the adjustment is as follows:
@7
where by default fnor = 1 and Eshift =  0  (i.e.,  unal- tered values from the tables). Since these microscopical
strength functions have not yet been adjusted to exper- imental data, they are not yet used for TENDL, but in  the near future they will be.
4.Compound Nucleus Model
For low incident energies, the Hauser-feshbach model with width ﬂuctuation correction has been implemented. We still stick to the conclusion of [33] that Moldauer’s model is appropriate for this. For compound nucleus angular distributions, the Blatt-Biedenharn model is in- cluded. TALYS adds the resulting Legendre coeﬃcients to those of the shape elastic scattering and direct inelas-tic scattering to yield the total Legendre coeﬃcients at each incident energy. At higher incident energies, the bi- nary compound nucleus cross section becomes small, and compound nucleus evaporation proceeds mainly via the multiple emission Hauser-Feshbach model, which is a bit simpler than that of the binary reaction since width ﬂuc- tuation corrections and angular distributions are not in- cluded. A lot of eﬀort has been invested in the early years of TALYS development to make these calculations as ef- ﬁcient as possible, since often more than 95% of the cal- culation time is spent inside the Hauser-Feshbach loops. Essential is to store intermediate data, containing a com- bination of level densities and transmission coeﬃcients,
in arrays outside the various loops, and to use integer, instead of real, manipulation for angular momentum vari- ables.
5.Pre-equilibrium Model
For pre-equilibrium reactions, TALYS makes use of the two-component exciton model model[34], involving one general matrix element M 2 steering the damping from simple to more complex exciton states, and the single- particle state density parameters gν and gπ which deter- mine the particle-hole state density for each nuclide con- sidered in the reaction chain. The latter two parameters are often used to adjust (n,2n) and (n,p) cross sections. The emission rates that appear in the exciton model are based on the same OMP transmission coeﬃcients as those used in the other reaction mechanisms, such as compound nucleus decay. The pre-equilibrium stage takes place af- ter the ﬁrst stage of the reaction but long before statisti- cal equilibrium of the compound nucleus is attained. It is imagined that the incident particle step-by-step creates more complex states in the compound system and grad- ually loses its memory of the initial energy and direction. The two-component exciton model has been proven to be a powerful method to describe this process. As described in [21], for deuteron up to alpha emission also stripping, pick-up, break-up and knock-out parameters are required as additions to the aforementioned exciton model. Also an exciton model for photon emission and multiple pre- equilibrium reactions are included.
6.Fission
For ﬁssion, the default model implemented in TALYS is based on the transition state hypothesis of Bohr and the Hill-Wheeler expression.  As described in  e.g.,  Ref.
[8], this represents the probability of tunneling through a
single barrier with height Bf and width kωf for a com- pound nucleus with excitation energy Ex. It reads
@8
For a transition state with excitation energy εi above the top of the same barrier, one has
@9
which means that the barrier is simply shifted up by εi. This provides the input for formulae for double and triple humped barriers.   These transmission  coeﬃcients
enter the Hauser-Feshbach model to compete with the particle and photon transmission coeﬃcients. The ﬁs- sion calculation in TALYS is steered by many parame- ters, such as for each ﬁssion barrier its height, width and level density parameters for each nuclide in the chain. In addition class II/III states for resonance structure in between the wells may alter the prediction. Experimen- tal parameters are taken from the RIPL database [31], which contains a large collection of actinide ﬁssion bar- rier heights and curvatures for both the inner and outer barrier based on a ﬁt to experimental data.   It has to    be noted here that even with a seemingly robust set of parameters for all these ﬁssion and level density param- eters it is very hard to describe the ﬁssion channel, and we consider this as one of the weak points of TENDL. Often, we have to normalize the TALYS calculations to other well-evaluated nuclear data libraries to get a satis- factory answer.
IV.UNCERTAINTIES
A.Bayesian Monte Carlo for Covariances
The uncertainty quantiﬁcation of TENDL follows a method called Bayesian Monte Carlo (BMC), see e.g.,
[35] and proceeds in various steps of inference.  The full
framework has been described in [36] and here only the essential steps are repeated. Note that the outline below holds for the fast energy range. A mathematically rigor- ous description is given in Ref. [37]. The combination of Bayesian updating, using likelihood functions, and Monte Carlo sampling of parameters applied to nuclear model
codes has ﬁrst been applied by Smith, [38], Capote et  al. [39] in a method called UMC-B. and Bauge et al. [40] in a method called BFMC. Our approach is inspired by
all these initiatives, but diﬀers in a few aspects.
The probability that an experimental data set with un- certainties, represented by the vector x is described by a nuclear model calculation driven by a parameter vector p, is given by the likelihood function
@10where χ2, an abbreviation of χ2(p; x), is a measure for the deviation of model from experiment, weighted by the experimental covariance matrix. The choice of χ2 is dis- cussed below in Section IV A 1. We have omitted factors
that cancel out in the ﬁnal formulae, see Eq. (12). As outlined in [36], the Bayesian update integral needed to go from prior to posterior uncertainties is calculated by the Monte Carlo technique.  In more practical terms,  we
take a total of K random parameter samples and for each sample k, all L nuclear model parameters are randomly
@11
Hence, for each individual TALYS run. L may represent hundreds of parameters, to which the calculated cross sec- tions are sensitive or not (the sensitivity is less relevant
in a Monte Carlo approach). With these random param- eter samples the TALYS code is run to produce all cross sections,  spectra,  angular distributions etc.  of interest.
The χ2(k)  for the entire experimental data set under con-
sideration is calculated so that the weight of each random TALYS sample is given by
@12
which automatically entails that  K  w(k) = 1. Values  of K around 3000 are used to obtain proper posterior parameter distributions.
The concept of BMC is thus rather straightforward: Run a model code K times and calculate w(k) for k=1,K. Each random sample k contains information about the speciﬁc combination of parameters that leads to a cer- tain set of cross sections, and thus a value χ2(k). Clearly, a large deviation between TALYS and experiment means a large χ2  value  and low  and w(k)  for that particu-   lar combination of parameters. In this sense, knowledge on the parameter distribution is gradually built up. We obtain the full posterior probability distribution for each parameter, simply by applying the weights w(k) to each value of the sampled parameters. For completeness, we mention that by averaging over the K samples, we also obtain converged values for average, variance and covari- ances for the model parameters, although we do not need these moments in our approach. The weighted average of each of the L model parameters is
@13
the variance,
@14
and the covariance,
@15
The process is depicted in Fig. 11. We start with a suf- ﬁciently wide uniform probability distributions for the model parameters. Each random sample leads to a par- ticular weight by considering the deviation from model to experiment for all channels, and applying that weight to all parameters of the sample gives the ﬁnal numer- ical parameter distribution. In principle, the posterior parameter distribution is such that subsequent sampling from it no longer leads to unrealistic cross sections, since those parameter combinations have been ﬁltered out in the ﬁrst step which starts from the uniform parameter distribution.  Hence, the ﬁnal probability distribution for the cross sections is obtained by starting from the nu- merical parameter histograms that were obtained in the ﬁrst step. That process is depicted in Fig. refBMC2. The random TALYS cross sections are now closer to the exper- imental data than after uniform parameter sampling, and we obtain weighted posterior probability distributions for the cross sections this time. This is also the step in the
process where for each sample of the parameter vector the random ENDF library for use in Total Monte Carlo
is obtained.   Also, for this second step the number of random runs is typically a 1000, although practical expe- rience shows that results converge rather well after 300 runs already. It is important to restart with a credible pa- rameter distribution because Total Monte Carlo is com- putationally expensive and so is the computation of the covariance matrix. Again, the ﬁrst moments of the dis- tributions can also be obtained, which are relevant since traditional uncertainty propagation makes use of covari- ance data in ENDF-6 format, as also used in the TENDL libraries. Hence, the weighted average of each theoretical
(T) cross section i is
@16
the variance,
@17
and the covariance,
@18The above is the entire uncertainty propagation proce- dure. It is very straightforward, but it contains some awkward issues which we discuss below. First on the method, although the most exact for uncertainty propa- gation, the disadvantage is that it is generally not easy to identify which actual parameter(s) are responsible  for
e.g., large χ2 values, as in simple linear sensitivity meth-ods, although the particular parameters which have a large impact on the cross sections, and thus χ2, eventually
emerge through strongly non-uniform posterior probabil- ity distributions, after sampling enough times. Prior and posterior correlations between parameters are also irrele- vant, apart from possible academic interest in that they
conﬁrm expected correlations, such as e.g., the correla- tion between the optical model real potential depth and
radius (V.r2 = constant, which is expected from OMP volume integral considerations). Hence, parameter corre- lations are output, not input, of the method. The only disadvantage of sampling the model parameters uncorre- lated is that the process is somewhat less eﬃcient, since certain parameter combinations which should have been sampled correlated may give a larger distance of model
from experiment and thus a smaller weight, but these do not count in the end result anyway. Parameter combina- tions which lead to physical results, i.e., theoretical cross
sections close to measurement,  automatically emerge in
the parameter covariance matrix of Eq. (15) as strongly correlated.
1.Goodness-Of-Fit Estimator
We think that the main problem of BMC resides in the correct deﬁnition of the goodness-of-ﬁt estimator χ2, and a solution for this has not been found yet. Obviously, with a weighting function like exp( χ2/2) no statistical convergence, or even meaningful results, are obtained if the distribution of χ2 values is too wide. Suppose that after random sampling all K values of χ2 vary between 10 and 30, then more than 99% of the w(k) values corre- spond to useless parameter combinations resulting in very small weights and the ﬁnal posterior distribution for both cross sections and parameters is too wide. Of course, sta- tistical theory dictates that models leading to such large χ2 1 values should be rejected, but models missing the experimental uncertainty band by several factors are not uncommon in nuclear data, and we have no choice but to use the models to provide complete data libraries!
There are both experimental and theoretical causes for this χ2 problem:
In general, a full covariance matrix for the experi- mental data used in the evaluation is not available from the original work or not (even approximately) assigned by someone else, like an evaluator. Some- times, the distinction between statistical and sev- eral sources of systematical uncertainties is given. If data points are treated as completely uncorre- lated, by lack of better option or hesitation to as- sign correlations, it is easy to see that the total χ2 may become huge. In addition, the total χ2 will be even larger if wrong experimental data sets are not ﬁltered out.
The limitation of the nuclear model, also called the model defect. Even if the problem of the previous item, a consistent experimental covariance matrix to keep χ2 constrained, would be solved, a deviation of the model of several factors larger than the ex-perimental uncertainty, and which can not be solved
by model parameter variation inside physically ac- ceptable limits, will again lead to high χ2 valuesand thus low and widely scattered values for w(k).
In sum, we try to relate limited nuclear models to good and wrong experimental data without proper experimen- tal covariance assessment. It was outlined in [21] that automatic data ﬁtting is hopeless in such cases.
Since we feel that the proper deﬁnition of the GOF estimator χ2 is more important for the ﬁnal result than the particular variant of Bayesian inference, whether it isBMC, UMC-B or BFMC, we will make this a bit more ex- plicit. The generalized χ2 formula for N calculated cross section points σTi, put in vector σT and N experimental cross section points σEi, put in vector σE is@19
If the N N matrix span one experimental data set,  e.g., cross sections for N incident energies of an excitation function, then the full experimental covariance matrix E
may be available, if we are lucky. This is, or should be, published as the outcome of one measurement, and be available in EXFOR. If the correlations between points are reasonable, then the E matrix in Eq. (19) can be in- verted. If the N  N matrix spans various measurements of the same reaction channel, the situation is already more complicated, This occurs for the evaluation of neu- tron standard reactions, where some correlation between
experimental data sets, via e.g., common monitor reac- tions or other common experimental circumstances, need
to be included to get a proper complete covariance ma- trix. If the N N matrix spans all available experimental data sets for all reaction channels of a target nucleus, the situation is already running out of hand, even though a full covariance matrix would be the only consistent entity to take into account when making an evaluated data ﬁle for one target isotope. To ﬁll E, assumptions on cross channel correlations will have to be made (including the simple assumption of keeping them zero). And ﬁnally, if the N N matrix spans many nuclides and reactions si- multaneously, a proper evaluation is (so far) out of reach. For all these scenarios, E needs to be inverted to come to a χ2 value, and it is not at all guaranteed that the as- sumptions we impose on the cross correlations will keep a constrained χ2 value, if E can be inverted at all. In ad- dition, low-quality or suspicious experimental data sets need to be discarded.
The most extreme assumption that can be made for Eq. (19) is that all values in the data set are entirely uncorrelated.  In that case the matrix elements Ei,j  are   0 if i = j, with the diagonal elements of E given by the
square of the total, i.e., systematic + statistical, uncer- tainty of the experimental cross section, i.e., dσEi. This leads to
@20What becomes clear is that assuming uncorrelated val-
ues will directly lead to large total χ2 values. Never- theless, this assumption is often imposed when there  are
e.g., various diﬀerent measurements for the same incident energy, and the experimental values + uncertainties are
compared with a model calculation or the value given by a data library.
Even if the above problem of establishing a credible experimental covariance matrix would be solved, we still
have the problem that all nuclear models are limited. A signiﬁcant deviation of the model values from the exper- imental data will directly give large χ2 values.
Promising developments are underway to tackle these issues. A recipe for assuming and including experimental uncertainties and their correlations has been proposed by [37]. Model defects can be taken into account by using Gaussian processes, as detailed by [41].
Unfortunately, neither of these promising develop- ments has yet made it into the TENDL evaluation scheme. Until that time, model defects and the absence of experimental covariance data has been taken into ac- count in a more approximate way by the following prag- matic formula for χ2 for each random sample k,
@21
where χ2(0) serves as a damping value belonging to the TALYS run with central values for all parameters, as a remedy for the model defect, c runs over the C reaction channels with experimental data, m runs over Mc exper- imental data sets per reaction channel, where each data set has NMc data points, N is the total number of data points and wm is a weight to discard or include particu- lar experimental data sets. The second term can be con- sidered as the prior since it covers pseudo-experimental data (of which we give more details below), with NTc the number of data points for each theoretical, or pseudo- experimental, channel. The establishment of Eq. (21) is explained in much more detail in Ref. [36].
2.Pseudo-Experimental Data
The uncertainty quantiﬁcation of evaluated data in TENDL goes according to the following scheme. Before doing a TENDL evaluation of one particular isotope in a Bayesian framework, we wish to establish a prior that is independent of the experimental data for that particular isotope. For that we have two valuable assets:
•A universal nuclear model code, TALYS
The entire EXFOR database for all nuclides and re- actions (including the particular isotope under eval- uation, but that is statistically irrelevant)
In Ref. [36] it was outlined how we established the predic- tive power of TALYS for neutron-induced reactions. We perform one ’blind’ TALYS calculation for each target nu- clide with all nuclear model parameters taking on their global, default values and compare the results with the
entire EXFOR database for cross sections. By binning the deviations of model from experiment in histogramswe establish the average deviation of TALYS from exper- iment, as a function of reaction channel and incident en- ergy, averaged over all nuclides close to the stability line with experimental data. The predictive power of TALYS for neutron-induced reactions is then given by the fol- lowing function for the standard deviation s, in %, per reaction channel,@22
where smin is the minimal standard deviation for the global prediction, often obtained near the peak of the ex- citation function, and generally somewhat lower than the average value for the reaction channel save. The second
term depends on E∗ = E − E, with E the threshold
tions with positive Q-values, like (n, p) and (n, α) chan- nels, we obviously apply this only to the rising part of
the excitation function, and not in the thermal range).
The exponent, scaled by parameter b, mimics the sharp decrease in relative deviation between TALYS and exper- imental data when the energy exceeds threshold. Param- eter a is a measure for the uncertainty at threshold. The last term mimics the change in uncertainty at higher ener- gies, depending on parameters c and d, and E∗∗ = E∗   Ec, where Ec is a measure for the energy at which the rela-
tive uncertainty is minimal, i.e., generally at the highest cross section value.  The parameters for the various reac-
tion channels are given in Table I.
TABLE I. Global cross section uncertainties per reaction channel from default TALYS calculations; average deviation and parameters for energy-dependent variation, see Eq. (22). The relative deviations save, smin, a and c are given in %, b is a dimensionless factor, while the energies d and Ec are given in MeV.
Note that almost all EXFOR data, i.e., neutron reac- tions on all target nuclides between F and Fm, determine
the parameters of this global formula. Eq. (22) can then be applied to one nuclide in particular to create pseudo- experimental  data  for  that  target  nucleus,  i.e.,  we ﬁll
every reaction channel with central values provided by
the global TALYS calculation for that nucleus and as- sign uncertainties to each point using formula (22).  In
some sense, we create an entire ”EXFOR” database with pseudo-experimental data.
A perhaps controversial way of explaining Eq. (22)  is: if a measurement is less precise than the uncertainty given by the predictive power, then one can better take  a TALYS estimate instead! Of course, in practice exper- imental uncertainties of good measurements are signiﬁ- cantly smaller than that associated to the global predic- tive power of TALYS, so this seldom occurs. Creating these sets of pseudo-experimental data for all reaction channels of a nucleus has two purposes:
A.It can be used for the ﬁrst zoom-in of parameter un- certainties, as illustrated in Fig. 11. We start with a very wide uniform distribution for each TALYS parameter, representing an uninformative prior for the parameters (’knowing nothing’). Each weight is now determined by a χ2 that is obtained relative to the pseudo-experimental data for this nucleus. By means of the Bayesian Monte Carlo update,  the wide uniform parameter distribution is reduced to a non-uniform, narrower distribution which corresponds to a ﬁnal cross section distribution that over- laps more or less with the uncertainties of the pseudo- experimental data. If for a particular reaction no real experimental data exists, the ﬁnal evaluation including covariance data is hereby already obtained.
B.We need to keep the pseudo-data on board as es- sential information when we further zoom in on well- measured data per nuclide. We must include pseudo-data for unmeasured reaction channels or energy ranges. Oth- erwise one may obtain unrealistic cross section results since the optimization process will steer certain parame- ters to any unrealistic value if they are not constrained
by data. In other words, if for a nuclide e.g., only some experimental  (n,γ) data  points  exist  and  no  (n,α) data
while hundreds of TALYS parameters are varied simulta- neously to produce the optimal result, then speciﬁc pa- rameters for e.g., the alpha knock-out reaction may reach
unphysical values,  since the optimization process is  not
penalized for attaining unrealistic theoretical (n,α) cross sections. Filling the entire reaction channel space with pseudo-data remedies this. Once credible real experimen- tal data sets are included for a particular reaction chan- nel, it will make the pseudo-data for that channel and energy range directly irrelevant since the uncertainty of the real experimental data will be much smaller and thus have a larger weight to the χ2.
3.Summary
In sum, we ﬁrst create pseudo-experimental data for all nuclides, energies and reaction channels. This leads to the parameter distribution of Step 1, i.e.,  the one that repro-
duces the pseudo-experimental data from global TALYS
calculations. Next, if real experimental data from EX- FOR exist for various reaction channels of a nucleus, they will, thanks to their smaller uncertainties, dominate the Bayesian updating process and the parameter distribu-
tions will become narrower and the posterior parame- ter and cross section pdf’s for this nucleus are obtained. Next, after assigning weights based on well-selected ex- perimental data for the particular nucleus under study, the posterior parameter distribution is used for produc- ing the covariance matrix for this target nucleus and for random ENDF data ﬁles for Total Monte Carlo (TMC) application.
B.	Covariances for Resonance Regions
Similar to the section on the resonance parameters, the uncertainties (or covariance) approach varies depending on the level of knowledge of the isotope.
1. Parameters Covariance for Stable and Long-lived Isotopes
In the case of covariance information for the resonance range, diﬀerent possibilities have been explored in the past TENDL releases. The ﬁrst choice was to provide so-called “ﬁle 32” (or MF32) together with a compact version (smaller in size, but retaining only part of the MF32 information), being the uncertainties and corre- lations for (and between) resonance parameters. To be useful to users, such information needs to be processed (for instance with NJOY [42]), implying that the right processing of MF32 had to be done by persons not al- ways familiarized with the speciﬁcs of MF32, and with speciﬁc NJOY versions, not always veriﬁed for this type of processing. An example of such processed MF32 in terms of cross section is presented in Fig. 13 (top) as    it was included in TENDL-2010. More recently, a diﬀer- ent approach was taken, following the development of the ENDF/B-VII.0 library, using MF33 only. It was shown in Ref. [23] that the use of MF33 instead of MF32 can present some limitations for the calculations of uncer- tainties on self-shielded cross sections. In practice, this drawback is partly compensated by the convenience of the MF33 processing. The process to produce a MF33 in the resonance range is based on SAMMY:
The existing resonance parameters are provided to SAMMY to reconstruct the cross sections. The un- certainties on these parameters were ﬁrst assessed to reproduce pointwise cross sections uncertainties, as for instance provided in the thermal range by the Atlas of Neutron Resonances [12]. For higher energy, a general concept of increasing cross section uncertainties with energy is used, assuming that the thermal cross sections are better known that other ones in the resonance range.
SAMMY is also ﬁtting the obtained cross sections, leading to energy-energy cross section correlations based on a speciﬁc energy grid.
FIG. 11. Bayesian update of model parameters. Eq. (10) is used to denote the weighting.
FIG. 12. Probability distributions for cross sections. Eq. (10) is used to denote the weighting.
FIG. 13. (Color online) Uncertainties and correlations for
92Zr(n,tot) from MF32 (top) and MF33 (bottom).
The uncertainties of the thermal cross sections are reproduced by multiplying the whole range by a speciﬁc factor. This is possibly one of the most crucial steps in this process. For many applications, the thermal neutron region is of prime importance. Having a good educated guess for the uncertain- ties in this region will determine the quality of the covariance ﬁles for a large number of users. In the majority of cases, we follow the recommendations of the Atlas [12] for uncertainties of the thermal cross sections. If no values are given, other considera- tions are involved (such as the isotope half-live, its abundance, its relevance for speciﬁc applications). At this point, it is diﬃcult to list all the possible cases and we invite the reader to visit the TENDL website. Speciﬁc examples for Pb and Bi can be found in Ref. [16], and in Ref. [43] for 147Nd.
The new correlations and uncertainties are included in a MF33 ﬁle.
This approach is very pragmatic and is far from a cor- rect analysis of the resonance range. It is anyway not its purpose. In practice, the obtained correlations and un- certainties can be considered satisfactory as long as it is not proven otherwise. An example is presented in Fig. 13 (bottom) for the 92Zr(n,tot) cross section, where two ver- sions of a TENDL covariance ﬁle, one with MF32 and one with MF33, are compared.
C.Covariances for nubar and PFNS
As for the other nuclear data quantities, the covari- ance information for nubar and the prompt ﬁssion neu- tron spectra (or pfns) are produced based on the variation of model parameters. Diﬀerent models can be used, from very simpliﬁed considerations [44] to more advanced prin- ciples [45], leading to very diﬀerent results, for uncertain- ties as well as for energy-energy correlations. In TENDL, the Madland-Nix model is used for all actinides [46], and various developments are being currently done to use the GEF code. As the Madland-Nix model is not linked to other models (for instance for cross section calculations), the nubar and the pfns are not correlated with other nu- clear data quantities. Such correlations can only be “cre- ated” with the use of integral benchmarks, as presented in Ref. [47] (for the time being, these additional correla- tions are not included in the TENDL library). Examples of the uncertainties and correlations for the nubar and pfns of 235U are presented in Fig. 14.
The shape of the correlations is typical from the varia- tions of the parameters of the Madland-Nix model, with one rigid point at high energy, and anti-correlated varia- tions from each side of this point. In a complete evalua- tion, these strong correlations would be attenuated by the use of experimental data. In the future, more dedicated model codes for ﬁssion fragment distributions, such as the GEF code but also newer approaches, will oﬀer an al-
FIG. 14. (Color online) Example of the uncertainties and correlations for the nubar of 235U in 44 groups, at thermal energy for nubar (top) and 1 MeV for the pfns (bottom).ternative approach, especially once connected to TALYS. This would provide the possibility to obtain correlations between diﬀerent quantities (ﬁssion yields, nubar, pfns, and cross sections).
V.LIBRARY PRODUCTION
A.Production Flow for TENDL with ‘T6’
This section contains a description of the various tools which are needed for the production of TENDL. The soft- ware system built around TALYS is called T6, which is named after the six core codes which are needed to pro- duce a complete nuclear data library.
1.T6 Software
The four major (in the sense of possibilities, develop- ment eﬀort and size) codes are:
TALYS
The nuclear reaction code TALYS has been exten- sively described in Section III.
TEFAL
The ENDF-6 formatting code TEFAL processes the nuclear reaction results of TALYS, and data from other sources if TALYS  is not used, into ENDF-  6 nuclear data libraries. TEFAL was constructed to avoid any error-prone human interference in the creation of nuclear data ﬁles: the whole ENDF-6 ﬁle is created at once, on the basis of all nuclear reaction info that we  feed it. Hence,  the idea is  to ﬁrst run TALYS for a projectile-target combina- tion and a range of incident energies, and to obtain a ready to use nuclear data library from the TEFAL code through processing of the TALYS results, pos- sibly by merging it with experimental data or data from existing data libraries. All possible covariance data that the ENDF-6 format allows is included as well. For all this, a signiﬁcant part of the ENDF-6 formats manual [48] was implemented in TEFAL.
TASMAN
TASMAN is statistical software for TALYS. The most used function of TASMAN is the generation of probability distributions for all the outputs of TALYS such as cross sections, spectra, angular dis-
tributions etc. and their related ﬁrst  moments such  as  averages,  variances  and  covariances.  It
does this on basis on Monte Carlo sampling of the TALYS input parameters. The Bayesian Monte Carlo method described in Section IV is now the preferred method for uncertainty quantiﬁcation and propagation. TASMAN is thus able to sample from arbitrary, tabulated probability distributions ob- tained in a ﬁrst inference stage and is also the driver
for the generation of random ENDF ﬁles which form the basis of Total Monte Carlo uncertainty propa- gation. In addition, TASMAN can create parame- ter sensitivity proﬁles for all cross sections, spectra
etc. that TALYS produces and it can automati- cally ﬁt TALYS cross sections to experimental data
from the EXFOR database through deterministic or stochastic (simulated annealing) search methods on the model parameters. For the BMC method, TAS- MAN is run a ﬁrst time to produce the weighted pa- rameter distributions, starting from a uniform prior parameter distribution. After that, TASMAN is restarted and samples from the weighted distribu- tion.
TARES
TARES is a code to generate resonance information in the ENDF-6 format, including covariance infor- mation. In short, it produces a full set of RRR   and URR parameters for MF2, and a complete set of covariance information in MF32, which is stored either in the “regular” covariance format or in the compact format, or in MF33 (which is the default in TENDL-2017). It is discussed in more detail in Section II.
Two additional codes are needed, TANES
TANES is a simple program to calculate the ﬁs- sion neutron spectrum based on the Los Alamos model [49]. The original Madland-Nix [46] or Los Alamos model for the calculation of prompt ﬁs- sion neutrons characteristics (spectra and multiplic- ity) has been implemented in a stand-alone module. The TANES code is using this stand-alone module, combined with parameter uncertainties (on the to- tal kinetic energy, released energy and multi-chance ﬁssion probabilities) to reproduce and randomize the ﬁssion neutron spectrum. The program pro- vides the central and random values for the ﬁssion neutron spectra at diﬀerent incident energies (MF5) and their covariances (MF35). For covariance ﬁles, a simple Monte Carlo variation of the parameters is performed, using the Madland-Nix model. The random spectra (or more speciﬁcally their standard deviations) are then formatted into MF35.
TAFIS
TAFIS calculates ﬁssion yields, prompt neutron emission from ﬁssion and other necessary ﬁssion quantities (kinetic energy of the ﬁssion products, kinetic energy of the prompt and delayed ﬁssion neutrons, total energy released by prompt and de- layed gamma rays). For ﬁssion yields, it is using the systematics of ﬁssion-product yields from A.C.
Wahl [44], combined with ad hoc uncertainties. It calculates the independent and cumulative ﬁssion
yields at any incident energy up to 200 MeV and for diﬀerent incident particles (spontaneous, neutrons,
protons, deuterons, etc). Empirical equations rep- resenting systematics of ﬁssion-product yields are derived from experimental data. The systematics
give some insight into nuclear-structure eﬀects on yields, and the equations allow estimation of yields from ﬁssion of any nuclide in the range Z  =  90   to 98 and A = 230 to 252. For neutron emission, diﬀerent models are used depending on the energy range and are presented in Ref. [44]. The output of this program is a ﬁssion yield ﬁle with uncertain- ties, prompt and delayed neutron emission ﬁles for central and random values (MF1 MT452), a list of central and random ﬁssion quantities (MF1 MT458) and prompt neutron covariances (MF31). For the delayed neutron emission, as the correct physics is yet not included in TAFIS, a simple copy from ex- isting libraries is done. For isotopes which are not included in other libraries, the delayed neutrons are simply copied from the nearest isotope. As for the MF35, a simple Monte Carlo variation of the model parameters is used to generate covariance ﬁles.
TARES, TAFIS and TANES provide everything which TALYS can not (yet) do. Note that all  three  codes come with complete covariance information and/or ran- dom sampling of parameters to produce probability dis- tribution for all outputs.
Hence, central in the T6 software system are the above six codes. However, inspection of the entire T6 system reveals no less than 58 executable codes. We will explain, or at least shortly mention, most of them since that will elucidate how complicated the process still is. We want to stress here that a future clean up of T6 means getting rid of some of these codes, or merging some of them into one code. For sure, it is planned to have all 6 abovementioned
core codes in TALYS-2.0, i.e., in one and the same source code, usable by everyone. Further important software for TENDL production is:
autotalys: a bash shell script that drives the entire T6 system.  With autotalys, TENDL is produced,
random ﬁles are made, TALYS parameters are op- timized, etc. An isotope data ﬁle in TENDL is produced by  one autotalys command, with several
ﬂags of course. Hence, TENDL-2017 is produced by 7 (projectiles) times about 2800 (target nuclides) = 19600 autotalys commands.
ENDFTABLES: a FORTRAN code to transform an existing ENDF-6 formatted nuclear data ﬁle into   a directory-structured x-y or x-y-dy database (if covariance matrices are available). Hence, all iso- topic nuclear data ﬁles from the major world li- braries are completely dismantled using ENDFTA- BLES and put into single ﬁles per reaction channel. This makes handling such data, for plotting, adop-
tion into TENDL ﬁles etc. much more eﬃcient and robust.
•plot : a plotting script which plots the cross sec-
tion for one particular nuclear reaction comparing all EXFOR data with TENDL-2017, ENDFB-VIII, JEFF-3.3, JENDL-4.0, CENDL-3.1, EAF-2010 and IRDFF-1.05.
plotall: a plotting script which runs the aforemen- tioned ’plot’ for all important reaction  channels per target nuclide, thereby making a ’cross section book’ for the nuclide under consideration.
autonorm: A FORTRAN code to normalize TALYS against data from other nuclear data libraries. In the input ﬁle for autonorm, the reaction channels for which normalization to other libraries needs to take place are given. Next, the procedure is the following:
–Run TALYS. This will produce for each re- action channel c, and incident energy E, the original TALYS cross section σT (E).
–We create a speciﬁc input ﬁle for autonorm for the normalization of the TALYS results to the cross sections of the nuclear data library, e.g.,
IRDFF-1.05 for some channels and ENDF/B-
VIII for some other channels, in a certain en- ergy range that we want to restrict the nor- malization to. This deﬁnes for a few chan- nels the library cross sections σL(E) Then, autonorm reads both the TALYS and the li- brary cross sections and produce cross section ratios Rc(E) = σL(E)/σT (E) and stores these
other ranges of nuclides. The code driplist may be called from autotalys.
mf1maker: a bash shell script that produces a full MF1 documentation ﬁle. mf1maker can be steered by various ﬂags and is called from the autotalys script.
extrema: a very short FORTRAN program that de- termines the maximum and minimum of a cross sec- tion, for plotting purposes.
ZAres: code to calculate the Z and A of the residual product given the MT number. This is needed for, and called from, ’plot’.
select: bash script to randomize selected parts of an ENDF-6 ﬁle for sensitivity purposes, or to make covariance ﬁles during, instead of after, the random runs. This script is called from the TASMAN code.
run-plots: script to run NJOY and produce plots from its ACER module, called from autotalys.
run-compa: script to compare two ENDF ﬁles and compare the diﬀerence, makes use of PREPRO and NJOY, called from autotalys.
run-errorj: script to run NJOY and produce covari- ance plots from NJOY, called from autotalys.
Various well-known nuclear data codes from other au-
c	c	thors are included in T6
in a so-called ’rescue’ ﬁle (i.e., a ﬁle to be
used when anything else fails to get the perfect
TALYS result).
–Run TALYS for a second time, but now us- ing ’rescue’ keywords which make TALYS read in the rescue ﬁles with normalization factors. Hence, all channels for which normalization takes place now have their own ’rescue’ key- word in the second TALYS input ﬁle. TALYS then multiplies the ’original’ TALYS cross sec- tions, σT newc(E) = Rc(E)σT (E), so that af- ter the second TALYS run, the results for the speciﬁed channels will be exactly equal to that of the nuclear data library to which it was nor- malized. The diﬀerence σT newc(E) σT (E) is added to, or subtracted from, the elastic cross section, which also means that this approach becomes dangerous if the original TALYS re- sults are too far  from  the  data  one  wants  to normalize to. In a future version of au- tonorm, this redistribution will be made ac- cording to the cross-channel covariance matrix that is available from our calculations. This is physically better justiﬁed and also less risky than accounting for the diﬀerence in the elas- tic cross section.
driplist: A code to produce the list of nuclides for TENDL. This can be the full TENDL range or
BNL[50] checking codes for ENDF ﬁles (not all BNL codes are needed):
–CHECKR: Format checks
–FIZCON: Basic physics checks
–PSYCHE: Advanced physics checks
–INTER: Integral cross sections
•NJOY-12.99[51]: NJOY processing code
njoycovx ld: special version of NJOY for covariance data
PREPRO[11]: suite of ENDF processing codes (not all PREPRO codes are needed)
–recent: pointwise cross sections
–sigma1: broadening cross sections
–sixpak: transform MF6 into MF4 and MF5 (for testing only)
–groupie: groupwise cross sections
–evalhard: plot cross sections
–evalplot: plot cross sections
–activate: create MF10 out of MF3 and MF9
–legend: transform Legendre coeﬃcients into tables
–merger: combine evaluated data
–complot:  comparison of cross  sections
–comhard:  comparison  of  cross sections
–dictin: sum rules for cross sections
–linear: linearize cross sections
•xcalendf[10]: CALENDF processing code
fudge[52]: FUDGE toolkit for nuclear data man- agement and processing
To run all the processing codes, scripts have been written which produce, for the ENDF ﬁle under consideration, an input ﬁle for these codes and then runs it. These bash shell script are:
•run.bnl
•run.prepro
•run.njoy
•run.fudge
Also, diagnosis scripts have been written which ﬁlter the warning and error messages out of the most important checking codes:
•diag.checkr
•diag.ﬁzcon
•diag.psyche
•diag.prepro
•diag.njoy
•diag.fudge
Such diagnosis scripts are needed since TENDL contains about 20000 ENDF-6 formatted ﬁles which are to  be tested by each of these codes, so a clever method to dis- cover errors must be used.
2.Required Input for the TENDL Data Files
With the entire automated  system  ready  to  pro-  duce complete nuclear data libraries, all that is needed is nuclide-speciﬁc information (the actual ’evaluation’), which is available in input ﬁles for the various codes.
First of all, in the resonance parameter database the preferred resonance parameter set is ﬂagged, such that the TARES code produces resonance information, in both the RRR and URR, for MF2, and the associated covari- ance data in MF32 or MF33. Similarly, for each target nuclide a so-called ’best’ input ﬁle for TALYS may be available. Obtaining the optimal input parameter set for TALYS is an independent activity which takes place out- side T6. Examples of such ’best’ ﬁles are given in Section V B. It is done through trial and error and with expert
judgment, i.e., in this respect is equal to more traditional nuclear data evaluation. The diﬀerence in the case of TENDL is of course that at the end of each year, when
a new version of TENDL is produced, all these manually produced optimized TALYS input ﬁles are ready to be used as input in the automated production process.
The third set of input ﬁles for TENDL contains key- words for normalization to existing nuclear data eval- uations.   With  the  aforementioned  code autonorm the
TALYS  calculation is automatically forced to ﬁt exactly
the data which we want to adopt from other libraries such as standards and dosimetry cross sections.
B.Optimized Input Parameters for TALYS ‘Best’ Input Files
The current TENDL-2017 includes recent results of    a re-evaluation of neutron induced cross sections that was performed in the framework of the Fusion for En- ergy (F4E) and the CHANDA projects. This allowed to supply TALYS with a new set of improved input ﬁles with thoroughly optimized parameters for the generation of complete nuclear reaction excitation functions in the range of (0-60) MeV. The improvements have been per- formed for a list of nuclear reactions relevant for fusion technology applications as well as for ﬁssion products. In both cases the actions were targeted at getting a better cross section ﬁt with respect to TENDL-2014, TENDL- 2015, EAF, and JEFF-3.2 data libraries.
The optimization of parameters was done for a list of 154 ﬁssion products: 107Ag, 109Ag, 75As, 130Ba, 132Ba, 134Ba, 135Ba, 136Ba, 138Ba, 79Br, 81Br, 106Cd, 110Cd,
111Cd, 112Cd, 113Cd, 114Cd, 115Cd, 136Ce, 140Ce, 141Ce,
142Ce, 133Cs, 134Cs, 158Dy, 161Dy, 162Dy, 163Dy, 164Dy,
154Eu, 155Eu, 69Ga, 71Ga, 72Ge, 73Ge, 74Ge, 76Ge, 152Gd,
153Gd, 154Gd, 155Gd, 156Gd, 157Gd, 158Gd, 160Gd, 127I,
129I, 131I, 115In, 162Er, 164Er, 166Er, 167Er, 168Er, 170Er,
163Ho, 165Ho, 80Kr, 82Kr, 83Kr, 84Kr, 85Kr, 86Kr, 139La,
92Mo,  94Mo,  95Mo,  96Mo,  97Mo,  98Mo,  99Mo,  100Mo,
93Nb,  94Nb,  95Nb,  142Nd,  146Nd,  148Nd,  150Nd, 102Pd,
104Pd, 105Pd, 106Pd, 108Pd, 110Pd, 147Pm, 148Pm, 149Pm,
141Pr,   143Pr,   85Rb,   86Rb,   87Rb,   103Rh,   96Ru, 98Ru,
100Ru,  101Ru,  102Ru,  104Ru,  121Sb,  123Sb,  74Se,  76Se,
77Se, 78Se, 79Se, 80Se, 82Se, 144Sm, 148Sm, 150Sm, 151Sm,
152Sm, 153Sm, 114Sn, 115Sn, 116Sn, 117Sn, 118Sn, 119Sn,
120Sn, 121Sn, 125Sn, 126Sn, 84Sr, 86Sr, 87Sr, 88Sr, 99Tc,
159Tb, 160Tb, 120Te, 123Te, 124Te, 126Te, 128Te, 130Te,
132Te,  126Xe,  128Xe,  130Xe,  131Xe,  132Xe,  134Xe, 136Xe,
89Y, 176Yb, 90Zr, 91Zr, 92Zr, 94Zr, 95Zr, 96Zr. In total 285
nuclear reaction channels have been considered and im- proved by normalizing to high quality experimental data. The list of elements that was considered under the F4E project accounted for 70 elements: 27Al, 130Ba, 138Ba, 140Ba, 36Cl, 37Cl, 41K, 44Ca, 46Ca, 48Ca, 106Cd, 140Ce,
59Co, 50Cr, 52Cr, 53Cr, 54Cr, 63Cu, 65Cu, 72Ge, 74Ge,
164Er, 166Er, 167Er, 152Gd, 54Fe, 58Fe,180Hf, 163Ho, 86Kr,
24Mg, 92Mo, 94Mo, 95Mo, 54Mn, 55Mn, 142Nd, 58Ni, 60Ni,
22Ne,  23Na,  28Si,  30Si,  32S, 34S, 36S, 82Se,  144Sm,  114Sn,
116Sn, 117Sn, 120Sn, 123Sb, 88Sr, 181Ta, 46Ti, 48Ti, 50Ti,
171Tm, 187Re, 205Tl, 205Pb, 207Pb, 51V, 180W, 183W,
186W, 67Zn, 90Zr, 96Zr. In total 97 nuclear reactions rele- vant for fusion were improved by means of nuclear model parameter adjustment.
The procedure of cross section evaluation with T6 is a complex action that accounts for all energetically open reaction channels for speciﬁc target nuclides in a consis- tent way. For each element there is an individual opti- mized input ﬁle that is valid for one nuclide and may imply changes in several reaction channels of interest. One single TALYS run includes a set of parameters by default (general ﬁle) and a so called “best” ﬁle (see List- ing 1) that can be found in the code’s structure directory (talys/structure/best ). The parameters by default are basically the same for all elements but “best” ﬁles include adjusted values to guarantee the best agreement with ex- perimental data after diﬀerential and integral data analy- sis. Mainly those parameters were related to a change of: radius and diﬀuseness in optical potential (rvadjust, avad- just), choice of level density model (ldmodel 1-5), densi- ties of exciton model constituents (gpadjust, gnadjust), pre-equilibrium gamma emission (rgamma), branching ratios for isomeric states, formation of α/d/t in exit chan- nel (cstrip, cknock). The magnitude of those parame- ters was varied for diﬀerent reactions and was ﬁxed for certain residual nuclides. It should be emphasized that keeping a reasonable balance between potentially related channels is a crucial issue. The majority of TALYS pa- rameters aﬀects the excitation function overall but not in a local small area. Only several parameters similar   to “rvadjustF” do aﬀect the excitation function locally in the narrow energy range speciﬁed by the user.  This
is a very eﬃcient way to make the ﬁne tuning of e.g., the (n,α) reaction. The adjustment was aimed at achiev- ing the best agreement between the latest experimental
data, taken from the EXFOR database, and presently calculated TALYS curves. Also it was important to con- trol the value of the total cross section and the shape of excitation functions, and to adjust parameters within ac- ceptable physical limits. The applicability margins of all parameters are given in the TALYS manual. In addition it is important to be careful at the energies around thresh- old, specially, for reactions like (n,d) or (n,t), since the uncontrolled increase of cross section at energies around 8 MeV may easily cause a rapid increase of cross sec- tion at energies of 20 MeV or higher. In Listing 1 there is an example of an input ﬁle used for calculating the cross sections for 90Zr. This ﬁle consists of three clear parts. For the case of reactions with formation of reac- tion products in isomeric and ground states there is also a “best.branch” ﬁle that contains information on the level structure of the ﬁnal nucleus and probability of transi- tions. In the sample case of Listing 1 the assigned branch- ings are included in the input ﬁle itself. The experimental data regarding the level structure is taken from ENSDF but in those cases when a transition is unmeasured and
data is lacking then there is a chance to ﬁt isomeric cross sections by assuming the probability distribution within a speciﬁc transition branch.
Listing 1. Example TALYS input ﬁle for 90Zr, using optimised parameters.
C.TENDL Neutron Data Library
In Table II, a comparison of the contents of TENDL with the other major world libraries is given. There are,
according to the ENSDF-based discrete level database of RIPL, 2813 target nuclides, in either ground state or iso- meric state, which are either stable or have a half life be- yond 1 second, and these are all included in TENDL. The nuclides range from 1H to 289Fl. Of these 2813 nuclides, 2808 can in principle be provided by TALYS, which does not accept  H and He isotopes as targets. These cover  all of the nuclides shown in Fig. 15, including 543 isomer targets. For comparison, the most complete alternative activation library besides TENDL, EAF-2010, considers
the ratio between (γ,n) and (γ,2n). Fig. 16 shows an ex- ample of TENDL photonuclear cross sections compared with other nuclear data libraries. All photonuclear data ﬁles of TENDL contain covariance information. Table III shows the nuclear data libraries we considered.
TABLE III. Photonuclear nuclear data libraries considered in TENDL for adoption and comparison.
only 816 targets.  However, as Table II shows, all impor-
tant stable light nuclides and the major actinides have been adopted from ENDF/B-VIII, which make integral testing of TENDL-2017 practically an integral test for all other nuclides. With the exception of these adopted ﬁles from ENDF/B-VIII, the evaluation method and ENDF- 6 structure of all isotopic ﬁles in TENDL are the same. The entire structure of such a TENDL ﬁle has already been extensively outlined in Ref.[53] and will not be re- peated here.  We note that the TENDL neutron library
comes with a full set of covariance data, i.e., MF31-35,40 are generally ﬁlled with covariance data.  Although tech-
nically possible, we have not included cross-isotope cor- relations.
D.TENDL Photonuclear Data Library
The photonuclear library is, next to the neutron library obviously, the only TENDL-2017 sublibrary which is no longer produced using global TALYS parameters. For TENDL-2017, in contrast to earlier versions of TENDL, an eﬀort has been made to adjust nuclear model parame- ters such that TALYS predictions match as good as pos- sible experimental (γ, n) , (γ, 2n) etc data. In almost all cases, this concerns minor adjustment of the Giant-Dipole Resonance parameters, which are available in the RIPL database as Generalized Lorentzian parameters for the photon strength function. In some cases, also the radius of the real volume optical potential and the level density for the ﬁnal nucleus were adjusted, especially to ﬁne-tune
E.
TENDL Charged Particle Data Libraries
For the TENDL charged particle libraries all calcula- tions are performed with default parameters, even though we are well aware that better ﬁts to experimental data can be obtained for several nuclides. These data libraries can thus be seen as an alternative to the intranuclear cas- cade codes used at energies up to 200 MeV, in which also no adjustment to experimental data takes place. There is still no eﬃcient method to collect experimental data for charged particle-induced reactions, and to adjust TALYS input parameters to obtain the optimal description for each isotope. The semi-automatic method of TALYS pa- rameter adjustment for the neutron library has not been applied to the charged particle library, simply because there is little guidance to what level of quality charged particle libraries should be evaluated for each material. Also, sensitivity analyses for charged particle applications such as accelerator shielding and medical isotope produc- tion are to our knowledge non-existent and do not trans- late in real evaluation needs (though there are modelling and experimental needs in that community). In short, they are considered less important than neutron libraries, which holds to an even larger extent for other incident charged particles. Fortunately, it is our experience that the diﬀerence between a global and a parameter-adjusted
evaluation is larger for neutrons than for charged parti- cles, i.e., for charged particles there is less to be gained, and libraries produced by default TALYS calculations al-
ready perform reasonably well. We note that due to re- stricted computer power, only the proton nuclear data li- braries of TENDL contain covariance data. For the other charged particles this will be included when a real need is expressed.
FIG. 15. (Color online) Chart of the nuclides showing the targets include in the TENDL-2017 and EAF-2010 neutron-induced reaction libraries. Note that TENDL includes all isomeric states with half life above 1 second, amounting to 544 of the 2813 ﬁles.
FIG. 16. (Color online) Comparison of photonuclear data li- braries and EXFOR data for the 100Mo(γ,n)99Mo reaction.
1.Proton Sublibrary
For incident protons, the most important, and most measured, observables are elastic scattering angular distributions, total non-elastic cross sections, double- diﬀerential particle emission spectra and residual produc- tion cross sections. The general quality of TENDL for these reactions is directly related to the quality of the optical proton models of Ref. [21], and pre-equilibrium exciton model of Ref. [34] As can be inferred from these two papers, a rather good description can already be ob- tained without nuclide-by-nuclide adjustment. Fig. 17 shows an example of TENDL proton cross sections com- pared with other nuclear data libraries.
FIG. 17. (Color online) Comparison of proton data libraries and EXFOR data for the 56Fe(p,n)56Co reaction.
An extensive description of the ENDF-6 procedures used to build a proton library has been given in [8]. The TENDL-2017 proton libraries diﬀers in two aspects from
that. First, similar to neutrons, we use explicit MT num- bers for all reactions up to 30 MeV, while shifting to a MF6/MT5 description at higher energies. Also, covari- ance data are included, for cross sections in MF33 and iso- meric cross sections in MF40. These include the full range of residual nuclides, as seen in Fig. 18 for 120Sn. This allows accurate study of the activation-transmutation ef- fects in systems with incident particle energies up to 200 MeV. Note that corresponding ﬁssion yield data is re- quired for those with open channels.
In [8], a global comparison between TENDL-2011 and various intranuclear cascade models, as implemented in MCNPX, turned out favorably for TENDL-2011. Since then, the default parameters sets of TALYS have im- proved and it is expected that the TENDL-2017 perfor- mance is therefore at least as good. This an assumption, as global data validation for proton data libraries and  other models has not been approached as thoroughly as for incident neutrons. Table IV gives a comparison with other existing libraries.
TABLE IV. Proton nuclear data libraries considered  in TENDL for adoption and comparison.
2.Deuteron Sublibrary
Compared to the last general publication about TENDL [8], there have been some signiﬁcant improve- ments in the TALYS prediction for incident deuteron, triton, Helium-3, and alpha data, which is naturally re- ﬂected in the quality of the libraries. The major contribu- tion to this improvement comes from the latest break-up model published by  Kalbach [65], which provide a bet-
ter description of e.g., (d,p) and (d.n) reaction channels and thereby automatically also the competing reaction
40
50	55
55
50
45
40
50	55
60	65	70
N (number of neutrons)
60	65	70
N (number of neutrons)
10-4
10-5
10-1
10-2
10-3
10-4
10-5
channels. This model is used in combination with her pick-up and stripping models [66] for all combinations of projectiles and ejectiles. The TENDL deuteron libraries ﬁnd their application in various ﬁelds, most notably med- ical isotope production, and shielding and activation of deuteron accelerators for the IFMIF project, which sim- ulates fusion neutron spectra. A global comparison with other libraries is given in Table V. At the moment, no channel-by-channel data are adopted from other libraries. In other words, with exception of the 5 isotopic data ﬁles adopted in their entirety from ENDF/B-VIII, all dataFIG. 19. (Color online) Comparison of TENDL deuteron data library and EXFOR data for the 56Fe(d,2n)56Co reaction.
TABLE V. Deuteron nuclear data libraries considered in TENDL for adoption and comparison.
FIG. 18. (Color online) Cross sections for all residual iso-
topes in proton irradiation of 120Sn, as extracted from the full MF6/MT5 data set of TENDL-2017.
come from default TALYS calculations. Fig. 19 shows an example of TENDL deuteron cross sections.
3. Triton Sublibrary
Of all the charged-particle libraries, the triton library is probably the least used, if at all. There is obviously an application in fusion reactor design for light targets, but TALYS does not handle those, while for medium and heavy nuclides there is almost no EXFOR data, and we are also not aware of any technological application of the TENDL library for tritons. Table VI shows what is avail- able.
TABLE VI. Triton nuclear data libraries  considered  in TENDL for adoption and comparison.
5.
Alpha Sublibrary
Besides TENDL, there are only a few nuclear data li- braries for alpha particles available, see Table VIII. There are a few relevant reactions which have been evaluated for ion beam analyses and medical isotope production. Perhaps the best known nuclear energy related applica- tion for alpha libraries is the (α, n) reaction in spent fuel. This has several important aspects for safeguards [67], and for this reason important data sets have been evalu- ated for the JENDL-2005/AN library. One of the future actions for TENDL is to adopt the evaluated reactions from JENDL-2005/AN channel by channel, thus keeping the original evaluation, and/or to do an authentic re- evaluation of the data, In both case, the TENDL struc- ture, and its applicability in various application codes, remains intact. In the last few releases of TENDL, the alpha optical model potential by Avrigeanu [28] has been used as default, improving the comparison of TENDL with experimental data for both alpha-induced reactions as reactions with alpha particles as ejectiles. Fig. 20 shows an example of TENDL alpha cross sections.
TABLE VIII. Alpha nuclear data libraries considered in TENDL for adoption and comparison.
FIG. 20. (Color online) Comparison of TENDL alpha data library and EXFOR data for the 89Y(α,3n)90Nb reaction.
FIG.  21.	(Color online) Excitation function for the
74Se(n,2n)73Se reaction.
103
143Pr(n,)144Pr
102
F.Diﬀerential Validation with EXFOR
In order to assure the reliability of an evaluation it    is essential to validate the cross section ﬁts at diﬀeren- tial and integral levels. Diﬀerential validation means a comparison of the evaluated data taken from TENDL- 2017 to currently available experimental data. For this reason the experimental data for neutron cross sections were taken from the entire EXFOR database as well as the curves from other evaluated neutron data libraries namely JEFF-3.2, JENDL-4.0, EAF-2010, and TENDL-
2015. Unfortunately the experimental data is mainly available in the range of 0 MeV to 20 MeV, which is linked to the amount of neutron sources used for measurement techniques. The ﬁtting procedure was performed in a way that the TALYS curve is in good agreement with the recent experimental data points. Every time when there was a change in the set of nuclear model parameters the ﬁnal plot was examined until the agreement was at a re- quired level. Figs. 21-24 show several comparisons for diﬀerent nuclear reaction channels. TENDL-2017 is rep- resented by solid black line labelled ‘TALYS’. The visual analysis is a prompt way of judging the achieved level of agreement. The experimental points that are out of the general trend were not taken into account. In some cases the agreement is good but still some deviations are ob- served and that is explained by the fact that there is a connection between calculated channels and, for instance, a large variation of (n,2n) may cause the increased dis- agreement to the other ones. Therefore it is important  to keep a balance between all channels in order to stay within physical limits
FIG.  22.	(Color online) Excitation function for the
143Pr(n,γ)144Pr reaction.
133Cs(n,p)133Xe
FIG.  23.	(Color online) Excitation function for the
133Cs(n,p)133Xe reaction.
FIG.   24.	(Color online) Excitation function for the	70
99Tc(n,α)96Nb reaction.
60
Special attention was paid to ﬁtting the isomeric cross	50
sections.  Data on cross sections for formation of short	40
and long-lived nuclear metastable states are relevant be-
cause these values can be used for determining decay con-	30
stants and internuclear transitions.  In case of isomeric	20
cross sections, ﬁrst of all there was a need to analyze
the level structure of the ﬁnal nucleus in detail and to	10
vary the probability of transitions between isomer and
95Mo(n,p)95gNb
ground states. Where the information on that  transi- tion probability was not given in experimental data ta- bles (incorporated into the TALYS system) the values could be estimated.  In order to introduce this option it is needed, ﬁrst of all, to set bestbranch n in the general TALYS input ﬁle and next to add additional input with the transition probabilities between the considered levels of the ﬁnal nucleus. The ﬁtting procedure includes ﬁxing the total cross section ﬁrst, and next with the variation of the transition probabilities, the ﬁtting was focused at metastable and ground states separately. In Fig. 25 there is the example of the 95Mo(n,p)95Nb, 95Mo(n,p)95mNb, and 95Mo(n,p)95gNb nuclear reactions. One of the most important steps of the evaluation procedure is a decision on which experimental set is the most credible for ﬁtting and should be taken for evaluation. Unfortunately it is hard to analyze all experimental data but in some cases when there were several discrepant experimental values, there was an attempt to ﬁnd a reason. The study of the publications related to measurements supported to un- derstand why some points are outliers as compared to the general trends. The diﬃculty to judge those mea- surements can be explained by lacking as well as insuﬃ- cient descriptions of the experiments. The most common problems are based on using wrong time information or technique with measurements of β-spectra which is less selective than γ-spectroscopy. During the preparation of TENDL-2017 some eﬀort was applied in order to ﬁnd out the arguments of outliers in support of the current eval- uation. For example, for the 170Er(n,p)170Ho reaction there are experimental data measured for formation of
FIG. 25.  (Color online) Excitation function for the
95Mo(n,p)95Nb reaction.
isomeric and ground states.  In case of 170Er(n,p)170gHo a diﬀerence of one order of magnitude was observed be- tween data of Luo [69] comparing to measured by Dzy- siuk [70] (Fig. 26). The most evident explanation of such a diﬀerence is that Luo did not account for an interfering reaction, namely 162Er(n,2n)161Er. They claimed their use of the 931.4-keV γ-line from 170gHo decay for de- termination of this cross section, but for an interfering
reaction 162Er(n,2n)161Er the decay of 161Er is accompa-
nied by the emission of a 931.7 keV from 161Ho.   Due
to the high value of the 162Er(n,2n)161Er reaction cross
section with respect to (n,p) reactions for the considered
energy range, an essential increase in the cross section of
interest  170Er(n,p)170gHo  might  be  observed. As could
be seen in Fig.  6 the JEFF curve is in agreement with
the upper experimental point and we  would recommend
to reconsider that.  The current evaluation (TALYS black
curve) is more reliable. For the 100Mo(n,p)100Nb reaction
FIG. 26.  (Color online) Excitation function for the
170Er(n,p)170Ho reaction.
there are also only two experimental data points. After analysis of the measurements, the decision to include the Amemiya [71] data into the evaluation was made. The data of Cuzzocrea [72] was not taken into consideration as the authors used the wrong information for the T1/2 of the reaction product. The actual half-life of ground state of 100Nb is 1.5 s (Fig. 27) but in the paper there      is a description of the measurement that resulted in a half-life of about 3 min. That most possibly caused an overestimation of the cross section value.
FIG.   27.	(Color online) Excitation function for the
Diﬀerential validation is a very eﬃcient and practical approach to analyze each reaction channel individually and it helps to increase the total accuracy of evaluation. However a fully comprehensive validation can not be ac- complished due to the limitation of available diﬀerential as well as integral nuclear reaction cross section measure- ments at the considered energy range (0-30 MeV). For that reason it is relevant to prioritize the experimental needs on appropriate lists, such as the High Priority Re- quest List, which can then be combined with modelling eﬀorts.
G.Validation with Compiled Experimental Data Sets
1.Thermal Cross Sections
Cross sections for thermal Maxwellian spectra or diﬀer- ential measurements are the subject of many experimen- tal measurements and these data have been compiled into
databases such as the Atlas of Neutron Resonances [12]  and the CRC Handbook [73]. A review of these data, with a cross-comparison using EXFOR, was performed at the UKAEA, producing an amalgamated database of thermal cross sections [74]. In a few cases, discrepancies between these databases were dealt with by direct interrogation of the cited publications, or review of the most recent data in EXFOR.
The ratio of calculated (TENDL-2017) to experimental cross sections in this thermal cross section database are shown in Fig. 28. The distribution is very closely cen- tred around perfect agreement at 1.0, although some few errors are present. Notably, some reaction channels are absent in the adopted ﬁles from ENDF/B-VIII.0, such as the 233U(n,α) channel. The majority of all values that  are not in perfect agreement are due to partial, isomeric production cross sections. A system to handle isomeric production is in development for future versions.
FIG. 28. Distribution of TENDL-2017 thermal cross section to experimental cross section ratio (C/E) using the UKAEA thermal cross section database.
2.Resonance Integrals
Resonance integrals provide a standard, simple method for comparing reaction probabilities in the resonance range, which are due to potentially many hundreds or thousands of parameters. The standard integral is calcu- lated in the slowing 1/E spectrum, as
@23
where the integration limits are subject to modiﬁcation
for each experimental measurement. Typically values are E1 = 0.5 eV and E2 = 100 keV. The reference set for these data is the Atlas of Neutron Resonances. The data
in  the  Atlas includes  two  options,  the  calculated data
based on the resonance parameters provided in that  text,
or experimental data. In the cases where these are in disagreement, such as the 196Pt(n,γ), experimental values are selected.
The distribution of TENDL-2017 calculated resonance integrals to reference resonance integrals is shown in Fig. 29. These are helpful, integral quantities to com- plement evaluation of resonance parameters and provide a veriﬁcation check. As the majority of the resonance in- tegral for measured nuclides are due to resonances with evaluated parameters, this eﬀectively provides a test for the TARES resonance parameter database. It also pro- vides a veriﬁcation check for the High-Fidelity Resonance generation (see Section II B), where these resonances are a signiﬁcant contribution of the total integral.
3.MACS and the KADoNiS Database
The importance of neutron capture to stellar astro- physics, through the nucleosynthesis s-process, has led to
tions and are not experimental. These are instead code to code comparisons, although they provide a veriﬁcation check for nuclear data libraries.
For TENDL-2017, the capture cross sections are pro- vided with full MF=33 covariance data. In the unre- solved resonance range, this data comes from Bayesian Monte-Carlo TALYS evaluations and within the reso- nance range a novel approach using SAMMY has been employed. For more details, see Section IV and Sec- tion IV B, respectively. These covariances can be used to calculate the uncertainty in the spectrum-averaged cross sections, as shown in Fig. 30, as a continuous plot over the temperature-dependent averaged cross sections.
The overall performance of the TENDL-2017 library, as compared with the full KADoNiS database, is shown in Fig. 31. Most of the well-known nuclear data li- braries exclude several of the KADoNiS isotopes, result- ing in the ‘<1/10’ peaks on the left-hand side. That TENDL contains complete nuclear data to cover all of these isotopes is well-known, but TENDL contains reso- nance evaluations that best match these MACS measure- ments and combines this will covariance evaluations that cover all incident neutron energies. KADoNiS contains several datasets that are not experimentally determined, but approximately 22% are derived from some statistical model calculations, including values obtained from the
FIG.  30.   (Color  online)  Temperature-dependent  MACS  for Fe55,  including  temperature-dependent,  full  covariance  un- certainties calculated using TENDL-2017 and Fispact-II.
NON-SMOKER code [77]. The agreement found in this case does not provide direct validation with experimental data, yet the cross-validation with diﬀerent model calcu- lations provides a measure of veriﬁcation in the absence of experiment.
FIG. 31.  (Color online) Comparison of C/E distributions  for all 357 KADoNiS 30 keV cross sections with TENDL- 2017, JENDL-4.0 and ENDF/B-VIII.0 values calculated with maxwav. C/E values for missing nuclides in JENDL-4.0 and ENDF/B-VIII.0 are tallied in the <1/10 bin.
VI.ADJUSTMENT
The adjustment of the calculated nuclear data is a necessary step for many libraries. In a usual evalua-  tion scheme, the evaluators will spend a large amount of time in ﬁnding the right model parameters to reproduce diﬀerential data, often from the EXFOR database [1].
Once this is done, there might still be discrepancies be- tween calculated cross sections and the measured ones. The next step would then be to change the calculated values to be more in agreement with the desired values. Such changes can be done by hand, or using some more
advanced methods (e.g. Kalman ﬁlter [78], GLLS [79], Bayesian [80, 81]). In the next section, this step is de- scribed under the adjustment via diﬀerential data. Once since step is performed, a true general purpose nuclear
data library is obtained, meaning that it has not been targeted, or adjusted to speciﬁc particular applications. This was up to TENDL-2015 the general adopted ap- proach in TENDL.
An additional step of adjustment is generally taken for almost all libraries, i.e., ﬁnal modiﬁcations to obtain good  performances for a selection of integral benchmarks. His-torically, these benchmarks have been a selection of crit- ical experiments, most often restricted to the calculation of keﬀ . There are many reasons for these choices, but the most obvious one is the availability of these benchmarks in terms of simulation inputs and the relatively ease to use them. It can be noted that this is slowly changing: not only keﬀ are being analyzed, but also spectral indexes. Additionally, other types of benchmarks are now used: shielding and reactor benchmarks. This second step of adjustment is making sure that a library  is also perform-
ing well with this selection of integral benchmarks. This being done, the updated ﬁles are no longer as general as before, but they are still claimed to be of general appli- cation. In the case of the TENDL library, this approach
was taken for TENDL-2017, where the major actinides and light elements were adopted from ENDF/B-VIII.0.
There is another advantage of using integral bench- marks  during  the  evaluation  procedure: the  reduction  of calculated uncertainties. By using some  well  se-  lected benchmarks, cross-correlations between certain re- actions and isotopes  appear, together  with  a  reduction  of uncertainties for speciﬁc channels (see for instance Refs. [82, 83]).
In the following, a practical example will be presented with 56Fe with a few steps adjustment, for future releases of TENDL versions. The steps to be undertaken will be the following:
1.Produce prior based on model knowledge,
2.Based on the prior, update with diﬀerential data,
3.A second adjustment step is done with integral data.
This simple description has the advantage to clearly sep- arate the diﬀerential and the integral adjustments. Such approach is also currently under discussion for the prepa- ration of the JEFF-4.0 library, where one possibility will be to produce two sets of ﬁles, A and B, the ﬁrst one would correspond to the EXFOR adjustment, the second one to EXFOR plus integral data.FIG. 32. (Color online) Example of random 56Fe(n,p) cross sections based on the variations of model parameters (top) and on the variations of model parameters and of models (bottom).
A.Prior Nuclear Data
As explained above, the prior nuclear data can be pro- duced in TENDL based on the “T6” code package. The output of such calculation is one single ENDF ﬁle (for in- stance for 56Fe), solely based on the knowledge included in the T6 models, and on the global (and sometimes lo- cal) adjustment of model parameters. An example for 56Fe(n,p) is presented in Fig. 32 (top). Such random curves are obtained by varying in a random way all model parameters (optical model, pre-equilibrium, level densi-ties, etc.,) but the model themselves are ﬁxed. The re- sults for the (n,p) reactions are representative of the other
channels. As observed such spread of curves have a lim- ited degree of freedom: they are more or less parallel and do not cross each other.
To increase the variability of the random curves, one can also change the models together with the parameters: optical model, level densities, mass models, etc. Table IX
presents the list of the models which are varied to in-
crease the dispersion of the prior.  For details on these
TABLE IX. Example of the models which are randomly changed in the TALYS calculations to create a large spread in the prior calculations. This example represents the 56Fe calculations presented in the following. OM means optical model.
models, see Refs. [8] and the TALYS manual.  The com-
bination of some of these keywords has also to be done in a proper way, otherwise some of them will have no eﬀect. In Fig. 32, one can see two groups of calculations in the case of the variation of models. The main diﬀerence be- tween these two groups is the change of the mass model: the highest cross sections near the threshold are obtained
with theoretical masses based on the Gogny force (key- word massmodel equal to 3).
The results are presented in Fig. 32 (bottom): the
spread is indeed larger (larger uncertainties and some of the random curves are crossing each other). To also vary models is not usually done when creating prior curves, but it certainly leads to a better coverage of possibili- ties. One can say that such model variations are similar in the experimental side to considering diﬀerent types of measurements for a speciﬁc reaction.
The variation of models for the prior also have con- sequences for the prior correlation matrices. Similar to Fig. 32, the (n,p) correlations are presented in Fig. 33.
FIG. 33.(Color online) Example of 56Fe(n,p) correlations based on the variations of model parameters (top) and on the
variations of model parameters and of models (bottom). The
x and y axis are from 1 to 20 MeV.As expected, correlation matrices are diﬀerent, due to the use of diﬀerent models. This is emphasizing again that covariance matrices are coming from the assump-tions made during the calculation procedure.  They do
not represent absolute truth.  By repeating this for all important channels, one can generate prior realizations which cover a large part of the possibilities. Naturally, the next step will be to compare such prior cross sections with diﬀerential data.
B.Inference via Diﬀerential Data and Monte Carlo
Based on the above curves, one can make a selection of EXFOR data and compare them with the prior cal-  culations. Of course this is only the ﬁrst step towards producing an adjusted library, and users may not want to deal with hundreds of such cross sections for one iso- tope. There are at least two  convenient ways to produce a posterior, once the EXFOR selection is made.
The ﬁrst one was presented in Refs. [84, 85] and was subsequently called the Petten method. It simply consists in comparing the random curves with the diﬀerential (or integral) data and to select the random ENDF ﬁle which is in best agreement with EXFOR. This is equivalent to calculate a simpliﬁed or generalized χ2 for each random ﬁle, and to select the one with the smallest χ2. Such method is extremely simple but has the disadvantage to not providing covariance data. One could still, based on the best ENDF ﬁle and its T6 input parameters, calcu- late a covariance ﬁle by sampling the parameters used to produce this best ﬁle.
A second approach is based on Bayesian updating. It was presented in a few papers, with its origin Bayesian Monte Carlo (BMC) deﬁnition [36, 81, 86], or with practical changes within the Backward-Forward Monte Carlo [87] (BFMC). Such methods consist in calculat- ing (again) χ2 values for each random ﬁle and to derive weights for these ﬁles. As explained in Section IV, in the BMC method, the weight wi for a random ﬁle i is calculated by
@24
while in BFMC, the weight w is
@25
The second deﬁnition avoids having a very large spread of χ2 values due to good, bad and very bad agreement be- tween diﬀerential data and random ﬁles. The deﬁnition of the χ2 is not of prime importance for the following, but one can consider the simpliﬁed χ2 (without experimen-  tal correlations) or the generalized χ2 (with experimentalcorrelation). These experimental correlations are diﬃcult to obtain and require a precise study of all experimental set-up. In this case, and considering that the underly- ing reaction models are correct, the BMC method is the most appropriate choice. Otherwise, in order to compen- sate for missing (experimental) correlations and defaults in the reaction models, the BFMC method oﬀers a practi- cal solution. By normalizing with the minimum χ2 (called χ the distribution of weights is not as widely spread as in BMC. Based on these weights (one for each random
ENDF ﬁle), one can calculate cross section weighted av- erage, standard deviations and correlations, as in Eqs. (15)-(15).
Examples for the 56Fe(n,p) cross section are presented below in the case of the BMC and BFMC approaches: Figs. 34 for the cross sections and uncertainties and 35 for the correlation matrices. As expected, the prior provides a large range of possibilities, as in the case of the (n,2n) cross section. Because of the spread from the models, the average prior is also relatively away from the experi- mental data. One should notice that the prior spread is much larger than in the conventional TENDL approach FIG. 34. (Color online) Example of posterior 56Fe(n,p) cross sections and uncertainties based on BMC and BFMC. The grey band represents the prior uncertainty.
where only the model parameters are varied. It was nev- ertheless repeatedly noticed that by selected only one set of models, a number of experimental data could not be represented by random calculations; such deﬁciencies is up-to-now compensated by so-called model defects, with the use of Gaussian processes [41, 87, 88]. As presented in Figs. 32 and 34, another advantage to vary models (such as mass models) is to allow the threshold of the reactions to increase or decrease. Additionally, the changes from
the pre-equilibrium models also helps to obtain a better agreement at the threshold or reactions such as (n,2n) and (n,p). In Fig. 34, it is clear that the ﬂexibility from the prior random cross sections helps to obtain a poste- rior in better agreement close to the (n,p) threshold.
As expected, the posterior cross sections and uncer- tainties, taking into account diﬀerential data, are quite diﬀerent than the prior. The average cross sections (for both BMC and BFMC) are in closer agreement with the experimental data, and the uncertainties are strongly re- duced. As observed, the BMC method leads to smaller uncertainties than the BFMC method (as also observed in Ref. [47]). Small diﬀerences appear for the posterior averages, which are still within the 1-sigma uncertainty bands represented in the ﬁgure. In the present case, both BMC and BFMC are very eﬀective to produce posterior distributions (being in fact simply prior random calcula- tions with speciﬁc weights) in agreement with the exper- imental data. In a similar way, the posterior correlation matrices can be produced and are presented in Fig. 35. Both of these correlation matrices are relatively diﬀerent and one can also notice that the prior correlations are hardly subsisting.
One can still notice in Fig. 34 that some experimen-
FIG. 35. (Color online) Example of posterior 56Fe(n,p) cross section correlations based on BMC (top) and BFMC (bot- tom).
tal data cannot be ﬁtted, especially below 15 MeV. This means that the variations allowed by the set of selected models does not provide enough ﬂexibility to lead to an agreement with these data. One solution which is cur- rently being explored is also to sample models as well, and not only model parameters. This will certainly en- large the possibility space and provide a better match with experimental data. But such work is not yet at the stage to be included in a library.
For a full evaluation as performed in the TENDL li- braries, many channels are considered together. Also, diﬀerent isotopes of the same element can be analyzed together, for instance including experimental data on nat- ural targets. The use of natural diﬀerential data will have a similar eﬀect compared to using integral benchmarks: it is leading to the creation of cross-isotope correlations for speciﬁc reactions.
C.Inference via Integral Benchmarks and Monte Carlo
The use of integral benchmark in nuclear data evalua- tion is still a debated question. As explained in Ref. [47], the main drawback in avoiding integral experiments in the evaluation process is that the calculated uncertain- ties for these benchmarks, taking into account diﬀerential data solely, are larger than the reported integral bench-
mark uncertainty. This is not a fundamental problem per se, but nuclear data users from large-scale facilities do not always appreciate the origin of such diﬀerences. One
reason for this diﬀerence may be the omission of correla-
140
120
100
80
60
40
20
0
-2700
-1620
-540
540
1620
2700
tion between important data which is not available from
measurements. Another drawback in not clearly includ- ing some integral benchmarks in the evaluation process is that it is done anyway, but without explicitly stating it. Unfortunately, when an adjusted library is produced by other experienced people (see for instance Refs. [79, 89]), some benchmarks are therefore used again in the adjust- ment process, therefore biasing the results towards spe- ciﬁc benchmarks.There are also possible drawbacks in including integral benchmarks in the evaluation process. The selection of such benchmarks cannot be done blindly and needs to be representative of a large variety of applications. It should not be restricted to criticality benchmarks only, and at least makes use of spectral indexes, shielding benchmarks,
and possibly reactor experiments.  The main diﬃculties in using a variety of integral systems are the availability of computer models, the possibility to run them in a rea- sonable amount of time, and a convenient way to compare calculated and measured values. For these reasons, the nuclear data community has for a long time restricted it- self to keﬀ calculations for systems deﬁned in the ICSBEP database [90].
The TENDL libraries were up to 2015 free of integral adjustments. This was achieved by only considering dif- ferential experiments, with or without Bayesian adjust- ments. As observed, this has lead to good agreement with EXFOR data, but to a limited one with criticality bench- marks. Additionally, the calculated uncertainties with TENDL were in general larger than the ones coming from other libraries. Even if this can present some advantages, the TENDL-2017 release (and probably the future ones) did not follow this approach for some isotopes, and a lim- ited number of evaluations were adopted from ENDF/B-
VIII.0 (e.g., 235,238U and 239Pu). The consequence is a stronger agreement with ICSBEP benchmarks.
The 56Fe evaluation was not adopted from another li- brary in any TENDL release, except for the elastic an- gular distributions (MF4 MT2) which was directly taken from JENDL-4.0 [56]. In the following, a simple example of the use of keﬀ integral benchmarks is presented, with the BFMC method in order to rank random 56Fe realiza- tions. The ﬁrst step was presented in the previous sec- tion: producing a large amount of random 56Fe ﬁles; the second step is to use these random ﬁles with a selection
keﬀ  spread (pcm)
FIG. 36. Examples of the calculated keﬀ spread for two bench- marks highly sensitive to iron. All nuclear data of 56Fe are varied, using about 2400 random ﬁles, with diﬀerent models and model parameters.
of benchmarks. Here, only two criticality benchmarks are selected, highly sensitive to iron: the hmf13 and hmt13-
02.The ﬁrst one is a sphere of highly enriched uranium reﬂected by steel and the second one is a highly enriched uranium system including an iron plate and moderated by polyethylene. Examples of the spread of the calcu- lated keﬀ are presented in Fig. 36, where the spread is calculated around the mean keﬀ values.
One can see that the variation of the 56Fe nuclear data has a large eﬀect on the keﬀ of these two benchmarks. For the thermal benchmark, the standard deviation is about 400 pcm, which is not negligible, and in the case of the fast benchmark, the eﬀect is much larger, with a standard deviation of 770 pcm. What is more striking for the fast benchmark is the double-hump distribution of the keﬀ , due to the use of very diﬀerent optical models in the fast neutron range. Such results come from the method pre- sented above, and such approach unfortunately leads to average keﬀ values which are not in agreement with the benchmark values, as presented in Table X. The bench- mark  keﬀ  values are 0.99900	0.00150 and 1.00060 0.00220 for hmf13 and hmt13-2, respectively. The cal-
TABLE X. Characteristics of the probability density functions for both benchmarks, varying the 56Fe nuclear data. “All random MF” means that the full 56Fe ENDF ﬁles are random from one calculation to another. Other isotopes come from the JEFF-3.3 library.
culated values are not in agreement with the benchmark
values, and possibly not only because the 56Fe nuclear data are not correct.   Still,  the  calculated uncertainties
point to a possible room for improvements.
A simple way to check the origin of these uncertaintiesis to repeat the above calculations, this time ﬁxing some
nuclear data.  This was done by  producing  random 56Fe nuclear data ﬁles with non-random angular distributions (MF4).  The MF4 was taken from the JENDL-4.0 library.The results of such new calculations are also presented inTable X and in Fig. 37. As observed, the average keﬀ
value  for the fast benchmark is closer to  the benchmarkvalues, and the uncertainties are sensibly  reduced (espe-cially for hmf13), indicating that the elastic and inelasticangular distributions have a relatively important  impact.
Note that the double hump for the hmf13 benchmark hasdisappeared. Such double-hump distribution is simply the reﬂection of the large variations of the prior nuclear data distribution. The main information that we can ex- tract is an indication that the angular distributions play an important role for this criticality benchmark; this can be related to the similar expected sensitivity in the case of shielding benchmarks (for instance the ASPIS bench- mark), pointing out at the necessity of a dedicated eﬀort towards better evaluated angular distributions (mainly through better modeling).
If we consider that the prior 56Fe random nuclear data ﬁles were covering a large part of the likelihood, and that other nuclear data important for these benchmarks are well known, it can be proposed that the current theoreti- cal knowledge for 56Fe (including its angular distribution) is not adequate. In this case, the compensation of calcu- lated nuclear data with speciﬁc approaches using model defects can be very useful.
VII.PROCESSING ROUTES
An important aspect of the quality and usefulness of any evaluated ﬁle resides in its ability to be properly parsed through processing codes prior to being used. This could take many forms - use in solvers for transport (Boltzmann) or Bateman equations or simply to be displayed to a human eye and compared with experi- mental data. It is necessary to demonstrate the ability
keﬀ spread (pcm)
FIG. 37. Same as Fig. 36, but keeping MF4 ﬁxed (no variation of the angular distributions).
to successfully convert evaluations into formats useful for the many applications that need them.
Depending on the applications, the data formats have quantitative and qualitative requirements that are not alike or even always compatible. The cross section re- construction accuracy criteria, for example, are diﬀerent for simple display and for pointwise cross sections used in Monte Carlo transport. The strength behind a uniform data structure format for all evaluations is that it allows the use of single script that contains all necessary input sequences to process an entire library. This uniqueness in processing also allows one to compare the data on an equal footing. The general processing workﬂow used with TENDL is shown in Fig 38.
A.PREPRO
The PREPRO suite contains several codes that pro- vide data pre-processing functions to transform the raw ENDF-6 data into diﬀerent data and formats that may be used by some application codes. Each code performs
FIG. 38. Enhanced processing steps, three codes NJOY, PRE- PRO and CALENDF [10, 11, 91].
one or more independent operations on the data while reading and writing structured data formats respecting the ENDF-6 format frame at any stage.
The high-energy (> 30 MeV) part of the ﬁle stored as MF-3/MT-5*MF-6 is parsed by the sixpack module that extracts the production cross sections before embedding them as an MF-10 MT5. Furthermore, gas production, kerma and dpa information uniquely produced by NJOY from the same original data ﬁle are also carefully stacked into the ENDF-6 formatted ﬁle before the relevant data formats, the cross sections and other derived quantities, are group averaged by the groupie module.
1.Data Formats for Applications
One important aspect of nuclear data has been the for- mat and structure in which basic physics information could be stored, distributed and used in applications. These basic physics data must be stored in evaluated ﬁles and then processed into engineered applied data sets. The TENDL formats are multiple but coherent, originat- ing from the same technological setup. They serve many diﬀerent research ﬁelds ranging from astrophysics to med- ical; earth to life sciences; fusion and accelerator-based systems. Those formats can be categorized as:
physic tables, X-Y format for cross-sections, emit- ted spectra and angular distributions;
S0 ENDF-6 formats, single reaction channel: MF- 3/MT-5*MF-6 to 200 MeV;
S30 ENDF-6 formats, multiple reaction channels: MF-3/MT-1 to 891*MF-4/MF-5/MF-6 to 30 MeV,
then as S0 afterwards; and
model	based	variance-covariance	information, ENDF-6 formats MF-32, -33, -34, -40 and -31, -35
for the actinides.
Most of the above exist for α, γ, 3He, triton, proton and neutron incident and would need to be manipulated and/or processed to be used in publications or applica-tions. The applications are wide ranging but when fed into a modern simulation platform such as FISPACT-II [92, 93], MCNP [94], SERPENT [95], and TRIPOLI [96],
those enhanced, complete data formats enable detailed and probing study of nuclear phenomena. This allows   a more reliable and safe study of  scenarios  that  are  less well studied – particularly those beyond the legacy
systems already commissioned. This is most valuable
when no experimental information exists to pave the way. The applications formats can be described as:
•pointwise or groupwise temperature-dependentcross section;
•background, dilution cross section;
•double diﬀerential cross section;
•total or partial channel cross section, macro-partial;
coherent, incoherent, elastic, inelastic, bound cross section;
•cross section for production of radioactive nuclide;
•isomeric branching ratio;
tabulated and/or legendre polynomial angular dis- tribution;
function based and/or tabulated particles and recoil emitted spectra;
•primary and secondary, prompt and delayed data;
•resonance parameter;
•probability distribution function;
•Bondarenko, probability table self-shielding factor;
•damage energy metrics, particles heating;
A< 4 particles plus residual production matrix (yield-energy,angle); and
•covariance-related quantities for many of the above.
B.NJOY Processing
The nuclear data evaluations are physics represen- tations of the data encoded in the uniﬁed, computer- readable format structure. They need to be converted into suitable data formats for applications, such as trans- port, activation-transmutation, criticality or shielding simulations using either pointwise or multi-group repre- sentations. The NJOY12 [42] or NJOY2016 [91] codes have been designed and maintained over decades to han- dle the uniﬁed format and so feed the many nuclear codes that rely on nuclear data. These include various modules to produce diﬀerent outputs for these applications, such as HEATR, LEAPR, THERMR, and GROUPR.
With the full residual emitted particle spectra from TALYS stored in the TENDL data ﬁles, NJOY may be used to extract this information, as shown in Fig. 39.  When coupled with codes such as SPECTRA-PKA [97], this uniquely allows users to extract the full primary knock-on atom spectra for materials damage studies. An example for aluminum is given in Fig. 40.
The diﬀerent NJOY modules can be sequenced to out- put nearly all the necessary information required for many modern codes used in R&D, but also the legacy ones deployed by the nuclear industry worldwide. Its most useful and unique formats are the thermal S(a,b) or free gas, heating and displacement energy, primary knock on atoms spectra, matrices of emitted particle and recoil, probability tables for the URR and ACE (A Com- pact ENDF) that serve several Monte Carlo simulation codes.
C.CALENDF Processing
The CALENDF [10] nuclear data processing system is used to convert the evaluation deﬁning the cross-sections in  ENDF-6  format  (i.e.,  the  point-wise  cross-sections
and the resolved and/or unresolved resonance parame-
ters) into formats useful for applications. Those formats used to describe the neutron cross-section ﬂuctuations or resonances correspond to cross-section probability tables based on Gauss quadratures and eﬀective cross-sections. CALENDF accesses the data stored in MF-2 (reso- nance parameters) and MF-3  (point wise cross-sections) of the ENDF-6 data ﬁle provided as input, ignoring all other MFs. ‘Ladders’ of resonance parameters are gener- ated into deﬁned energy ‘zones in the unresolved range, which are then treated in the same was as the resolved range.	The random ladders of resonances are gener- ated with energies extracted from a  table  of  1185  val- ues that are the eigenvalues of a random matrix (Dyson and Mehta) and with resonance widths chosen from a distribution laws with stratiﬁed and antithetic sampling. Additional resonances are added below and above the range of the energy zone to properly handle edge eﬀects. Checks of the consistency of the evaluated MF-2 reso- nance  parameters  are  performed  and  messages  emitted
when necessary.
Within the resonance range the cross-sections are cal- culated from the resonance parameters described in the ENDF ﬁle using diﬀerent formalisms. All the diﬀerent formalisms can be handled, but when judged necessary, a slightly modiﬁed multi-level Breit and Wigner formal- ism (Multi Niveau Breit and Wigner MNBW = MLBW) is applied by CALENDF in this energy range allowing the pointwise cross-sections to be reconstructed with psi-
chi Doppler broadening. There are no normalizations done except for the cases with LSSF=1.  The moments  of these cross-sections are computed, and the probabil- ity tables deduced from them. The table orders will mainly depend on the required accuracy, with a max- imum of 11. The probability tables for all zones and ladders are then merged to get the ﬁnal table in each en- ergy group. The Gauss-quadrature mathematical princi- ple gives those probability tables their sturdiness, allow- ing many utilitarian operations such as table condensa- tion, isotope mixing, or interpolation to be performed. In the URR range CALENDF applies the “statistical hy- pothesis” based on the fact that the resonances can only be statistically described.
VIII.BENCHMARKING
A.ICSBEP
The neutron library can be tested by making use of the criticality safety benchmarks available in the In- ternational Handbook of Evaluated Criticality Safety Benchmark Experiments, a product of the International Criticality Safety Evaluation Project (ICSBEP) [100]. This was done for the recently released library versions ENDF/B-VIII.0 [4] and JEFF-3.3 [55], and is repeated here for TENDL-2017. All benchmark runs were per- formed using MCNP version 6.1.1 [94].
The number of benchmark cases used was 2528, which is why the results are not all listed individually here. In- stead the average results for all these calculations are summarized in Tables XI–XII, for each main category of ICSBEP.
Based on a close inspection of the results for the indi- vidual benchmark cases, the following can be said.
The average performance of the library is good. This is mainly due to the fact that the evaluations for the major isotopes were adopted from ENDF/B- VIII.0.
There are many cases where the results are also strongly inﬂuenced by isotopes other than the ma- jors, such as Fe  isotopes,  or Gd isotopes.  In most  of those cases, the results are good.
For some elements, it can be observed that the TENDL-2017 evaluation inﬂuences the criticality safety results negatively. These elements are nickel, copper, lead, and thorium.
When  the  evaluations  for  Ni  isotopes  are  replaced by ENDF/B-VIII.0, the criticality results are lower, on average, by 1200 pcm.  This average is based on the  benchmarks  pu-met-fast-014  (simple  case), pu-met-fast-045  (detailed  cases  1   7),  and  heu- met-fast-003 (case 12).
When the evaluations for Cu isotopes are replaced by ENDF/B-VIII.0, the criticality results are lower,
FIG. 39. (Color online) Q positive (7.3 MeV) (n, α) 184W, residual 181Hf (blue), emitted 4He (beige) energy spectra [98], neutron incident energy in red, note the alpha energy superior to the neutron incident one, the spectra truncation above the 30 MeV upper energy. Figure taken from Ref. [92].
TABLE XI. The average value of C/E 1 in pcm (100 pcm=0.1%) for TENDL-2017 per main ICSBEP category for compound (COMP) and metal (MET) systems.  Also listed is the standard deviation of the C/E   1 values per main ICSBEP category,   except for the categories with only benchmark case. Shown in italics is the number of benchmarks in that category.
on  average,  by  660  pcm.    This  average  is  based on the benchmarks heu-met-fast-072, heu-met- fast-073, and heu-met-inter-006 (cases 1−4).
When the evaluations for Pb isotopes are replaced by ENDF/B-VIII.0, the criticality results are lower, on  average,  by  630  pcm.    This  average  is  based on the benchmarks het-met-fast-027, het-met- fast-057   (cases   1−6),   and   het-met-fast-064 (cases 1−3).
An attempt was also made to analyze the data in a dif-
ferent way, to obtain information per element. The tool DICE, which comes with the ICSBEP package, was used to ﬁnd benchmarks that are sensitive to speciﬁc elements. The focus was on the elements from Na through Th, since the evaluations for elements H through F were adopted from ENDF/B-VIII.0. In ﬁrst instance the minimum for the sensitivity was set low, which led to too many bench- marks per element. In this situation one can only analyze the average of those benchmarks, which does not reveal anything new. However when the minimum sensitivity was set higher, the number quickly reduced to zero for FIG. 40. (Color online) Isotopic and elemental residual Primary Knock on Atoms energy, n-induced on aluminum, extended during processing at low energy for non-elastic events [99]. Figure taken from Ref. [92].
TABLE XII. The average value of C/E 1 in pcm (100 pcm=0.1%) for TENDL-2017 per main ICSBEP category for solution (SOL) and miscellaneous (MISC) systems. Shown in italics is the number of benchmarks in that category.many elements. It has proven diﬃcult to ﬁnd a useful setting for the minimum sensivity, not least because this setting needs to be diﬀerent per element: for a strong absorber such as B or Gd, the sensitivity is much higher than for an element with low cross section such as Al, even though Al may be present in high amounts in the experimental setup. More time is needed to retrieve ele- ment speciﬁc information from the benchmark results in this way.
Another way to summarize all criticality safety bench- mark results is shown in Fig. 41. For this ﬁgure, a his- togram was made of all the C/E 1 results (expressed in units of a standard deviation) of benchmark cases with
e.g., a thermal spectrum. Subsequently a normal distri- bution was ﬁtted to this histogram.  If the nuclear data
were perfect (and the benchmark evaluations were per- fect), the distribution of C/E 1 would be the normal distribution with average zero (0) and standard devia- tion one (1). The ﬁgure shows that for thermal spec- trum cases, the performance of TENDL-2017 is close to that of JEFF-3.3 and ENDF/B-VIII.0. For the fast spec- trum cases, the performance of TENDL-2017 is close to that of JEFF-3.3, and only marginally less than that of ENDF/B-VIII.0. For the intermediate and mixed spec- trum cases there are not enough benchmark cases to draw ﬁrm conclusions, but the bias in keﬀ  calculations is roughly the same for TENDL-2017 as for the other libraries.
In sum, Fig. 41 can be regarded as a challenge for statis- tically consistent predictions from nuclear data; improve- ment in criticality estimation is represented by smaller deviations from the exact Gaussian distributions.
B.Shielding
A number of shielding benchmarks were used to test TENDL-2017. The benchmarks used were FNS  [101, 102], Oktavian [101, 103], and  the  LLNL  Pulsed Spheres [104], all of which start from a fusion neutron source. For the Oktavian benchmark,  for  each  element the neutron leakage spectrum was measured for a single case of material thickness. For the LLNL Pulsed Sphere benchmark, the neutron time of ﬂight was measured for, with most elements, two or three thicknesses of shielding material. For the FNS benchmark, several  thicknesses were used, as well as ﬁve diﬀerent angles.
The elements for which benchmark calculations were performed are Be (FNS, LLNL), Li (LLNL) C (FNS, LLNL), N (FNS, LLNL), O (FNS, LLNL), Mg (LLNL),
Al (Oktavian, LLNL), Si (Oktavian), Ti (LLNL, Okta- vian), Cr (Oktavian), Mn (Oktavian),  Co  (Oktavian), Cu (Oktavian), Zr (Oktavian), Mo (Oktavian), Fe (FNS, LLNL), W (Oktavian), Pb (FNS, LLNL, Oktavian).  The materials for which benchmark calculations were performed are H2O (LLNL), D2O (LLNL), polyethy- lene (LLNL), teﬂon (LLNL), LiF (Oktavian), concrete (LLNL).
FIG. 41. (Color online) The distribution of C/E for criticality safety benchmarks, in units of the combined benchmark and statistical uncertainty. The normal distribution (in black) would be the perfect situation.
The results of all these calculations were compared with results based on ENDF/B-VIII.0 and JEFF-3.3. In al- most all cases the diﬀerences were small. Noticeable dif- ferences occurred for only two elements, viz. Mg and Fe, as shown in Figs. 42 and 43.
FIG. 42. (Color online) The results for the LLNL Pulsed Sphere benchmark for magnesium. The results for Mg thick- nesses of 0.7 mean free path (mfp) and 1.9 mfp are shown. All results for 1.9 mfp are multiplied by 5, for visibility. The results based on ENDF/B-VIII.0 are hardly visible, because they are almost identical to the results based on JEFF-3.3.
The results for Mg shielding based on TENDL-2017 are closer to the benchmark data than the results based on either ENDF/B-VIII.0 or JEFF-3.3. For Fe shielding, the situation is the other way around. For all other ma- terials used here in the shielding benchmark calculations, the diﬀerences between the results on the various data libraries are of less importance.
FIG. 43. (Color online) The results for the LLNL Pulsed Sphere benchmark for iron. The results for Fe thicknesses of 0.9 mean free path (mfp) and 4.8 mfp are shown. All results  for 0.9 mfp are multiplied by 5, for visibility.
C.Delayed Neutron Data Testing
The TENDL-2017 data have also been tested against measurements of eﬀective delayed neutron fraction βeﬀ in critical conﬁgurations. For βeﬀ only a handful of mea- surements have been reported in open literature with suf- ﬁciently detailed information. Here we use 21 measure- ments of βeﬀ and 10 measurements of Rossi-α, which is closely related to βeﬀ through the prompt neutron gener- ation life time. These measurement data were also used for testing among others ENDF/B-VIII.0 [4] and JEFF-
3.3 [55]. The results based on TENDL-2017 are given in Tables  XIII  and  XIV.  The  results  for  TENDL-2017 are
TABLE XIII. The values for C/E 1 for the βeﬀ calculations. The uncertainty quoted for C/E 1 includes only the statis- tical uncertainty of the calculation. All the cases have a fast spectrum, except for TCA and IPEN/MB01.
TABLE XIV. The values for C/E 1 for the Rossi-α calcula- tions. The uncertainty quoted for C/E 1 includes only the statistical uncertainty of the calculation. All the cases have a thermal spectrum, except for Big Ten.
similar to those for ENDF/B-VIII.0, which was expected because the evaluations for 233,235U and 239U in TENDL- 2017 were adopted from ENDF/B-VIII.0.
D.Integral Activation Data
The study of radioactive properties of nuclear devices relies upon calculations of neutron-induced activation/-
transmutation reactions. These have resulted in a suite of experimental data for spectrum-averaged cross sections and other quantities, such as post-irradiation decay heat, gamma activity or isotopic inventory. In the develop- ment of modern activation nuclear data ﬁles, this data was assembled under the European Activation File (EAF) project [105] and comprehensively reviewed in work car- ried out for TENDL [106]. The resulting database was used to validate the TENDL-2017 neutron sub-library, including full re-analysis of the Japanese Fusion Neutron Source (FNS) decay heat measurements [107]. These are complemented by measurements from the Technis- che Universit¨at Dresden (TUD), Forschungzentrum Karl- sruhe (FZK), Frascati Neutron Generator (FNG), Ju¨lich Nuclear Physics Group and National Physics Institute Rˇeˇz.
1.JAEA FNS Decay Heat Data
Several   experiments   were   performed   by   the   Japan Atomic   Energy   Agency   (JAEA)   using   the   deuteron- tritium Fusion Neutron Source (FNS) [108, 109].  The de- cay heat was measured using the a Whole Energy Absorp- tion Spectrometer (WEAS) method, taking into account both  the  β  and  γ  emissions.   Measurements  were  taken over a broad range, between a few seconds and up to thir- teen months.  To validate the TENDL-2017 data, full in- ventory simulations are performed using the Fispact-II code [92, 93].  An example simulation is shown in Fig. 44. In this case, seven radioisotopes or isomers are responsi- ble for the decay heat in the measured time period, and of  these  52V  and  56Mn  are  dominant  at  early  and  late cooling times, respectively.  The reactions responsible for these are almost exclusively 52Cr(n,p) and 55Mn(n,γ), so the  ratio  of  the  total  calculated  decay  heat  (C)  to  the experimental  decay  heat  (E)  may  be  attributed  to  the primary reaction of the dominant radionuclide.
Comparing the results of all FNS cross section C/E ratios over the nuclide number of the target provides an overview of the results, as shown in Fig. 45. The results show several corrections due to eﬀorts made to improve several cross sections for fusion applications [111].
2.Integro-diﬀerential Validation
The results of integral tests are not,  by  themselves, an indication of the quality of the library, and must be taken in context with the other available experimental data. For example,  the 206Pb(n,α) cross section,  which is responsible for the measured 203Hg decay heat, shows a bias of a factor of 20 between the FNS measurement and TENDL prediction in Fig. 45. However, the diﬀer- ential data from EXFOR shows a much better agreement in this energy range, as seen in Fig. 46.  This motivates a more rigorous method for testing, which must consider not only integral data, but all relevant diﬀerential and FIG.  44.   (Color  online)  Total  decay  simulation  for  an  FNS 5 minute irradiation of Inconel-600, with experimental results compared against calculated solutions using  Fispact-II with diﬀerent nuclear data libraries.  The grey band represents un- certainty  from  TENDL-2017  calculated  using  the  full  covari- ance data the FISPACT-II pathways-based uncertainty prop- agation algorithm [110].  Individual radionuclide contributions are shown for those that are dominant in the time-periods that were measured.
integral comparisons simultaneously. Statistics such as the well-known χ2 are not shown, as they are dominated by a few reaction channels with signiﬁcant disagreement. These are often also reaction channels with very limited experimental data and global statistics represent primar- ily these cases which are of less interest to the nuclear data community.
102
101
Incident Neutron Energy (MeV)
FIG.   46.	(Color online) Diﬀerential cross section for
206Pb(n,α), against data from EXFOR.
The comparison of integral and diﬀerential data for each of the FNS decay heat cases, as well as a set of other fusion and accelerator-driven systems, was performed in another validation report from the UKAEA [112]. This study utilised a suite of incident neutron spectra rang- ing from 14 MeV deuteron-tritium sources to 150+ MeV deuteron-Li sources. The cross-comparison of both the integral and diﬀerential data provides the veriﬁcation and validation of activation-transmutation cross sections that has been standard practice for years [105]. While the in- tegral measurements have various uncertainties and sys- tematic errors which require caution when interpreting, the global distribution of all calculation (C) to experi- ment (E) ratios provides a useful metric. This is shown in Fig. 47. While the distributions are quite similar, it should be noted that the EAF-2010 library was speciﬁ- cally tailored, with manual renormalisations performed, to match this data. Moreover, the log-mean C/E value,
@26
for  TENDL-2017  is  1.046,  while  the  EAF-2010  data
gives 0.850. This indicates a systematic under-prediction that is intuitively due to the evaluation methodology for EAF: reaction cross sections are added when and where
Baryon Number of Target (A)
FIG. 45. (Color online) All FNS decay heat calculation (C) to experiment (E) ratios for simulations using TENDL-2014,
-2015 and -2017. Uncertainty bars are from the experiments. The grey band represents a factor of 2, whilst the inner band represents a 10% range. Individual reactions with signiﬁcant disagreement are labelled individually.
required, rather than automatically for all allowable re- action channels.
While there is undoubtedly more work left to improve activation nuclear data, the fact that  this  distribution has a relatively large variance should not necessarily be cause of concern, as the experimental uncertainties are often much larger than 20%, or likely include some sig- niﬁcant systematic error (such as the 206Pb(n,α) FNS measurement).
FIG. 47. (Color online) Distribution of C/E values for TENDL-2017 and EAF-2010 neutron-induced activation- transmutation cross sections, taken from [112].
IX.APPLICATIONS
The TENDL libraries (both nominal and random ﬁles) have been used over the past years in many applications. A short list is given below.
A.Use in Nuclear Software
Since its creation in 2008, TENDL has been used in many diﬀerent applications, as shown in Fig. 2. In spe- ciﬁc cases, TENDL has been directly implemented in code packages, with our without additional processing. Exam- ples are provided below.
FISPACT-II (for activation and transmutation, de- pletion burn-up, inventory and radiation damage). The complete TENDL library is now available with the FISPACT-II package [92, 93]. FISPACT-II is taking advantage of all the information included in the diﬀerent reaction sections (MT) for all iso- topes, as well as the covariance data. The group TENDL cross-section libraries for charged particles (from proton to alpha) and gamma are selected by default. For more details, see the above reference or Ref. [110].
GEANT4 (toolkit for the simulation of the passage of particles through matter). GEANT4 now con- tains a speciﬁc database for the TENDL-2014 in- cident proton reactions, including 2400 isotopes up to 200 MeV. See Ref. [113] for additional details.
MARS (Monte Carlo code for hadronic and elec- tromagnetic cascades, muon, heavy-ion and low- energy neutron). The charged-particles sublibraries for TENDL were included in the MARS15 code package to improve the prediction of secondary
B.
Use in Nuclear Libraries
The TENDL libraries have also been used in other nu- clear data libraries. A list is provided below.
JEFF-3.2. JEFF is the OECD Nuclear Data Bank nuclear data library. The library version 3.2 is using about 300 TENDL-2012 ﬁles out of 473 neutron evaluations.
JEFF-3.3. The version  3.3  is  the  latest  release of the JEFF library. All charged-particles sub- libraries are coming from TENDL-2017 (alphas, deuterons, helium-3, protons and tritons), as well as the gamma sublibrary and the activation ﬁles (in EAF format). Concerning the neutron sublibrary, 312 evaluations come from TENDL-2015, out of 562 ﬁles.
ENDF/B-VIII.0. The US library, released in 2018, includes 36 TENDL-2015 neutron evaluated ﬁles. See Ref. [4] for details.
FENDL-3.1. FENDL is  a  nuclear  data  library for fusion application. It uses a large number of TENDL neutron ﬁles, mostly from TENDL-2010, with some isotopes updated to TENDL-2014 (in the FENDL latest version).
IRDFF-1.05. IRDFF is an international library for reactor dosimetry and fusion application and it is making use of TENDL data by extrapolating IRDF- 2002 evaluations from 20 to 60 MeV [116].
C.Total Monte Carlo
The TMC approach was developed in 2008 and has been applied tens of times in various cases. A full list of publications is not provided here, but rather groups of examples, based on their area of application. The main advantages of the TMC method compared to other un- certainty propagation method are (1) its simplicity and
(2) its capability to be applied in complex simulations. Of course, this comes at a computational price. Over  the years, the name “TMC” was also used when pro- ducing random nuclear data libraries based on existing cross section covariance ﬁles. Even if the original TMC developments were based on random model parameters, the production of random cross sections based on exist- ing cross section covariance ﬁles follows the same logic, even if an additional degree of approximation is implied by using a covariance ﬁle (often used in combination with the assumption of Normal distribution).
Diﬀerent comparisons of uncertainty propagation meth- ods can also  be  found  in  the  literature  (e.g.  with  GLLS [117], Polynomial Chaos [118], NUSS [119]) show-
ing that under speciﬁc assumptions, results can be con- sidered equivalent from a statistics point of view. To facilitate its application, the TENDL libraries come with a variety of random ﬁles, based on the variations of model parameters. Below are presented a few examples for spe- ciﬁc categories.
Assembly. A fuel assembly is a relatively simple system to study, with or without depletion calcu- lation. For a nominal calculation (without using random nuclear data ﬁles), one has to be able to perform it in a reasonable amount of time, using ei- ther a deterministic or Monte Carlo transport code. Nowadays, this type of calculation can (almost) be performed on any computer for a two-dimensional geometry, and on a small computer cluster for a three-dimensional geometry. Additionally, no spe- ciﬁc knowledge on a reactor core is required (such as amount of assemblies, speciﬁc histories...). For these reasons, such systems has been studied in some details. See Refs. [120–123] for speciﬁc ex- amples.
Criticality-safety benchmarks. As for the assembly system, some criticality-safety and shielding bench- marks can be calculated relatively easily on modern computers, therefore the application of the TMC method is also feasible. Examples can be found in Refs. [117, 124–126].
TMC applied with ﬁssion yields. In the majority of cases, the propagation of the ﬁssion yield un- certainties is diﬃcult to perform based on exist- ing covariance matrices solely.  The main reason   is that the covariance ﬁles included in the nuclear data libraries do not contain correlation terms. A convenient solution is to use the GEF code [45] and randomly vary its parameters to produce ran- dom ﬁssion yields, available on the TENDL web- page. One can notice that other solutions exist, as presented in Refs. [127, 128]. For the application of random ﬁssion yields from the GEF code, see Refs. [81, 129, 130].
Fusion system. Similar to the above example, the TMC method was applied to a speciﬁc MCNP
model of the JET fusion machine [131] and to fusion shielding benchmarks [132].
Full core without burn-up. This type of applica- tion presents an additional complexity due to the calculation time requirement. In this case, a con- venient solution is to apply the fast-TMC method for Monte Carlo  transport  code  [123,  133], or  to use deterministic codes: see Ref. [134] for a PWR, Ref. [135] for a Sodium Fast Reactor and Refs. [136, 137] for a Lead Fast Reactor.
Full core with burn-up. This type of simulation is obviously computational intensive and Monte Carlo transport calculation cannot be applied with uncer- tainty propagation for the time being. The solution lies in the use of deterministic code, with ﬂexibility on nuclear data (ability to change them), and the
availability of input models (realistic cycles, assem- bly histories, shutdown periods, etc). Examples on such calculations with TMC-like uncertainty prop-
agation can be found in Refs. [138, 139].
Full core non linearity. Based on the full core cal- culations (with burnup), many core parameters can be calculated. Some of these parameters might be very sensitive to speciﬁc cross sections. Such an ex- ample is presented in Ref. [140] where the impact of the 238U(n,inl) cross section on the peak pin power (ppp) analysed. It was shown that a strong nonlin- ear behavior due to the variation of this cross sec- tion exist, indicating that smaller uncertainties for the 238U(n,inl) cross section are required to make this eﬀect disappear. Note that the use of adjoint methods can also be investigated.
Transient. Following the above example of steady- state calculations, the additional step towards the simulation complexity is to study the eﬀect of nu- clear data on transient systems, such as reactivity- initiated accident [141, 142].
Spent fuel. Another type of complexity can be achieved by staying with steady-state calculations, but extending the calculation scheme towards the characteristics of spent nuclear fuels (for transport and storage). Important quantities such as decay heat and source terms can be obtained with un- certainties due to nuclear data, see for instance Refs. [143, 144]. Whereas the uncertainty propa- gation is performed in [143] up to the decay heat calculations of single assemblies, there is the pos- sibility to associate such calculations with those of criticality of canister for long-term storage [144]. Such methods, combining both full-core determin- istic and criticality-safety Monte Carlo approach will nevertheless require relatively important cal- culation power.
dpa. The calculation of dpa (displacement per atom) and related quantities are important for the
determination of the damage of material. Dpa can also be calculated with uncertainties due to nu- clear data using the TMC method, as presented in Refs. [145, 146].
D.Other Applications
The completeness of TENDL makes it usable in many applications, also outside traditional ﬁssion energy appli- cations. The pie chart of Fig. 2 already gives an indica- tion about this. A non-exhaustive list is
Medical isotope production. Especially the proton and deuteron TENDL ﬁles, and to a lesser extent the photonuclear ﬁles, are used regularly for the es-
timate ofe.g., (p,n) or (p,2n) production cross sec- tions for isotope yield estimates.
Fusion. As discussed in this paper, the complete- ness in terms of activation cross sections, including isomers, makes TENDL an obvious library for ac- tivation studies of fusion reactor designs. Also the deuteron data libraries are used for IFMIF  studies.
Astrophysics. TENDL has a a rather extensive nu- clide range and can be used to predict reaction rates for e.g.,  r-processes.   Other applications  are
the study of solar ﬂares and cosmic rays.
X.FUTURE
The future of TENDL is not as statistically secure as other ﬁle projects, where several tens of contributors de- liver their isotopic data ﬁles in one place. Making the entire TENDL system open source, and thus for every-
one to use, and as easy to use as e.g., the TALYS code itself,  is  a  huge  challenge  but  necessary  for  survival of
this concept. There are several particular issues which require special attention.
A.Systematic Evaluation of Experimental Reaction Data
The systematic approach behind TENDL takes away many deﬁciencies and restrictions of the so-called ’man- ual’ data evaluation which forms the basis of the other world libraries. Nevertheless, expert knowledge of ex- perimental data is still one of the key ingredients of the procedure. Much of this experimental knowledge, espe- cially about older measurements is lost because opinions of experts have often not been documented or have ended up in data libraries in a way that in not recognizable, and thus not reproducible. In the community of technologi- cal analyses the adopted approach is often ”That nuclear data library performs good, although we don’t know ex- actly why anymore, and we are not going to change it”. Sometimes, there exists documented quality assessment
of diﬀerent experimental data sets, but then the opinion of evaluators is expressed in textual form. That is better than nothing, but a truly systematic approach would be if an evaluator would ’score’ experimental nuclear data sets in an internationally agreed manner. This does not exist yet, although some global approaches have been applied [6, 111] which at least rule out the most severe outliers in EXFOR. We call for a more in-depth and systematic ap- proach by experimental nuclear physicists who give their opinion on EXFOR subentries, realizing that this is sub- jective. A format for such experimental nuclear data eval- uation database should be agreed upon.
B.Solving Deftciencies in Uncertainty Quantiftcation
The approach that combines Bayesian inference and Monte Carlo is basically set, and from a statistical point of view there is no reason to doubt the approach. How- ever, two main deﬁciencies enter the formulation of the χ2 estimator which basically prevents a ﬂawless uncer- tainty approach: (a) the correct experimental covariance matrix, which in addition to the aforementioned quality scoring which rules out outliers, also should address the issue of assigning correlations between data points and data sets, and (b) a robust method for handling model defects to enable evaluated curves to perfectly describe experimental data. Both aspects should lead to an ef- fective χ2 which can be used as a proper goodness-of-ﬁt estimator.
C.Systematic Integral Data Evaluation
What holds for the EXFOR database also holds for in- tegral experiments. In principle, only with a reliable co- variance matrix for (perhaps all) integral benchmarks a sound validation procedure for nuclear data libraries can be used. Also here, various experts know which bench- marks are more important and more reliable than others but again this is not yet represented in a numerical table. That such a table is subjective per expert is less relevant. A WPEC subgroup is currently addressing the reliability of ICSBEP benchmarks, in particular the quoted uncer- tainties. For integral testing outside criticality the situa- tion is perhaps less complicated. Here we would at least
call for a complete collection of available shielding, de- cay heat, activation, etc. benchmarks which can directly be tested with TENDL, if possibly during and not after evaluation.
D.Upgrade to Microscopic Nuclear Models used in TALYS
One of the core assets behind the TENDL database are what we call ‘best’ TALYS input ﬁles per target nu- clide. This means that all nuclear model parameters for TALYS have been adjusted so that an overall optimal
description of all reaction  channels  is  obtained. So far, for all TENDL releases this is done for phenomenological models for the structure of the nucleus, such as Fermi-gas based level density models, Hill-Wheeler forms for the ﬁs- sion barrier and Lorentzian forms for the photon strength functions. The reason for using this is not the physics, which is actually expected to be better for several of the microscopic models, but rather the experience in adjust-  ing the parameters for these models. However, in TALYS for all microscopic models sets of adjustable parameters are now available.  Hence, as basis we can use tabulated
nuclear structure ingredients coming from e.g., Hartree- Fock-Bogolyubov calculations and we are able to multi- ply  this  with  smooth  functional  parameterized  forms  to
deviate from the original tables in order to ﬁt the reac- tion data better. We feel this will increase the predictive power of TALYS, and thus TENDL, for nuclides, reaction channels and energy ranges where no experimental data is available. It should however be realized that this work is tedious since proper nuclear data evaluation is hard to automate, and new ’best’ ﬁles need to be made with span the whole nuclide range.
E.Cleaning up the Software
Less is more. In the past decade, features have been added to T6 which makes it more diﬃcult to maintain. Diﬀerent codes like TALYS and TARES may be using dif- ferent fundamental databases and this has not yet been streamlined completely. For some fundamental data, like the upper energy limit of the resolved and unresolved
resonance range, or values of thermal cross sections also no uniﬁed picture has been accomplished yet. Other databases may be hidden in the system which could acci- dentally overrule settings that we thought we were using. This is a result of  adding  more  and  more  options  and not suﬃciently cleaning obsolete options. (Though again here, looking at other software in nuclear data, every-  thing is relative). This is one of the reasons why TALYS-
2.0will be released at the end of 2019, which will con- tain the major T6 codes in one software package. The resonance parameters from  TARES  will be read in as  a
database, which could be produced on e.g., a yearly basis, and no longer  be produced  on the ﬂy.  After that,  inter-
national testing and use could lead to improved versions of TENDL and other libraries.
ACKNOWLEDGEMENTS
The authors are grateful to Arjan Plompen and En- rique Gonzalez for the coordination of the EU project CHANDA which funded TALYS and TENDL develop- ment, Ulrich Fischer for guiding the work on  activa- tion data within the EuroFusion project on nuclear data, Stephane Hilaire and Stephane Goriely for TALYS devel- opment, Stanislav Simakov for his useful feedback on the use of random TENDL ﬁles for uncertainty quantiﬁcation of damage calculations, Chikara Konno and Oscar Ca- bellos for all their feedback on the format and processing aspects of TENDL, Jura Kopecky for his eﬀorts on val- idating TENDL with diﬀerential data,  Henrik Sj¨ostrand and Petter Helgesson for testing and applying our evalua- tion system on JEFF evaluations and Total Monte Carlo.
