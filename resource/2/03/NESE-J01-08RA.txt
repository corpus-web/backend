<title>NECP-hydra: A high-performance parallel SN code for core-analysis and shielding calculation</title>
<author>1,YongpingWang,2,YouqiZheng,3,LongfeiXu,4,LiangzhiCao</author>
<Affiliation>1,Xi’an Jiaotong University, Xi’an, China;2,Institute of Applied Physics and Computational Mathematics, Beijing, China</Affiliation>
<year>2020</year>
<Jounral>Nuclear Engineering and Design</Journal>
<Publishing_house>ELSEVIER</Publishing_house>
<Text_Collector>XiaFan，HEU</Text_Collector>
<DOI>10.1016/j.nucengdes.2020.110711</DOI>
<URL>https://www.sciencedirect.com/science/article/pii/S0029549320302053</URL>
NECP-hydra: A high-performance parallel SN code for core-analysis and shielding calculation
YongpingWang,YouqiZheng,LongfeiXu,LiangzhiCao
Xi’an Jiaotong University, Xi’an, China;2,Institute of Applied Physics and Computational Mathematics, Beijing, China
<Section>Highlights</Section>
The theory of the SN parallel code NECP-Hydra for reactor core analysis and radiation shielding calculation is introduced.
Four distinguishing features of NECP-Hydra are presented.
Numerical verifications show that NECP-Hydra achieves both high accuracy and efficiency.
<Section>Abstract</Section>
NECP-Hydra is a three-dimensional high-performance parallel code for reactor core analysis and radiation shielding calculation. This paper summarizes its main features and carries out some verifications. NECP-Hydra empolys finite difference and discrete ordinates for spatial and angular discretization respectively, and it supports both rectangular and cylindrical geometry. The parallelization calculation was performed by the Koch-Baker-Alcouffe (KBA) algorithm based on domain decomposition. Besides existing four-octant sweep (FOS) scheme, a diagonal-octant sweep (DOS) scheme is proposed to further improve the efficiency for the problems with reflective boundaries. Moreover, acceleration methods such as parallel mesh-corner DSA acceleration, inner-group GMRES and multi-group GMRES are also implemented in the code to reduce the computational time. Additional function like continuing computation with surface source for coupling with Monte-Carlo code is also implemented. Finally, we present some numerical results for evaluating NECP-Hydra’s accuracy, efficiency and its preformance in practical analysis of reactors, from which we can draw a conclusion that NECP-Hydra achieves both high accuracy and efficiency throughout these verifications.
Keywords:Discrete ordinates method;Parallelization;Acceleration;KBA
<Section>1. Introduction</Section>
In the past few decades, due to the limitation of computational capability, a large number of approximations have been applied in the neutronics calculation. For reactor core analysis, the ‘two-step’ approach (Smith, 1986) is usually empolyed: first, the two-dimensional (2D) transport equation is solved for the assembly to obtain homogeneous cross section. Second, the diffusion equation is solved for the core and reconstruction process is carried out to obtain the pin power distribution. For radiation shielding calculation, the ‘point kernel integration method’ and “synthesis method” have been widely used in the past decades. Both methods require little computational resources (CPU and memory), but cannot achieve satisfactory accuracy.
Fortunately, the fast development of computers makes it possible to directly solve the three-dimensional (3D) transport equation of large-scale problems (Chen et al., 2018, Zhang et al., 2017, Zhang et al., 2018). Therefore, it is of great significance to study the large-scale parallel algorithm for supercomputing for accurate neutronics simulations.
SN method (Carlson, 1953, Zheng, 2013, Azmy and Sartori, 2010) has good parallelization potential because of the way of its angle discretization. In addition, SN method has obvious advantages over MC method in handling deep penetration problems and obtaining the global distribution of physical quantities. Therefore, the SN method is one of the most promising methods for solving large-scale problems. Thus, in this paper, the large-scale parallel algorithm of SN method and its application in reactor physics and shielding calculation are studied.
The SN method was first applied to one-dimensional (1D) transport equation in 1953 by Carlson (Carlson, 1953). The angular flux was expressed by N linear segments, so the discrete ordinate method was also called the SN method. After SN method was proposed, Lathrop, Carlson, Gelbard and other scholars made efforts for decades before it was really applied to engineering practice. Los Alamos National Laboratory (LANL) and Oak Ridge National Laboratory (ORNL) are pioneers in SN code development. In 1965, LANL developed the first 1D SN code DTF-IV (Lathrop and Carlson, 1965), followed by the 1D SN code ANISN (Engle, K-1693, 1967) which was developed in 1967 by ORNL. In the 1970 s, LANL introduced the coarse mesh rebalancing acceleration into its TRAN series SN code, and accordingly developed ONETRAN, TWOTRAN and THREETRAN. In the 1980s, based on the Diffusion Synthetic Acceleration (DSA) Technique, LANL developed its DANT series of SN codes, named ONEDANT, TWODANT and THREEDANT. In order to treat the complex geometry, in 1996, LANL developed the famous SN code ATTILA (Wareing et al., 1996) based on discontinuous finite element method. ORNL developed the 2D code DORT in 1988 and 3D code TORT (Rhoades and Simpson, ORNL/TM-13221, 1997) in 1997. Since 2000, the parallelization of SN method has gradually become a research focus. Based on THREEDANT, LANL developed parallel SN code PARTISN in 2005. ORNL released the parallel SN code DENOVO in rectangluar geometry in 2009. In addition to LANL and ORNL, other institutes have also carried out active researches on SN methods, such as the Japanese Mitsubishi Atomic Energy Corporation developed the code ENSEMBLE in right-angle geometry in 1980; in 1997, University of Pennsylvania developed the parallel code PENTRAN (Sjoden and Haghighat, 1997) for 3D rectangular geometry; in 2009, the Argonne National Laboratory (ANL) developed the parallel code UNIC for unstructured geometry. In 2014, Beijing Institute of Applied Physics and Computional Mathematics developed a parallel SN code JSNT (Cheng et al., 2014) based on JASMIN software framework. In 2015, North China Electric Power University also developed a 3D parallel code ARES based on KBA parallel algorithm (Baker and Koch, 1998) in right-angle geometry.
It can be seen that SN method has always been a research hotspot in deterministic methods for solving transport equation. In this paper, we develop a high-performance parallel SN code NECP-Hydra with several distinguishing features: first, it supports both 3D rectangular and cylindrical geometry; second, we propose the Diagonal-Octant Sweep (DOS) for problems with reflective boundary conditions in the framework of KBA parallelization, and implemet DOS in the code along with existing Four-Octant Sweep (FOS) scheme (On, 1991); third, besides parallelization, NECP-Hydra combines several effective accelaration methods, including mesh-corner DSA acceleration (Larsen, 1982), inner-group GMRES (Saad and Schultz, 1986) and Multi-group GMRES algorithms; fourth, it has an interface for coupling with Monte Carlo(MC) code (Zheng et al., 2020).
This paper has been organized as follows. In Sec. 2, the method is introduced, including the basic formulations of SN finite difference discretization, DOS scheme in KBA parallel framework, acceleration algorithms and the interface with MC code. In Sec. 3, the numerical results are given and discussed. Finally, Sec. 4 gives the conclusions of this paper.
<Section>2. Theory</Section>
2.1. Basic formulations
The theory starts from the multi-group transport equation:@
where@ is the angular flux;@ is the macroscopic total cross section;@ is the source term including fission, scattering and external source.
2.1.1. Spatial and angualr discretization in X-Y-Z coordinate system
The angular flux integrals can be approximated by using an M-point quadrature set. Therefore, Eq. (1) becomes a system of M differential equations coupling together (the group index is omitted):
where m stands for the m-th direction. Integrating each term of Eq. in a control volume as shown in Fig. 1 yields:@
where @, @ and V are the surface normal vector, area and volume of the control volume, respectively.  and  are the surface and volume average angular flux, respectively. is the average source in the control volume. Additionally, we assume the cross section in the control volume is homogenized. In the Cartesian geometry,  and  can be easily expressed as
where ,@, and . Fig. 1 indicates that  are the coordinates of the center of the mesh i, and @ are the coordinates of the mesh corners.  can be written as:
Fig. 1. Control volume in the 3-D Cartesian geometry.
Finally, the discretized form of Eq. (2) is:
@(9)
where@ and @ (s = i, j, k) are the incident and outgoing average angular flux at the surfaces.
2.1.2. Spatial and angualr discretization in R-θ-Z coordinate system
In cylindrical geometry, for an explicit angle , Eq. (2) can be written as
@(10)
where @ is the cosine of the included angles between  and r-, θ- and z -axis at position , respectively;  is amplitude of Ωm.
We can find Eq. (10) is very different from Eq. (2) because of the angular redistribution term. As in cylindrical geometry, the angular coodinate  changes with the spatial coordinate even if the neutron travels straight without changing its direction, the angular redistribution term can be regarded as the neutron leaking out of angle  because of the changed spatial location. This term can be discretized as:
@(11)
where n is the index of polar level,@ is the m-th angle on the n-th polar level, and . Similar with that illustrated by Fig. 1, @ indicates the boundaries of the corresponding domain. Others terms can be discretized through traditional process which is detailed in our previously published work (Xu et al., 2017).
2.2. Parallelization scheme
2.2.1. General description
Eq. (9) indicates that the solutions in downstream grids can be obtained after solving the upstream grids. It is like a wave that sweeps from one corner to its diagonal corner along the given direction. Thus, the KBA algorithm is employed to perform the parallel calculation. The detailed description of KBA algorithm can be found in Ref. (Baker and Koch, 1998) and Ref. (Koch, 1992).
For angular sweeping two schemes are implemented in NECP-Hydra: pipeline and clock. Actually, as there is no angular redistribution term in X-Y-Z coordiante system, the angular sweep order can be arbitrary. However, as it is benificial for the parallel efficiency to sweep the angles in a row which have the same spatial sweep order, we choose pipeline method for X-Y-Z coordiante system as is shown in Fig. 2.
Fig. 2. Pipeline method.
However, in the cylindrical geometry, as the adjacent discrete angles on the same polar angle layer are coupled, the clock angular scanning method is employed as shown in Fig. 3. First, since the discrete angles within the same octant have the same spatial scanning order, the angles within the same octant should be scanned continuously. In order to maintain the coupling relationship between adjacent angles on each polar layer, the angles within each octant are scanned layer by layer. The green dots in Fig. 3 are the start angles on each polar layer of one octant, then store the angular redistribution term after the last angle on each polar layer is scanned, as is shown by the red dots in Fig. 3. Finally, begin the scanning on the radially adjacant octant with the order of 7, 8, 9,10, 11, 12 as shown is Fig. 3. In summary, the clock method balances the parallel efficiency and CPU memory.
Fig. 3. Clock method.
2.2.2. Diagonal-Octant sweep (DOS)
In order to improve the parallel efficiency of traditional KBA algorithm, multi-angle sweep sheme is developed. It launches the spatial sweep from the corners in the 2-D processor map simultaneously. The Four-Octant sweep (FOS) algorithm is a common way for multi-angular sweep, it launches the spatial sweep from four corners at the same time. For the problems with vacuum boundaries, the FOS scheme is equivalent to classic KBA as each spatial sweep starts from the zero incident angular flux. However, for those problems with reflective boundaries, the angle-to-angle dependencies on the boundaries will be introduced. As all angles are swept simultaneously in the FOS algorithm, for the current iteration, we can only utilize the reflective boundary condition to equalize the incident angular flux and the outgoing angular flux of the last iteration. We can find that in this situation, compared with the KBA algorithm, the FOS algorithm suffers from convergence degradation for problems with reflective boundaries (from Gauss Seidel to Jacobi iteration).
For this reason, this paper proposed another multi-angle sweep: Diagonal-Octant Sweep (DOS) which launches the spatial sweep from the diagonal corners. The basic idea of DOS is that it first sweeps the pair of octants (++) and (--) by using the exiting flux of the previous iteration as incident flux, and then the updated angular flux of octants (++) and (--) can be used in the sweeps of the octants (+-) and (-+). This allows implicitly taking into account reflective boundary conditions for half of the unknown boundary fluxes.
As spatial sweep are launched simultaneously from the diagonal corners, the will arrive at the “diagonal line” at the same time. Thus, the processors along the “diagonal line” may have multiple tasks at the same time. To ensure the communication safe and efficient, the outgoing angular fluxes of those processors along the “diagonal line” are sent using the buffered synchronous function ‘MPI_Bsend’. The blocking communication functions (MPI_Send & MPI_Rcev) are used for other processors. Fig. 4 shows the sweep steps for a problem which is divided into 4 × 4 blocks (one processor for each block). The red line means “diagonal line” of this problem. The details of how the “diagonal line” is determined and the analysis of the scaling efficiency is included in the Appendix.
Fig. 4. Sweep steps evolution in the 4 × 4 processor map.
2.3. Acceleration methods based on KBA
The solution of transport equation for large-scale problems is very time-consuming, while effective acceleration techniques can reduce computing time significantly. The acceleration method should be matched with the discrete scheme and parallel strategy of transport equation. Combining the domain decomposition strategy of KBA algorithm, NECP-Hydra applied three acceleration methods: parallel Mesh-Corner DSA acceleration algorithm, inner-group GMRES algorithm and multi-group GMRES algorithm for upper scattering iteration.
2.3.1. Parallel Mesh-Corner DSA acceleration
In this section, starting from the discretized transport equation, the transport equation is transformed into the form of “diffusion equation + correction term” by using spherical harmonic function. The leakage term at the mesh corner is constructed by using the angular flux of the outgoing surface of the mesh, and the diffusion equation is solved for the mesh corners. Finally, the scalar flux at the corners is used to modify the scalar flux at the mesh center in the transport equation. The conjugate gradient algorithm is used to solve the diffusion equation, preconditioned by the red–black sweep algorithm to match the domain decomposition of the KBA parallel algorithm.
The transport equation is discretized on a mesh (i, j, k) in rectangular geometry as follows (energy group and angle label are omitted):
@(12)
where @ is the self-scattering cross section of mesh (i, j, k);@  is the summation of the source except self-scattering;@  is the scalar flux of mesh(i, j, k). By multiplying spherical harmonics  to both sides of Eq. (12) and integrating over angle, we can obtain:
@(13)
where@  is the angular flux moment expressed by:
@ (14)
By adopting the same procedure for meshes(i, j, k + 1), (i, j + 1, k), (i, j + 1, k + 1), (i + 1, j, k), (i + 1, j, k + 1), (i + 1, j + 1, k) and (i + 1, j + 1, k + 1), we can obtain other seven similar equations. Then, by adding the eight equations corresponding to the eight meshes and introducing the diamond difference, it leads to a mesh balanced equation in a “super-mesh”. The super-mesh consists of eight small meshes. The red dot in Fig. 5 is the center of the super-mesh.
Fig. 5. Super-mesh in Mesh-Corner DSA method.
Then eliminate the first- and second-order flux moments in the super-mesh balanced equation and we can finally obtain this form:
@(15)
where D' is the diffusion coefficient of the super-mesh;@ is the removal term of the center point of the super-mesh; R is the leakage correction term of the center point of the super-mesh, containing the high-order flux moments.
It should be noted that Eq. (15) calculates the scalar flux at mesh corners, while transport equation calculates the scalar flux at the mesh center. Therefore, when the solution of Eq. (15) is used to modify the solution of transport equation, it is necessary to convert the scalar flux at corners to the scalar flux at mesh centers. In three-dimensional geometry, the scalar flux at the center point is equal to the average value of the scalar flux at eight corners around it. Similarly, when the transport equation is solved, the relevant physical quantities at the center point need to be converted to those at the corner point to solve the diffusion equation. The acceleration process of the whole DSA algorithm needs to be switched between mesh corners and centers.
The diffusion equation is solved by conjugate gradient algorithm. In addition, red–black sweep scheme is applied to spatial scanning. For the two-dimensional plane shown in Fig. 6 (a), if the scalar flux of black corners is known, then the scalar flux of all red corners can be calculated (because the scalar flux adjoint to each red point is known). After the scalar flux of red dots is calculated, it can be used to update the scalar flux of black dots. By iterating over and over again, we can finally converge to the true solution. In this process, all red dots (or black dots) are computed completely independently, so they have inherent parallelism. In addition, the red–black scan will not cause iteration degradation with the increase of the number of parallel sub-domains.
Fig. 6. Red-black scanning. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)
Obviously, red–black scheme is also applicable in three-dimensional geometry as shown in Fig. 6 (b).
Therefore, when domain decomposition is carried out on the X-Y (or R-θ) plane, all sub-domains can be computed simultaneously, and only the corner scalar flux on the boundary need to be communicated between adjacent processors. Therefore, the X-Y (or R-θ) plane parallel red–black scanning designed in this paper can be effectively combined with KBA algorithm domain decomposition. In addition, when constructing DSA leakage correction term, only the integral value of the flow on the mesh boundary is needed, which is independent of the scanning order of the angle. Therefore, this scheme is also applicable to FOS and DOS scanning.
2.3.2. Inner-group GMRES algorithm
GMRES, also known as the Generalized Minimal Residual Method, is a kind of Krylov subspace algorithm (Saad, 2003). GMRES algorithm was proposed by Yousef in 1986 (Saad and Schultz, 1986). It is an effective algorithm for solving large asymmetric linear equations. In order to illustrate the application of GMRES in transport equation, the transport equation is firstly written in the operator form.
@(16)
where L is the transport operation;Qs,Qe , Qf are the scattering source, external source and fission source respectively. Assuming the coefficient matrix of quadrature weight is W, the matrix of sphrical harmonics function is M, the scalar flux moments can be written as:
@(17)
@(18)
By using spherical harmonic function matrix and scattering matrix S, the scattering source can be expressed as:
@(19)
Combining above equations, the scalar flux moment is:
@(20)
In the inner-group iteration of multigroup transport equation, the scattering sources from other energy groups to the current energy group remain unchanged. Therefore, when the Eq. (20) represents the transport equation of a certain energy group, it can be written in the following form:
@(21)
where Q~ is the summation of the fission source, external source and scattering source from other groups. Eq. (21) are linear equations such as “ Ax = b”. Assuming that the iteration vector of GMRES algorithm is v, the process of multiplying coefficient matrix and iteration vector is as follows:
Compute the scattering source: @
Sweep on angle and space:@
Convert the angle-dependent variables into moments:@
Return:@
It can be seen that the most time-consuming operation in matrix and vector multiplication is angle-space scanning to achieve the inversion of the transport operator, which can be realized in parallel by KBA, FOS or DOS algorithm. Therefore, GMRES algorithm can be combined with various parallel algorithms discussed above to accelerate the process of inner-group iteration.
2.3.3. Multi-group GMRES algorithm
For the problem with no up-scattering, Gauss Seidel (GS) is the best multi-group iteration scheme. However, in LWR, up-scattering happens frequently. Obviously, up-scattering will slow down the convergence of multi-group iteration due to the fact that the flux of the low energy group is not known when calculating the scattering source of current energy group.
From the derivation of inner-group GMRES, it can be seen that when the transport equation is transformed into a moment-dependent one, the scattering source can move to the left side of the equation. Therefore, if all the scattering sources of the upper scattering related energy groups are moved to the left end of the equation, then these energy groups can form a new variable which can be solved simultaneously. In this paper, this method is called the multi-group GMRES algorithm (MGMRES). Therefore, we adopt MGMRES iteration for the upper scattering related energy group and keep the original GS iteration for the lower scattering related energy group. The MGMRES algorithm is illustrated below.
Applying the same transformation of inner-group GMRES (Eq. (21)), we can obtain the multigroup transport equation for scalar flux moments:
@(22)
where []g stands for the matrix of the moments of all the meshes and angles for one energy group. For example, if there is a 5-group problem with up-scattering on the last group, the scattering matrix is in this form:
(23)
Then the scattering matrix can be divided into three blocks: down-scattering block, up-scattering block and the rest part of the scattering matrix:
@(24)
@(25)
@(26)
The multi-group GS iteration is solved from high energy to low energy. When solving group g, the flux from group 1 to group g-1 have the latest value. In this case, the multi-group GS iteration in the lower scattering block can be expressed as follows:
(27)
All energy groups in the upper scattering block are solved simultaneously by the MGMRES algorithm derived in (25):
@(28)
where @ is the down-scattering flux obtained by GS iteration, Qeup and Qfup are the external source and fission source of the up-scattering related groups. Thus, we solve the multigroup iteration by a combination of GS and MGMRES methods.
2.4. Coupling of SN calculation with Monte-Carlo calculation
Continuing Computation (CC) is widely used in shielding calculation. Generally, there are several coupling types of CC: MC to SN, SN to MC, SN to SN and their further combinations. The angular flux output and the definition of surface source are two main issues involved in CC. In the framework of KBA parallel domain decomposition, the functions of volume-area angular flux output, surface-area angular flux output, and internal/external sourface source output are developed in NECP-Hydra.
In this paper, six boundaries are used to describe the output region of angular flux: -x(r), +x(r), -y(θ), +y(θ), −z, +z. When two boundary values in a certain direction are equal, they represent the surface-area, otherwise they are the volume-area. Because the angular flux is not stored during the iterative procedure, when the output angular flux is needed, NECP-Hydra will do an additional iteration after the convergence of the calculation to output the angular flux of the required area in the calculation process, so as to avoid a large amount of memory usage.
It is more complicated to output angualr flux in in parallel framework than that in serial code, because the overlap relationship between the defined angular flux output region and the sub-domain is more complex. Since KBA algorithm decomposes three-dimensional geometry on two-dimensional plane, the overlap between output region of angular flux and sub-domain is divided into 16 categories: they are 4 overlapping types at corners, 4 overlapping types on the sides, 4 overlapping types over the sides and 4 overlapping types within domain, as shown in Fig. 7. After determining the overlap type, we can determine the local angular flux need to be recorded in each sub-domain and the processors involved in the communication. After the communications, the results are summarized to the main processor and the angular flux of the designated region is output.
Fig. 7. The overlap between sub-region (for angular flux output) and sub-domain (KBA domain decomposition).
In addition, NECP-Hydra outputs the angular flux at the center of each mesh when the designated region is a volume region; when it is a surface region, the extrapolated angular flux obtained by the diamond difference on the outgoing surface of each grid is output. Therefore, NECP-Hydra does not need to approximate the angular flux of the surface area by compressing the mesh thickness of the volume area, which improves the computational efficiency.
According to its normal vector, surface sources can be divided into three types: X(or R)-direction, Y(or θ)-direction and Z-direction. For the Z-direction, Fig. 7 indicates the same overlap types between surface sources and sub-regions. The overlap of X(or R)-direction and Y(or θ)-direction surface sources with sub-regions is relatively simple, which can be divided into four categories (see Fig. 8). According to the cosine value of the angle between the normal vector and the unit vector of the coordinate axis, the area source in each direction can be divided into positive and negative sources. As shown in Fig. 9, red arrows represent positive sources, black arrows represent negative sources. NECP-Hydra can define two-way area sources, i.e. both positive and negative sources exist simultaneously. In addition, when the surface source falls on the boundary surface of the problem, it is called external surface source, otherwise it is called internal surface source. NECP-Hydra realizes the definition of surface source at any location and allows their complicated combinations.
Fig. 8. The overlap between surface source and sub-domain in X  and Y directions.
Fig. 9. The definition of positive/negative surface source.
<Section>3. Results</Section>
In this paper, we evaluated NECP-Hydra in terms of its accuracy, efficiency, Continuing Computation (CC) function and applications. In each aspect, we presented one or two typical problems.
3.1. FBR benchmark calculation
This benchmark (Liu, 2006) is based on a fast breeder reactor model, with two blanket regions in the radial direction. The configuration of the benchmark is shown in Fig. 10. The 4-group macro cross sections of the materials are given. NECP-Hydra and TORT are both applied to calculate this problem with 100 × 40 × 120 meshes and S4 quadrature. Table 1 shows the results of eigenvalue and computational time. As TORT is a serial code, the comparison in this section is all based on serial calculations. We can find that the difference of the eigenvalue of NECP-Hydra is 26 pcm. The difference may come from two aspects: first, Tort is a single precision code while Hydra is a double precision code; second, the quadrature in hydra is slightly different with that in Tort. Table 1 also indicates that the efficiency of Hydra is about 2.4 times higher than TORT. This is mainly because during the calculations, Tort will write some data to the computer hard disk and then read them back to save memory (as it is developed at that time when the computer memory is limited), which is very time consuming. However, Hydra stores all the data in the memory.
Fig. 10. Configuration of FBR benchmark.
Table 1. The results of eigenvalue and computational time.
Table 2 indicates the comparison of the normalized flux in the five material region, from which we can find that the maximum derivation of 0.62% is located at the No. 1 blanket of the 1st energy group. Overall, the results of NECP-Hydra and TORT agree well, the average error of the active core is less than 0.1% and the average error of the whole core is less than 0.3%.
Table 2. Comparison of normalized flux in each region and each energy group.
The derivation of NECP-Hydra compared with TORT [%].
3.2. Test of diagonal-octant sweep
In order to test the advantage of DOS in problems with reflective boundaries, a problem with four materials is calculated in this section. The view of the problem on the X-Y plane is shown in Fig. 11 and it extended along the Z direction uniformly with the thickness of 1 cm. Two energy group macroscopic cross sections are applied. Besides, reflective boundary condition, S8 quadrature and 60 × 60 × 1 meshes are employed.
Fig. 11. The view of the X-Y plane.
KBA, DOS and FOS are used to calculate the problem in parallel, and the iterative convergence curves are compared, as shown in Fig. 12. The number of iterations of KBA, DOS and FOS are 211, 208 and 218 (no acceleration methods), respectively. It can be seen that DOS and KBA have similar number of iterations, saving ~ 10 iterations compared with FOS. As expected, DOS algorithm has better convergence capability than FOS algorithm with reflective boundary conditions.
Fig. 12. Comparison of the convergence performance of KBA, DOS and FOS algorithms with reflective boundary conditions.
The weak parallel efficiency of KBA, FOS and DOS is evaluated from 100 to 10,000 processors, fixing the number of meshes for each processor. In the test, the quadrature of S8 is used and 8 × 8 × 300 = 19200 meshes are assigned to each processor. In addition, the parallel efficiency is calculated based on the computational time of 100 processors. Fig. 13 summarizes the comparison results. The “theoretical values” in Fig. 13 means the communication time is not considered. Fig. 13 indicates that both FOS and DOS achieve better parallel efficiency than traditional KBA, and the improvements increase with the number of processors. With 10,000 processors, the parallel efficiencies of FOS and DOS are 13.38% and 10.41% higher than KBA, respectively.
Fig. 13. Comparison of the parallel efficiency of KBA, DOS and FOS.
3.3. Test of acceleration
In order to analyze the application scope of DSA and GMRES, KUCA benchmark (Takeda and Ikeda, 1991) and FBR benchmark are selected for testing. KUCA benchmark is based on the critical device used to simulate small light water reactor in Kyoto University, while FBR benchmark is based on the fast breeder reactor design.
The geometry description of the KUCA benchmark is shown in Fig. 14. The size of the problem is 25 cm × 25 cm × 25 cm, containing fuel, moderator and control rods. In this paper, the domain is divided into 50 × 50 × 50 meshes, and two energy group macroscopic sections are provided in the benchmark report. Furthermore, the benchmark problem includes two cases: control rod out and control rod in. This paper tests the case with the control rod out of the core. NECP-Hydra calculated the problem with 4 processors in parallel. DSA and GMRES are evaluated and compared with original source iterations (SI). The results of total number of outer iterations, computational time, and acceleration ratio are shown in Table 3.
Download : Download high-res image (148KB)Download : Download full-size image
Fig. 14. Configuration of KUCA benchmark.
Table 3. Comparison of the Acceleration Performace of DSA and GMRES for KUCA.
The description of FBR benchmark can be found in section 3.1 and the results in shown in Table 4.
Table 4. Comparison of the Acceleration Performace of DSA and GMRES for FBR.
From Table 3 and Table 4, we can find that in the light water reactor KUCA problem, DSA achieves better acceleration performance than GMRES. However, in the fast breeder reactor problem, GMRES obtains a speed-up ratio close to 8 which is much higher than 1.26 by DSA. Fig. 15 shows the iterative convergence curves by the three schemes for the first and last energy groups, respectively. As described in Section 2.3.1, DSA is based on diffusion calculation, thus if the problem follows the diffusion approximation which means the diffusion solution is very close to transport solution, DSA is supposed to have good performance. This explains the results in Table 3 and Table 4. In KUCA problem, it is a light water reactor and the neutron spectrum is soft. As the diffusion approximation is appliable for this problem, the DSA has good speed-up, better than GMRES. However, in the FBR problem, the anisotropic scattering is strong (as the spectrum is hard) which means diffusion approximation has large error in this problem. Thus, the DSA acceleration shows poor performance in the problem, much worse than GMRES.
Fig. 15. Convergence curves by the three schemes for FBR problem.
To evaluate the Multi-group GMRES (MGMRES) algorithm, we designed a fixed source problem with up-scattering. The geometry of the problem is shown by Fig. 16. The isotropic source is located in the central red region. The database is the 200-group MATXS form shielding database, of which upper scattering exists in the region of 165–200 group. The spatial domain of the problem is divided into 100 × 100 × 100; the order of quadrature is S16; the order of scattering source is P3, and the convergence critera of flux is set as 1.0e-6. In addition,100 processors are applied for parallel calculation.
Fig. 16. Geometry of the fixed source problem.
NECP-Hydra employs the original Guass Sediel (GS) and GS-MGMRES hybrid iterative algorithm to solve the problem respectively, and the inner GMRES algorithm is applied to each group iteration. Since the 1st to 164th groups are pure down scattering region, the two algorithms in this region are completely equivalent, and the calculation time is recorded as: Tdown. For the up-scattering region, the calculation time of GS and GS-MGMRES are recorded as  and  respectively. In order to compare the number of inner iterations of the two algorithms in the up-scattering energy groups, the number of inner iterations of GS-MGMRES is converted to the number of inner iterations of a single group. For example, if GS-MGMRES algorithm had done 10 inner iterations in the up-scattering region, it is converted into 360 (10 × 36) inner iterations in a single group. The number of inner iterations mentioned later refers to the number of inner iterations in a single group. Thus, we use  and  to record the number of inner iterations in a single group for GS and GS-MGMRES.
Table 5 summarizes the comparison of GS and GS-MGMRES. We can find that  is 4.27 times of  and  is 3.15 times of , which means GS-MGMRES achieves much better acceleration performance than traditional GS algorithm. The reason why the reduction on the computational time is less than that on the number of iterations is that the subspace orthogonalization process takes more time in GS-MGMRES when all the energy groups in up-scattering region are solved at once than the time required in single group GMRES.
Table 5. Comparison of GS and GS-MGMRES for the up-scattering problem.
3.4. Test of continuing computation (CC)
In this section, we tested the continuing computation (CC) function in NECP-Hydra in rectangular and cylindrical geometry.
As shown in Fig. 17(a), the source region of 10 cm × 10 cm × 10 cm is located in the center of a 30 cm × 30 cm × 30 cm cube. The whole domain is divided into 30 × 30 × 30, and the order of the quadrature is S8. First, conduct the shielding calculation with the volume source; second, output the surface source on the six surfaces of the source region, and conduct the shielding calculation again with the surface source (the volume source is removed). The flux distribution along the x-axis (x [0.0 cm, 30.0 cm], y [0.0 cm, 1.0 cm], z [0.0 cm, 1.0 cm]) obtained by previous two scheme is compared and the results are shown in Fig. 18. It can be seen that the two curves are completely overlapped, which demonstrates the correctness of the angular flux output and CC developed in this paper.
Fig. 17. Continuing Computation test problems.
Fig. 18. Comparison of flux distribution along the x-axis for the rectangular geometry.
The test problem of cylindrical geometry is shown by Fig. 17(b). The green region in the center is the volume source. The whole domain is divided into 40 × 40 × 30, and the order of the quadrature is S8. Then, we conduct exactly the same procedure as that in rectangular geometry and compare the radial flux distribution on the bottom surface of the cylinder. Moreover, we also compare the results with initial angle and without initial angle when use the surface source. The “initial angle” means the first azimuthal angle on each polar level. “With initial angle” means that the angluar flux of the initial angle has been provided by former calculation. “Without initial angle” means that the angluar flux of the initial angle is unknow. The results are shown in Fig. 19, which indicates that the surface source with initial angle agrees well with the volume source. However, the surface source without initial angle leads to the derivation of 20% in the center of the cylinder, and the deviation decreases with the increase of the radius. This problem demonstrates the implement of the angular flux output and CC is correct in cylinderical geometry, and it also draws the conclusion that the initial angle is essential for CC with surface source in cylinderical geometry.
Fig. 19. The comparison of flux distribution on the bottom surface of the cylinder.
3.5. DRF calculation of a 300 MW PWR core
In this section, we conducted a DRF (Detector Response Function) calculation of a 300 MW PWR core. For this kind of problem, the 2D synthesis method has been applied for years due to limitation of computing capability. However, in this paper, we carried out the calculation directly in 3D geometry. The following part introduces the modeling and evaluates the results.
This core contains 121 UO2 fuel assemblies with 3 enrichments as shown in Fig. 20 (one color stands for one enrichment). In each assembly, there are 204 fuel rods and 21 water holes.
Fig. 20. Radial Configuration of the 300 MW PWR.
In this paper, we established the calculation model from the active core to the concrete shielding layer in radial plane and from lower grid plate to upper grid plate in axial direction. The x-y and x-z profile is shown in Fig. 21 and the material of each region is listed in Table 6.
Fig. 21. x-y and x-z Profile of 300 MW PWR.
Table 6. Material of each region in Fig. 21.
The height of active core is 290 cm and the inner radius of the basket is 140 cm. The thicknesses of baffle, basket and vessel are 4.0 cm, 6.5 cm and 17.9 cm, respectively. The inner radius of vessel is 168.7 cm. In our calculation, the lined stainless steel and carbon steel are volume-weighted homogenized into vessel. Eight radiation monitoring tubes were arranged in the water gap between the hanging basket and the pressure vessel and this paper calculated the B-tube shown in Fig. 22. The reactor cavity is 20 cm thick insulation layer (mixed) and the thickness of the primary side concrete shielding wall is 30 cm.
Fig. 22. Locations of radiation monitoring tubes.
The fission source within the core is obtained according to the power distribution of some certain cycle of the reactor, which is treated as the external source in our shielding calculation. The whole domian is divided into 151 × 151 × 80 meshes in X-Y-Z geometry in NECP-Hydra (the sensitivity of mesh size has been studied in Ref. (Zheng, 2013) which is shown in Fig. 23. In this paper, the MATXS10 database (MATXS10, , 1994) is employed in the shielding calculation, which contains 30 groups of neutrons. The expansion order of scattering cross section is P3, and the order of quadrature is S8.
Fig. 23. The calculation model of NECP-Hydra.
In this paper, the results of NECP-Hydra are compared with those of TORT. The radial neutron flux distribution (0°radial angle) of 5th, 8th and 14th group in the middle plane of active core from the baffle to the concrete zone is shown in Fig. 24. We can find the flux decreases by 4–5 orders of magnitude from the baffle to the primary side of the concrete zone.
Fig. 24. The comparison of radial flux distribution.
The relative deviations of NECP-Hydra are shown in Fig. 25, respectively, with the results of TORT as the reference. In can be found that the calculated results of NECP-Hydra and TORT are in good agreement, and the relative deviation is really small, in the range of [0.01%, 0.001%].
Fig. 25. Deviation of radial flux distribution.
The neutron energy spectrum at the irradiation monitoring tube is shown in Fig. 26. The results of NECP-Hydra agree well with TORT, and the maximum deviation is less than 0.03%. Fig. 27 (a) and Fig. 27 (b) show the neutron energy spectrum and its deviation of the inner/outer surface of the pressure vessel at the mid-plane and 0°radial angle of the core, compared with TORT. We can find that NECP-Hydra obtained very accurate spectrum, and the maximum relative deviation is about 0.02% and 0.04%. In addition, as can be seen from Fig. 27, the neutron flux density decreases by about two orders of magnitude through the pressure vessel.
Fig. 26. The neutron spectrum at irradiation monitoring tube.
Fig. 27. The neutron energy spectrum and its deviations on the surfaces of pressure vessel.
Table 7 shows the fast neutron flux at the irradiation monitoring tube with energy higher than 0.1 MeV and 1 MeV. It can be seen that the calculated results of NECP-Hydra are in good agreement with those of TORT. Compared with the measurements, the deviations of fast neutron flux calculated by NECP-Hydra are 1.65% and 2.99% respectively.
Table 7. Fast neutron flux at irradiation monitoring tube.
In terms of efficiency, taking the advantage of parallelization, NECP-Hydra takes only 9.7 min. to finish the calculation with parallelization of 24 processors, while the computational time of TORT is 303.07 min as it is a sequential code. Obviously, NECP-Hydra greatly reduces the wall time by taking the advantage of parallelization.
3.6. Sheilding calculation of a PWR nuclear island
To further demostrate NECP-Hydra’s ability of computing very large-scale problem, this paper models a PWR nuclear island, and carries out three-dimensional shielding calculation. The model mainly includes the reactor core and its primary shield, steam generator, regulator, main pump, hot and cold pipe section, refueling pool, refueling storage tank, safety water injection tank, passive water storage tank, crane and its circular track, separation wall and working deck, etc. The radius of the containment is about 22 m, and the height from the base to the top of the passive driving storage tank is about 81.7 m.
In this paper, a simplified model is established. The internal complex structure of steam generator, regulator, main pump and other systems are not modeled in detail, all of them are replaced by homogenized mixture of iron and water. The upper head assembly is modeled by iron-air mixture, while the wall is considered as a concrete structure. Reactor core, baffle, reflector, hanging basket, descending water, pressure vessel, reactor chamber and primary side shield wall are modeled by equivalent rings. IRWST(In-Containment Refueling Water Storage Tank), passive water storage tank, ring crane track and other structures are detaily modeled. The model of Hydra calculation is shown in Fig. 28 and Fig. 29. Fig. 28(a) shows the layout of the nuclear island plant in the range of 0°–210°. Fig. 28 (b) is cross view of the containment. Fig. 29 (a) shows the local structure of the reactor and its primary side shield. The bottom layout of the containment model is shown in Fig. 29 (b). The green area with concave shape on the left side is IRWST. The cold and hot pipe sections connecting the two steam generators to the reactor are also shown in the figure. The cylinders and spheres on the north and south sides of the right steam generator are core recharge tank and safety water injection tank respectively.
Fig. 28. The model of the nuclear island established by NECP-Hydra.
Fig. 29. The local view of the nuclear island.
The P3 cross section of BUGLE-96 database (White et al., 1996) is used. The S16 quadrature group is selected and the domain is divided into 410 × 600 × 528≈1.3 × 108 meshes, with 67 energy groups (47 groups for neutrons + 20 groups for photons) employed. The processing of up-scattering and resonance in BUGLE-96 database is similar to that of MATXS10 database.
The memory required for storage of angular flux moments is 67 × 1.3 × 108 × 16 × 8 byte ≈ 1 T. The unknowns of the problem is 67 × 1.3 × 108 × 320 ≈ 2.8 trillion. NECP-Hydra uses 1230 processors in Tianhe No. 2 server to calculate the problem, which takes 2 h.
It should be noted that the shielding calculation of nuclear island includes two parts: source term calculation and transport calculation. However, as this paper focuses on transport calculation, the radiation source in nuclear island is given artificially. The magnitude of the radiation source in different parts of the nuclear island is chosen based on the numerical results of other PWR calculations. The radiation source is widely distributed in the primary loop fluid, refueling pool, IRWST and air, as shown in Fig. 30. Since no source term calculation has been carried out, this paper only presents some qualitative results. The first group of flux distributions in the axial section and the radial section at the core height of the nuclear island are shown in Fig. 31. From the flux profile, it can be seen qualitatively that neutrons are basically confined to the core and primary side shielding area, but a few fast neutrons will leak out from the reactor cavity. From Fig. 31 (b), we can see that there are a small number of neutrons in the refueling pool, the air area in the subsystem compartments and IRWST, but the neutron flux decay rapidly after encountering the wall.
Fig. 30. The radiation source distribution in axial and radial cross section of the nuclear island.
Fig. 31. The 1st group of neutron flux distribution in axial and radial cross section of the nuclear island.
The shielding calculation of the nuclear island demonstrated that NECP-Hydra has the computing capability for very large-scale transport problems and it achieves satisfatory computational efficiency with parallelization of over 1000 processors.
<Section>4. Conclusion</Section>
This paper summarized the theory and main features of the discrete ordinates code NECP-Hydra. The basic formulations are derived in both rectangular and cylindrical geometry. In order to achieve a satisfactory parallel efficiency and reduce the storage of angle redistribution items, the ‘clock’ and ‘pipeline’ angular scanning methods are applied for cylindrical and rectangular geometry. Apart from the existing Four-Octant Sweep (FOS) scheme, the Diagonal-Octant Sweep (DOS) is proposed to improve the convergence capability for problems with reflective boundaries. In the framework of KBA parallelization, three acceleration methods: Mesh-Corner DSA acceleration algorithm, inner-group GMRES algorithm and Multi-group GMRES (MGMRES) iterative algorithm, are implemented in the code. For the MGMRES iteration algorithm, GS iteration is used in the energy group containing only the down scattering, and MGMRES iteration is used in the up-scattering region. Additionally, NECP-Hydra also incorporates the function of continuing computation with surface source.
Several numerical tests are carried out in this paper to evaluate NECP-Hydra’s accuracy, the performance of DOS and acceleration methods, the correctness of contiuning computation, and its capability of large-scale problems. The FBR benchmark test shows that NECP-Hydra agrees well with TORT, with the deviation of 26 pcm for eigenvalue and ~ 0.3% for flux distribution. In addition, NECP-Hydra takes only 40% of the computational time of TORT. In the test of DOS algorithm, the results show DOS algorithm has better convergence capability than FOS algorithm with reflective boundaries as expected. For acceleration methods, DSA and GMRES achieved 2 to 7 speed-ups compared with original source iterations, depending on the specific problem. From the results in this paper, it seems DSA has better speed-ups than GMRES in water reactor while GMRES has more advantage in fast reactor. Moreover, apply the Multi-group GMRES algorithm to the up-scattering energy region obviously accelerate the original Guass Sediel iteration, with a factor of 3. Then, the tests of the continuing computation (CC) function in rectangular and cylindrical geometry proves the correctness of the implementation, and it underlines that the initial angle is essential in cylinderical geometry.
At last, two commercial reactor cores are calcualted by NECP-Hydra. First, in the DRF calculation of the 300 MW PWR core, the results of NECP-Hydra and TORT are comparable. The deviations of two code are less than 0.1% in the flux calculation. Compared with the measurement data, the errors of the fast flux (>0.1 MeV and > 1 MeV) at the irradiation monitoring tube are 1.65% and 2.99%, respectively. Moreover, NECP-Hydra achieves much higher efficiency than TORT, taking the advantage of parallelization. Second, NECP-Hydra is applied in the calculation of the nuclear island plant with 1.3 × 108 meshes, 67 energy groups (47 neutron groups + 20 photon groups), totally 2.8 trillion unknowns. This calculation is conducted on Tianhe 2 server using 1230 processors and it finished within 2 h, which shows its computational capability for very large-scale problems.
NECP-Hydra is planned to be extended for unstructured meshes. The effective parallel and communication schemes for unstructured geometry is under investigation.
<Section>CRediT authorship contribution statement</Section>
Yongping Wang: Writing - original draft. Youqi Zheng: Writing - review & editing. Longfei Xu: Software. Liangzhi Cao: Supervision.
<Section>Declaration of Competing Interest</Section>
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
<Section>Acknowledgment</Section>
This work was supported by the National Natural Science Foundation of China (11775170 and 11735011).
<Section>Appendix. Part i detail description of DOS</Section>
The key point of DOS is how to manage multiple tasks the on processors around the “diagonal-line”. The way to determine the “diagonal-line” is illustrated later in this part. Fig. A1 illustrates an example, in which a 6 × 5 2-D processor map is defined (each block is assigned to one processor). The 30 processors are numbered from 0 to 29. The red dots and green dots are the center points of processor map and processor meshes, respectively. Even though there are only four green dots shown in the 6 × 5 processor map, each processor mesh has its own center point.
Fig. A1. 6 × 5 2-D processor map.
If the wave fronts are launched from diagonal corners as illustrated in Fig. A2, they will arrive on the “diagonal-line” processors at the same time, namely, processors 4, 5, 9, 10, 14, 15, 19, 20, 24 and 25 as defined in Fig. A1. In order to manage the multi-tasks on these processors, the processor map is divided into two sub-processor maps using the following procedure:
Fig. A2. Wave fronts from the diagonal corners.
1) Draw the “diagonal line” (+45° or −45°, depending on the octant pair, +45° for (+,+) and (-,-) pair, and −45° for (+,-) and (-,+) pair) through the center point of 2-D processor map as illustrated in Fig. A3-(a) and Fig. A3-(c). The “diagonal line” goes through some processor meshes, therefore, these processor meshes belong to both sub-processor maps.
Fig. A3. Relationship of processor meshes with the sub-processor maps.
2) If one processor mesh belongs to both sub-processor maps, its center point is checked. The processor mesh belongs to the sub-processor map where its center point is located as illustrated in Fig. A3-(b) and Fig. A3-(d).
3) If the center point of a processor mesh just locates on the “diagonal lines”, the following rules should be obeyed:
a. If the center point locates on the + 45° line, the processor mesh belongs to the top-left sub-processor map (Fig. A4-(a));
Fig. A4. Rules for processor meshes whose center points are located on lines.
b. Otherwise, the processor mesh belongs to the bottom-left sub-processor map (Fig. A4-(b)).
With these rules, the “diagonal lines” become the inner boundaries between sub-processor maps.
Each sub-processor map has its own priority sweep octant. Other available tasks will not be calculated until all the directions in the priority octant have been swept. As shown in Fig. A5-(a), octants (+,+,±) are the priority octants for sub-processor map (1). Therefore, the directions in the octants (-,-,±) will not be swept in sub-processor map (1) until all the tasks in octants (+,+,±) have been done. When the wave fronts are launched from another pair of diagonal corners as illustrated in Fig. A5-(b), octants (+,-,±) and octants (-,+,±) are the priority octants for sub-processor map (1) and (2), respectively.
Fig. A5. Priority octants of each sub-processor map.
In the KBA algorithm, the (*, *,±) octants such as octant 1 and 5 in Fig. 2 have the same spatial sweep order for processors, hence the (*, *, ±) octants are pipelined to improve the scaling efficiency. The sub-processor maps are changed dynamically with the sweeping octants. When octants 4, 8, 1 and 5 are swept, the sub-processor map(1) means the bottom-left zone as shown in Fig. A5-(a). However, the top-left zone illustrated in Fig. A5-(b) will be the sub-processor map(1) if the octants 2, 6, 7 and 3 are swept. When the different diagonal-octant pairs are swept, the octant sweep order of sub-processor maps are given in Table A1.
Table A1. Octant sweep order of sub-processor map.
Part II The Scaling Efficiency of DOS
Fig. 4 illustrates 21 steps of sweep evolution to show the dynamics of wave fronts. The 21 consecutive steps contain 12 working steps and 9 idle steps for each processor. Thus, we can derive the scaling efficiency of DOS algorithm is (the detailed derivation is omitted):
@(A1)
The difference of scaling efficiency between the standard KBA algorithm and DOS algorithm is:
@(A2)
where @ and @ are the number of working steps and total steps, respectively.  and  are the computational time and communication time.  is the number of discretized angles in each octant;  is the number of working steps in each octant. Additionally:
@(A3)
where @ and @ are the number of processors in the x and y direction of the processor map, respectively. For example, in the processor map of Fig. 4,  and .
<Section>References</Section>
Azmy and Sartori, 2010
Y. Azmy, E. Sartori
Nuclear computational science: a century in review[M]
Springer Science+Business Media, New York, USA (2010)
Google Scholar
Baker and Koch, 1998
R.S. Baker, K.R. Koch
An SN algorithm for the massively parallel CM-200 computer
Nucl. Sci. Eng., 128 (3) (1998), pp. 312-320
View Record in ScopusGoogle Scholar
Carlson, 1953
Carlson BG. 1953. Solution of the transport equation by the SN method. Los Alamos Scientific Laboratory, United States, LA-1599.
Google Scholar
Chen et al., 2018
J. Chen, Z.Y. Liu, Z. Chen, et al.
A new high-fidelity neutronics code NECP-X
Ann. Nucl. Energy, 116 (2018), pp. 417-428
ArticleDownload PDFView Record in ScopusGoogle Scholar
Cheng et al., 2014
T. Cheng, W. Lei, B. Zhong, et al.
Reconstruction and Parallelization of 3D SN Program for Neutron/Photon Transport
Nucl. Power Eng., 35 (2014), pp. 147-150
View Record in ScopusGoogle Scholar
Engle, K-1693, 1967
Engle JR. 1967. ANISN, a one dimensional discrete ordinates transport code with anisotropic scattering[R]. Oak Ridge National Laboratory, USA, K-1693.
Google Scholar
Koch, 1992
Koch K.R., Baker R.S., Alcouffe R.E. 1992Solution of the first-order form of the three-dimensional discrete ordinates equations on a massively parallel machine. Los Alamos Scientific Laboratory, United States, LA-UR-91-4157.
Google Scholar
Larsen, 1982
E.W. Larsen
Unconditionally stable diffusion synthetic acceleration methods for the slab geoemtry discrete-oridinate equation, Part I: Theory
Nucl. Sci. Eng., 82 (1) (1982), pp. 47-63
CrossRefView Record in ScopusGoogle Scholar
Lathrop and Carlson, 1965
Lathrop K.D., Carlson B.G. 1965. Discrete ordinate angular quadrature of the neutron transport equation[R]. Los Alamos Scientific Laboratory, United States, LA-3186.
Google Scholar
Liu, 2006
Q. Liu
Discrete ordinates solutions of three-dimensional steady and transient neutron transport equations
Xi’an Jiaotong University, Xi’an, China (2006)
Google Scholar
MATXS10, 1994
MATXS10: 30-Group Neutron, 1994. 12-Group Photon Cross Sections from ENDF/B-VI in MATXS Format[R]. Los Alamos National Laboratory.
Google Scholar
On, 1991
Azmy Y. 1991. On the adequacy of message-passing parallel supercomputers for solving neutron trasnport problems. Proceedings of the ACM/IEEE Conference on Supercomputing, New York, USA.
Google Scholar
Rhoades and Simpson, 1997
Rhoades W.A., Simpson D.B. 1997. The TORT three-dimensional discrete ordinates neutron/photon transport code[R]. Oak Ridge National Lab, United States, ORNL/TM-13221.
Google Scholar
Saad, 2003
Y. Saad
Iterative methods for sparse linear systems
SIAM, Philadelphia, PA, USA (2003)
Google Scholar
Saad and Schultz, 1986
Y. Saad, M.H. Schultz
GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems
SIAM J. Sci. Statist. Comput., 7 (1986), pp. 856-869
CrossRefView Record in ScopusGoogle Scholar
Sjoden and Haghighat, 1997
Sjoden G.E., Haghighat A. 1997. PENTRAN – A 3-D Cartesian parallel SN code with angular, energy and spatial decomposition. Proceeding of Joint International Conference. Mathematical Methods and Supercomputing for Nuclear Applications, Saratoga Springs, New York, October 5-9.
Google Scholar
Smith, 1986
Smith, K.S. 1986. Assembly homogenization techniques for the light water reactor analysis. Prog. Nucl. Energy, 17(303).
Google Scholar
Takeda and Ikeda, 1991
T. Takeda, H. Ikeda
3-D neutron transport benchmark
J. Nucl. Sci. Technol., 28 (7) (1991), pp. 656-669
CrossRefView Record in ScopusGoogle Scholar
Wareing et al., 1996
T.A. Wareing, J.M. Mcghee, J.E. Morel
ATTILA: a three-dimensional, unstructured tetrahedral mesh discrete oridinates transport code
Trans. Am. Nucl. Soc., 75 (1996), pp. 146-154
Google Scholar
White et al., 1996
White, J.E., Ingersoll, D.T., Wright, R.Q., et al. 1996. Production and testing of the revised VITAMIN-B6 fing-group and the BUGLE-96 broad-group neutron/photon cross-section libraries derived from ENDF/B-VI.3 nculear data[R]. Oak Ridge National Lab, ORNL-6795/R1.
Google Scholar
Xu et al., 2017
L. Xu, L. Cao, Y. Zheng, et al.
Development of a new parallel SN code for neutron-photon transport calculation in 3-D cylindrical
Prog. Nucl. Energy, 94 (1) (2017), pp. 1-21
ArticleDownload PDFCrossRefView Record in ScopusGoogle Scholar
Zhang et al., 2017
T. Zhang, Y. Wang, E.E. Lewis, et al.
A Three-Dimensional Variational Nodal Method for Pin Resolved Neutron Transport Analysis of Pressurized Water Reactors
Nucl. Sci. Eng., 188 (160) (2017)
Google Scholar
Zhang et al., 2018
T. Zhang, H. Wu, L. Cao, et al.
An improved variational nodal method for the solution of the three dimensional steady-state multi-group neutron transport equation
Nucl. Eng. Des., 337 (2018), pp. 419-427
ArticleDownload PDFView Record in ScopusGoogle Scholar
Zheng, 2013
Z. Zheng
Calculation method for fast neutron fluence calculation of pressurized water reactor pressure vessel and cavity radiation streaming [D].Xi’an
Xi’an Jiaotong University, China (2013)
Google Scholar
Zheng et al., 2020
Q. Zheng, W. Shen, X. Li, et al. 2020. A Hybrid Monte-Carlo-Deterministic Method for AP1000 Ex-core Detector Response Simulation. PHYSOR 2020: Transition to a Scalable Nuclear Future Cambridge, United Kingdom, March 29th-April 2nd.
Google Scholar