<title>Prediction of critical heat flux for narrow rectangular channels in a steady state condition using machine learning</title>
<author>1,HuiyungKim,2,JeongminMoon,3,DongjinHong,4,EuiyoungCha,5,ByongjoYun</author>
<Affiliation>1,Department of Mechanical Engineering, Pusan National University, 2, Busandaehak-ro, 63 Beon-gil, Geumjeong-gu, Busan, 46241, Republic of Korea;2,Department of Information Convergence Engineering, Pusan National University, 2, Busandaehak-ro, 63 Beon-gil, Geumjeong-gu, Busan, 46241, Republic of Korea</Affiliation>
<year>2021</year>
<Jounral>Nuclear Engineering and Technology</Journal>
<Publishing_house>Elsevier</Publishing_house>
<Text_Collector>XiaFan，HEU</Text_Collector>
<DOI>10.1016/j.net.2020.12.007</DOI>
<URL>https://www.sciencedirect.com/science/article/pii/S173857332030975X</URL>
Prediction of critical heat flux for narrow rectangular channels in a steady state condition using machine learning
HuiyungKim,JeongminMoon,DongjinHong,EuiyoungCha,ByongjoYun
Department of Mechanical Engineering, Pusan National University, 2, Busandaehak-ro, 63 Beon-gil, Geumjeong-gu, Busan, 46241, Republic of Korea
Department of Information Convergence Engineering, Pusan National University, 2, Busandaehak-ro, 63 Beon-gil, Geumjeong-gu, Busan, 46241, Republic of Korea
<Section>Abstract</Section>
The subchannel of a research reactor used to generate high power density is designed to be narrow and rectangular and comprises plate-type fuels operating under downward flow conditions. Critical heat flux (CHF) is a crucial parameter for estimating the safety of a nuclear fuel; hence, this parameter should be accurately predicted. Here, machine learning is applied for the prediction of CHF in a narrow rectangular channel. Although machine learning can effectively analyze large amounts of complex data, its application to CHF, particularly for narrow rectangular channels, remains challenging because of the limited flow conditions available in existing experimental databases. To resolve this problem, we used four CHF correlations to generate pseudo-data for training an artificial neural network. We also propose a network architecture that includes pre-training and prediction stages to predict and analyze the CHF. The trained neural network predicted the CHF with an average error of 3.65% and a root-mean-square error of 17.17% for the test pseudo-data; the respective errors of 0.9% and 26.4% for the experimental data were not considered during training. Finally, machine learning was applied to quantitatively investigate the parametric effect on the CHF in narrow rectangular channels under downward flow conditions.
Keywords:Narrow rectangular channel;Critical heat flux;Machine learning;Neural network;Downward flow
<Section>1. Introduction</Section>
Plate-type fuel is employed to realize a high cooling efficiency in the core of new Korean research reactors with a high neutron density. Sub-channels that consist of plate-type fuels have a narrow rectangular structure as shown in Fig. 1; further, they exhibit different thermal–hydraulic phenomena compared with a circular tube owing to the different geometries of flow channels. Critical heat flux (CHF) is a crucial parameter among thermal–hydraulic phenomena and can be used to determine the threshold of flow boiling for the nuclear fuel. Therefore, an accurate CHF prediction is essential to realize the optimal design and safety evaluation of nuclear reactors. The parameters affecting the CHF in a narrow rectangular channel include hydraulic characteristics (e.g., mass flux, subcooling, and pressure [[1], [2], [3], [4], [5], [6], [7]]), geometric characteristics (e.g., wetted width, heated width, and gap size [[6], [7], [8], [9], [10]]), and heating wall characteristics (e.g., thermal properties and thickness of the walls [11]). However, systematic research has rarely been conducted on the effects of each parameter. Although several studies have attempted to investigate the CHF in a narrow rectangular channel, it is difficult to clearly understand the phenomena associated with heater properties and the geometries of rectangular channels. In fact, conducting an experimental investigation on the influence of the geometric and heating element parameters on the CHF in a narrow rectangular channel is practically difficult, because the test section must be designed with various sizes and thicknesses; further, the heater should be fabricated using various materials.
Fig. 1. Schematics of the narrow rectangular channel: (a) top view, (b) side view, (c) bird’s-eye view.
Machine learning is a highly successful data-driven approach for analyzing specific systems or for developing inference models. This approach can be advantageous when analyzing scattered CHF data in terms of thermal–hydraulic and geometrical parameters in a narrow rectangular channel. Previously, some researchers used machine learning to analyze and predict various thermal–hydraulic phenomena [[12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34]]. Multi-layer perceptron (MLP) and support vector machine (SVM) have been adopted to predict the CHF in studies by Moon et al. [12], Wei et al. [14], Cai [15], Jiang and Zhao [16], and He and Lee [34]. Since the convolutional neural network (CNN) was developed, deep learning algorithms based on convolutional operations have been reported to exhibit good performance in various applications [[35], [36], [37], [38], [39]]. Additionally, deep learning showed higher problem-solving performance in most cases than traditional methods (e.g. ILSVRC; ImageNet Large Scale Visual Recognition Challenge) [[40], [41], [42], [43]].
In general, convolutional operations require less computational capacity than traditional MLP but have a large network capacity. Thus, good performance will be obtained if a model that predicts CHF using a deep learning algorithm is applied to CHF prediction because of the large network capacity.
Therefore, the latest machine learning techniques should be applied to enhance the prediction accuracy of the CHF. When using advanced machine learning techniques, the effects of the various abovementioned parameters on the CHF are expected to be adequately analyzed without experimental investigation.
In the present study, a machine learning model based on a neural network considering thermal–hydraulic phenomena was developed, particularly to predict the CHF. In this regard, the proposed model was validated and evaluated against experimental data that are not used for its training. Finally, it was applied to investigate the parametric effect on the CHF in narrow rectangular channels under downward flow conditions.
<Section>2. Application of machine learning for the CHF prediction</Section>
2.1. Machine learning-based prediction method
Conventionally, the CHF is predicted using physical models or correlations developed based on prior knowledge or understanding of the CHF mechanism. In particular, the empirical correlation was developed using experimental data and exhibited excellent accuracy for the dataset used for its development. However, the performance of this method in terms of interpolation capability is low outside the experimental conditions.
In contrast, machine learning follows a data-driven black-box process that characterizes the available dataset and not the physical phenomena. The machine learning results depend on the quality of the dataset applied for the training; hence, high-quality data are essential for the successful application of machine learning.
Among the available machine learning approaches, artificial neural networks have been successfully employed in several fields. If the data are well sampled to characterize the system, it can predict the outcome with high accuracy for specific systems; thus, it is useful for solving several complicated engineering problems. In the present study, a neural network was adopted because the neural network can successfully predict the CHF using a dataset comprising many input parameters without any knowledge regarding physical phenomena or mechanisms. Additionally, it can analyze the tendency of each parameter which has a complicated contribution to the CHF in the wide range of flow and geometry conditions. To generalize and apply machine learning to predict the CHF in a narrow rectangular channel, datasets corresponding to a wide range of flow conditions are required. However, datasets including parameters related to CHF in the rectangular channel are limited; hence, the amount of available data is extremely small to train the neural network. To overcome this problem, we generated pseudo-data for the training of the neural network from the available correlations by considering their applicable flow and geometric conditions instead of experimental data. In addition, the applicable conditions of such correlations are determined by the experimental flow conditions of the data used for the development of each correlation.
2.2. Existing CHF correlations for narrow rectangular channels
To obtain the pseudo-data to train the artificial neural network, proper models and correlations should be selected that represent the CHF in the narrow rectangular channel. For this purpose, the following four existing correlations widely used for prediction of CHF in the narrow rectangular channel were considered.
Mirshak et al. [1] proposed the empirical correlation that can be applied to a narrow rectangular channel with one-side heating under a wide range of downflow conditions as follows:
@(1)
They concluded that the CHF in the narrow rectangular channels is similar to that in tubes under their test conditions and that the CHF is only affected by the outlet conditions.
Kaminaga et al. [5] proposed a CHF correlation scheme that enables prediction by dividing the correlation into three regions according to the dimensionless mass flux as shown in Eqs. (2), (3), (4). The correlations are applicable to rectangular channels under both downflow and upflow condition as described via Eqs. (5), (6), respectively.
@(2)
@(3)
@(4)
@(5)
@(6)
Kureta and Akimoto [6] devised a CHF correlation for a short, narrow rectangular channel. The applicable conditions of the correlation cover a wide range of mass fluxes and subcooling temperatures as well as various mini channel sizes based on the copper film heater. They found that the dimensionless CHF parameter K+ () increases as the critical quality decreases, which reflects the thermodynamic quality at the CHF occurring location in the channel. The correlation was obtained through a regression analysis between the critical quality and K+ with an available CHF dataset as follows:
@(7)
Tanaka et al. [10] determined a CHF correlation for the various narrow rectangular channels. The correlation was developed based on the CHF experimental dataset obtained for various channel gaps, heated widths, wetted widths, and lengths as follows:
@(8)
This correlation includes the geometric characteristics of the narrow rectangular channel. However, the applicable range for the mass flux, which influences the CHF most, is limited to low flow condition in the range 0–200 kg/m2s.
The overall application conditions, which include the flow channel geometry for each of the abovementioned correlations, are listed in Table 1. Each correlation has different applicable conditions and exhibits good agreement with the experimental data used for the development of each correlation. Therefore, the pseudo-data can be expected to represent a wide range of flow and geometric conditions for CHF in the narrow rectangular channel.
Table 1. Application conditions of the existing correlations.
2.3. Pseudo CHF data for machine learning
The pseudo-data are randomly generated using the thermal–hydraulic parameters corresponding to each correlation in applicable conditions as follows:
@(9)
where f is a CHF correlation. The range of the corresponding thermal–hydraulic parameters is determined by the conditions of the experimental data used for the development of each correlation. Therefore, the pseudo-data generated by above correlations cover hydraulic diameters in the range 0.39–19.10 mm; lengths, 50–760 mm; pressures, 100–4,000 kPa; mass fluxes, −25,800–19,740 kg/m2s; inlet subcoolings, 3–104 K; effectivities, 6,544–35,950 J/m2 K s0.5; and heater thicknesses, 0.005–1 mm. The accuracy of each correlation can be evaluated before training a neural network using pseudo-data. Fig. 2 shows the evaluation results of the pseudo-data for each correlation with the root-mean-square error defined as follows:
@(10)
where@is the predicted CHF,@is a CHF, and i is an index. The prediction results demonstrate that each correlation can only predict the pseudo-data generated by the same correlation. This implies that the previous correlations are applicable only to their own configuration of channel geometry and flow condition and that the application of the neural network is necessary to predict CHF with a wide range of prediction capabilities. In the next section, we develop a neural network model that can quantitatively analyze the trend of CHF based on the pseudo-data.
Fig. 2. Generated pseudo CHF data and the predicted CHF using correlations by (a) Mirshak et al., (b) Kaminaga et al., (c) Kureta and Akimoto, and (d) Tanaka et al.
<Section>3. Architecture of the neural network</Section>
Neural networks are machine learning algorithms frequently used for regression and prediction in engineering [[44], [45], [46], [47], [48], [49], [50], [51]]. Therefore, in this study, a neural network was adopted to develop a machine learning model that exhibits a high regression performance with respect to thermal–hydraulic parameters. The proposed neural network architecture is illustrated in Fig. 3 and its specifications are summarized in Table 2. The neural network is based on the CNN that performs the convolution operation using a number of filters, which are generally called kernels. In Table 2, the “kernel size” is a filter size that is used for the convolution. In addition, the “stride” is the moving step of the convolution operation and the “padding” is the addition of zeros to the edges to prevent the matrix from shrinking during the convolution operation. The “same” means that the size of the matrix is constant, “valid” means that the size of the matrix is reduced, and the “output” represents the number of feature maps.
Fig. 3. Proposed neural network architecture.
Table 2. Specifications of the proposed neural network architecture.
The network can be divided into the input, deep belief network (DBN) [52], residual network (ResNet) [42], and result stages. The DBN is an unsupervised learning network that performs feature extraction, whereas the ResNet performs regression based on supervised learning. Since the present study aims to accurately predict the CHF, we have determined a large number of layers and blocks.
By placing a DBN that can be used as a feature extractor in the front of ResNet, it was expected to find a feature that would improve the accuracy of CHF prediction even if the feature cannot be understood by investigators. The reason for predicting the CHF by providing features extracted from the DBN as the input of ResNet is that it can create a deeper network structure than MLP; however, it takes up a smaller number of weight parameters. In MLP, the weight of each layer is fully connected from the previous layer and is affected by all features. However, because of problems such as gradient vanishing/exploding, the network structure cannot be deepened. Thus, learning using a network with a larger network capacity than MLP would improve the prediction performance. Therefore, we adopted ResNet, which is known to be robust against gradient vanishing/exploding and can have a structure of 100 layers or more.
3.1. Input and output parameters for the neural network
Each correlation used to generate the pseudo-data possessed its own specific parameter for CHF prediction; the number of parameters considered is also different. However, the number of input and output parameters for the neural network must remain unchanged, regardless of the required parameters for each correlation. Therefore, to consider all parameters that affect the CHF, we prepared generalized parameters, including their ranges, which are listed in Table 3. The input parameters are divided into thermal–hydraulic, geometric, and heating element parameters. This study focuses on the CHF in a narrow rectangular channel; however, these parameters can also be applied to different flow channel geometries, including circular tubes and annuli. During training, normalization is performed to avoid the gradient vanishing or explosion. Although min–max normalization is typically adopted for a classification problem, we used a log normalization, as expressed below, to solve the regression problem:
@(11)
Table 3. Generalized parameters for the proposed neural network.
In the above equation, each weight represents the power of a parameter, and the bias acts as a coefficient for each parameter. This is similar to the formulation of thermal–hydraulic correlations, which mainly consists of the coefficient and the power of the parameter.
3.2. Deep belief network
The DBN is a generative graphical model that is used in machine learning and deep neural networks. It consists of multiple layers of latent variables, and it is appropriate for pre-training due to its generative characteristics. DBN is a stack of several restricted Boltzmann machines (RBMs) which is a generative stochastic neural network that can train a probability distribution over its input data. In the proposed structure, the DBN is at the front of the entire model, and it creates feature nodes to use the input parameters as input for the CNN structure. Each RBM layer in DBN is trained to compute a hidden layer (output) that creates a visible layer (input) with a high probability. After the DBN is trained, the weights obtained by the DBN can be fine-tuned through a backpropagation algorithm to increase the performance of the model. This feature is considerably useful when the number of training samples is small. Furthermore, when the amount of available training data is limited, the random initial weights substantially influence the trained model. On the contrary, the pre-training weights of the DBN are closer to the optimal values, as compared to the random values. This improves the prediction performance and shortens the learning time during fine-tuning.
We trained the DBN shown in Fig. 4 through unsupervised learning for each layer consisting of a restricted Boltzmann machine (RBM), which is an energy-based generative model based on an undirected bipartite graph with visible and hidden units. In the RBM, each visible layer is connected to all of the hidden units; However, there is no connection between the units within a visible layer and a hidden layer, as shown in Fig. 5. After the RBM training, additional RBMs are stacked on top to form a multilayer model. Thereafter, each previously trained RBM is used as the input for the subsequent RBM until all of the RBMs in the stack are trained (i.e., the wake-sleep algorithm [53]). In RBM, the energy function is defined as follows:
@(12)
where a is the bias of the visible layer, b is the bias of the hidden layer, and w is the weight set between the visible and hidden nodes. Through the sum of the marginal probabilities, the probability, p(x), can be obtained as follows:
@(13)
where h’ is a state that hidden layer nodes can have. The probability of a sample in RBM is determined by the parameter set, θ = {w, a, b}. Therefore, the objective function@maximizes the probability p(x) as follows:
@(14)
Fig. 4. Proposed DBN structure.
Fig. 5. RBM structure for the DBN.
In this case, multiplying the probability p(x) several times can cause underflow (condition in which the result of a calculation is a number of smaller absolute values than the computer can actually be represented); thus, we applied the log to Eq. (14) for conversion to log-likelihood as follows:
@(15)
In the process of training to maximize the log-likelihood, the gradient of log-likelihood to determine the value to update the parameter set θ is as follows:
@(16)
where p is the probability, θ is the parameter set, h is the hidden feature, and x is the visible feature. The first term on the right-hand side in the above equation can be calculated easily; however, the second term is complicated because the calculation must be performed for all possible combinations of x and h. Therefore, a gradient approximation called contrastive divergence is introduced for training, such as the RBM. Contrasting divergence is a method wherein Markov Chain Monte Carlo (MCMC) is performed only once [54,55]; instead, the MCMC is performed iteratively until the calculation of p (x,h) converges. MCMC constructs a Markov chain through Monte Carlo sampling when the Markov chain has a stationary distribution. In this case, the Markov chain is constructed in such a way that the samples obtained from the chain follow the posterior distribution of the parameters as they converge to a steady state. Therefore, MCMC is used to construct the stationary distribution of the Markov chain, which is the same as the posterior distribution of the parameters [56]. The training results of the RBMs can be evaluated based on a contrastive divergence loss [55,57]. In the proposed architecture, a 3-layer DBN is used for the pre-training. The DBN performance can generally increase with the number of layers; however, when using more than three layers, the improvement in the prediction is small compared to the required learning time. Therefore, we set a 3-layer DBN, with the output of each layer being 1D data. We reshaped the output of the pre-training stage into 2D data with a 64 × 64 size before applying the ResNet.
3.3. ResNet
The performance of a CNN improves on implementing a deep architecture. However, as the depth increases, degradation problems, which reduce network accuracy, can occur. A ResNet prevents such degradation by adopting residual learning. In general, training is performed to minimize the difference between the value F(x) of the neural network output layer for the x of input value and the y of target value. However, training of ResNet is performed to minimize the objective function, which is the difference between F(x)-x and y. Because it learns F(x)-x rather than F(x), the researchers who developed ResNet used the term “residual learning” to explain the model. Residual learning can help train each layer; it ensures that the layer is sensitive to small changes by training the difference between the input and output (residual) rather than training the outputs. This type of training is implemented by adding a shortcut between the input and output, such that additional parameters are not required to maintain computational efficiency. ResNet is a structure in which residual blocks are stacked. We used 14 residual blocks with pre-activation, as demonstrated in Fig. 6; the structure of a residual block presented in Fig. 7. Each residual block consists of batch normalization, rectified linear unit activation, CNNs between two sets, and a shortcut connection.
Fig. 6. ResNet structure.
Fig. 7. Residual block structure. (batch normalization (BN); rectified linear unit (ReLU); convolutional neural network (CNN)).
<Section>4. Results and discussion</Section>
4.1. Training of the developed neural network
We trained the proposed network in two steps. First, the DBN was trained to determine the characteristics of data. As the DBN is a probabilistic model without a convergence condition, DBN training was performed over 3,000 epochs, which is sufficiently large to reach convergence with a contrastive divergence loss [52,55,57], as shown in Fig. 8. Second, we trained the ResNet to predict the CHF by using the results of the DBN and a mini-batch size of 500.
Fig. 8. Contrastive divergence loss of the DBN for each attempt.
To verify the trained neural network, we applied a holdout cross-validation, which is commonly used for neural networks when they are trained with large datasets [58,59]. The dataset was divided into three disjoint sets: a training dataset, a validation dataset for evaluating the neural network during training, and a test dataset for evaluating the prediction capability of the neural network after training. The validation set was separated in order to minimize generalization error, which is the prediction error for previously unseen data [60]. The training dataset consisted of 73,000 pseudo-data. To include additional geometric information about the flow channel, 12 of the experimental data, which were obtained from Kim et al. [7] (Table 4), corresponding to the lowest CHF and the highest CHF of each flow channel, were added to the training dataset. The validation and test datasets consisted of 21,000 and 6,000 pseudo-data, respectively. The training conditions are tabulated in Table 5.
Table 4. Conditions of experimental data included in the training dataset (Kim et al. [7]).
Table 5. Training conditions of the neural network for the CHF prediction.
4.1.1. Evaluation of the trained neural network
Fig. 9 presents a comparison of the CHF pseudo-data for the test and the CHF predictions according to the previous MLP (Table 6) and the developed neural network with (present model) and without the pre-training of DBN (Table 7); Table 8 lists the prediction errors. Unlike the correlations shown in Fig. 2, neural networks can successfully predict all pseudo-data. The average error (Eq. (17)) and root-mean-square (RMS) error (Eq. (10)) of the proposed neural network model are 3.65% and 17.17%, respectively. In particular, the 3-layer MLP and the MLP with residual blocks used in previous studies exhibit significant scattering in the prediction of the low-value CHF well. However, the present model predicts the low-value CHF well. Compared with the MLP, which is the basic structure of a neural network, the proposed neural network predicts the CHF with an acceptable accuracy over a wide range of flow conditions; it also achieves a higher prediction performance compared to the MLP.
@(17)
Fig. 9. Test pseudo CHF data and predicted CHF in the full range (a) and in the low CHF region (b).
Table 6. Specifications of the MLP architecture.
Table 7. Developed neural network without pre-training.
Table 8. Prediction errors of the neural networks.
Fig. 10 shows the mean squared errors of the training and validation data during training, which is expressed as follows:
@(18)
Fig. 10. Mean-square error obtained from the training and validation data.
The errors for both datasets decrease gradually as the epoch increases. They reach a value along the order of 10−6, and the variance, which is the difference between the errors from both datasets, is also small. Hence, the trained neural network is successfully optimized without overfitting the training data.
4.2. Validation of the trained neural network
The neural network was successfully trained and suitably optimized as indicated by the 3.65% average error and 17.2% RMS error for pseudo-data. This verifies that the neural network has been trained appropriately. However, only 12 experimental data are used for training, and most of the data are composed of pseudo-data; hence, the trained neural network may not be valid for predicting real CHF values. Moreover, the process of repeatedly evaluating the neural network for the development dataset (pseudo-data) causes the model to gradually “overfit” the development dataset [61]. Therefore, the model must be validated against experimental data that is maintained in a “vault” to evaluate the generalization error of the final trained model [62]. In addition, we also compared the neural network with conventional CHF correlations for narrow rectangular channels. Fig. 11 shows the experimental and predicted CHF values obtained from existing correlations and the proposed neural network. Table 9 lists the conditions of the experimental data [7] used for the validation, and Table 10 lists the errors in the predictions of the correlations and the neural network. The present neural network predicts the experimental CHF data within an average error of 0.9% and an RMS error of 26.4%. The results of the neural network are in good agreement with the experimental data that were not used for training; thus, the trained neural network has been validated. As the trained neural network reasonably predicts the experimental data that are not used for training, as shown in Fig. 11, it can be concluded that the trained neural network can be employed to quantitatively analyze parameters that affect the CHF under various conditions and ranges.
Fig. 11. Experimental and predicted CHF for (a) the overall prediction and (b) the low CHF region.
Table 9. Conditions of the experimental data for the neural network validation (Kim et al. [7]).
Table 10. Errors obtained from the existing correlations and the proposed neural network.
4.3. Parametric analysis of CHF in the narrow rectangular with the developed neural network
We performed a parametric analysis using the neural network by calculating the CHF for the narrow rectangular channel under conditions that would be difficult to implement in experimental investigations. Table 11 lists the reference conditions for this parametric analysis. In addition, the CHF predictions based on the correlations of Kaminaga et al. [5] and Kim et al. [7] are applicable to the two-sided heating condition for the research reactor, and they are also shown in each figure.
Table 11. Reference conditions for the CHF parametric analysis.
Fig. 12 presents the results of the parametric analyses of the mass flux, pressure, and inlet temperature. As shown in Fig. 12a, as the mass flux increases, the CHF increases. When predicting the CHF by using the neural network, the difference in the CHF according to the flow direction is not significant for a mass flux exceeding 3000 kg/m2s, and it varies according to the mass flux under low flow condition. The predicted value of the CHF in the neural network lies between the predictions by the correlations of Kaminaga et al. and Kim et al. It is similar to those of Kaminaga et al. for the upflow condition and similar to those of Kim et al. for the downflow condition. As shown in Fig. 12b, as the pressure increases, the CHF increases almost linearly from 170 to 500 kPa, and this tendency is independent of the mass flux. Furthermore, Fig. 12c indicates that the CHF decreases as the inlet temperature increases, regardless of the mass flux. In addition, the decrease in CHF due to the increase in inlet temperature of the neural network was smaller than that of the existing correlation. Overall, the effects of mass flux, pressure, and inlet temperature on the CHF obtained from our analyses are consistent with existing correlations [[2], [3], [4], [5], [6], [7], [8], [9], [10]].
Fig. 12. Analysis results for the main thermal–hydraulic parameters: (a) mass flux, (b) pressure, and (c) inlet temperature.
The geometric parameters that affect the CHF are channel length, width, and gap size. Fig. 13 shows the results of the parameter analysis for the geometry of a rectangular channel. Fig. 13a shows that the CHF decreases as the channel length increases. However, the channel length has a smaller effect on the CHF in the neural network than in the existing correlations. As shown in Fig. 13b, the effect of the wetted width on the CHF is negligible, which is consistent with the experimental results by Kureta and Akimoto [8]. In this instance, the heated width was determined to be 0.931 (= 62/66.6) times the wetted width by following the typical design for a research reactor. Fig. 13c shows that, unlike the channel width, the channel gap has a considerable effect on the CHF. The Kaminaga correlation does not consider the effect of the channel gap; hence, it yields a constant CHF. On the contrary, the CHF predicted by the neural network increases with the channel gap, where it reaches an asymptotic CHF. This is similar to the results obtained by Kim et al. [7] and Kureta and Akimoto [8] under forced convection, and Xia et al. [63] and Kim and Suh [64] under natural convection. In previous studies [7,8,63,64], the CHF was found to decrease steeply when the gap size was less than the asymptotic thickness. However, when using the neural network, the CHF decreases steeply at a certain channel gap size. This may be caused by the bubble behavior near the heated wall, among other reasons. Moreover, the bubble departure diameter is estimated to be approximately 0.05–1 mm by using the Staub model [65] under the present flow conditions. As a bubble is generated from both heated walls, the neural network estimates that the CHF decreases steeply by nearly 2 mm.
Fig. 13. Analysis results for the geometric parameters of the rectangular channel: (a) length, (b) width, (c) and gap.
The heating element parameters that affect the CHF include the thickness and effusivity of the material. Fig. 14 shows the parameter analysis results for the heating element parameters. As shown in Fig. 14a, the CHF increases with the heater thickness. This is similar to the results by Del Valle [11] under forced convection and Tachibana et al. [66] and Guglielmini and Nannei [67] under pool boiling. Fig. 14b indicates that the neural network prediction exhibits a rather complicated tendency for the CHF according to the effusivity. This is because most of the correlations used to generate the pseudo-data are based on the experimental data obtained using stainless steel heaters. Therefore, there is a lack of information regarding other heater materials. Experimental CHF data that are based on a variety of heater properties are necessary to ensure more successful training for the effusivity effect.
Fig. 14. Analysis results for the heating element parameters: (a) thickness and (b) effusivity.
<Section>5. Conclusions</Section>
In this study, machine learning was applied to achieve wide-range prediction capability of a steady-state CHF in a narrow rectangular channel. For this purpose, we developed a novel neural network that considers multiple regression analyses for predicting the CHF. The proposed neural network can be summarized as follows:
1.
Conducting an experimental investigation on the influence of all parameters on the CHF is difficult under practical scenarios. To quantitatively analyze these influences, the latest neural network among machine learning techniques was employed.
2.
The developed neural network is composed of DBN and ResNet. DBN is composed of latent variables of several layers, and it is suitable for extracting features of the dataset; hence, it was used for pre-training. ResNet was used for prediction because it prevents degradation problems, which reduce network accuracy, through residual learning.
3.
A pseudo-dataset was used for training the neural network that was generated using four existing CHF correlations in order to cover a wide range of applicable conditions involving thermal–hydraulic, geometric, and heater element parameters.
4.
As a result of training a neural network with a training dataset based on pseudo-data and 12 experimental data, the trained neural network predicted the test pseudo-data with an RMS error of 17.2%; it also achieved a better performance than previous neural networks.
5.
To validate the novel neural network trained with the pseudo-dataset, we compared it to existing CHF correlations and experimental data applicable to narrow rectangular channels. The neural network exhibited the best agreement with the experimental data, with an average error of 0.9% and an RMS error of 26.4%. In addition, the trained neural network could accurately predict experimental CHF data that were not used during its training.
6.
The trained neural network is employed for parametric analyses on the CHF of a rectangular channel. It shows a reasonable analysis capability for the effect of each parameter that has a complex contribution toward the CHF in narrow rectangular channels.
Although the neural network achieves a good performance in predicting the CHF, it does not directly consider CHF mechanism. To overcome this drawback, a mechanistic CHF model based on parametric studies using the neural network will be developed in our future research.
<Section>Declaration of competing interest</Section>
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
<Section>Acknowledgments</Section>
This work was supported by the Nuclear Safety Research Program through the Korea Foundation of Nuclear Safety, South Korea, a grant from the Nuclear Safety and Security Commission, South Korea (Grant No. 1903001), and a grant from the Nuclear Research & Development Program of the National Research Foundation of Korea, South Korea funded by the Ministry of Science, ICT and Future Planning, South Korea (Grant No. NRF-2019M2D2A1A03056998).
<Section>References</Section>
[1]
S. Mirshak, W.S. Durant, R.H. Towell
Heat Flux at Burnout, No. DP-355
Du Pont de Nemours (EI) & Co. Savannah River Lab., Augusta, Ga (1959)
Google Scholar
[2]
Y. Sudo, K. Miyata, H. Ikawa, M. Kaminaga, M. Ohkawara
Experimental study of differences in DNB heat flux between upflow and downflow in vertical rectangular channel
J. Nucl. Sci. Technol., 22 (8) (1985), pp. 604-618
CrossRefView Record in ScopusGoogle Scholar
[3]
M. Kaminaga, Y. Sudo
A new CHF correlation scheme proposed for vertical rectangular channels heated from both sides in nuclear research reactors
The 1st JSME/ASME Joint International Conference on Nuclear Engineering (1991)
Google Scholar
[4]
Y. Sudo
Study on critical heat flux in rectangular channels heated from one or both sides at pressures ranging from 0.1 to 14 MPa
J. Heat Tran., 118 (3) (1996), pp. 680-688
CrossRefView Record in ScopusGoogle Scholar
[5]
M. Kaminaga, K. Yamamoto, Y. Sudo
Improvement of critical heat flux correlation for research reactors using plate-type fuel
J. Nucl. Sci. Technol., 35 (12) (1998), pp. 943-951
View Record in ScopusGoogle Scholar
[6]
M. Kureta, H. Akimoto
Critical heat flux correlation for subcooled boiling flow in narrow channels
Int. J. Heat Mass Tran., 45 (20) (2002), pp. 4107-4115
ArticleDownload PDFView Record in ScopusGoogle Scholar
[7]
H.Y. Kim, J.Y. Bak, J.J. Jeong, J.H. Park, B.J. Yun
Investigation of the CHF correlation for a narrow rectangular channel under a downward flow condition
Int. J. Heat Mass Tran., 130 (1) (2019), pp. 60-71
ArticleDownload PDFView Record in ScopusGoogle Scholar
[8]
M. Kureta, H. Akimoto
Critical heat flux of subcooled flow boiling in narrow rectangular channels
Proceedings of the 6th International Conference on Nuclear Engineering, ICONE-7, Tokyo, Japan (1999), p. 7016
Google Scholar
[9]
F. Tanaka, T. Hibiki, Y. Saito, T. Takeda, K. Mishima
Thermal-hydraulic Design Concept of the Solid-Target System of Spallation Neutron Source
(2001)
Google Scholar
[10]
F. Tanaka, T. Hibiki, K. Mishima
Correlation for flow boiling critical heat flux in thin rectangular channels
J. Heat Tran., 131 (12) (2009), p. 121003
Google Scholar
[11]
V.H. Del Valle
An experimental study of critical heat flux in subcooled flow boiling at low pressure including the effect of wall thickness
Paper Presented at the ASME·JSME Thermal Engineering Joint Conference, Honolulu, Hawaii (1983)
Google Scholar
[12]
S.K. Moon, W.P. Baek, S.H. Chang
Parametric trends analysis of the critical heat flux based on artificial neural networks
Nucl. Eng. Des., 163 (1–2) (1996), pp. 29-49
ArticleDownload PDFView Record in ScopusGoogle Scholar
[13]
T.B. Trafalis, O. Oladunni, D.V. Papavassiliou
Two-phase flow regime identification with a multiclassification support vector machine (SVM) model
Ind. Eng. Chem. Res., 44 (12) (2005), pp. 4414-4426
CrossRefView Record in ScopusGoogle Scholar
[14]
H. Wei, G.H. Su, S.Z. Qiu, W. Ni, X. Yang
Applications of genetic neural network for prediction of critical heat flux
Int. J. Therm. Sci., 49 (1) (2010), pp. 143-152
ArticleDownload PDFView Record in ScopusGoogle Scholar
[15]
J. Cai
Predicting the critical heat flux in concentric-tube open thermosiphon: a method based on support vector machine optimized by chaotic particle swarm optimization algorithm
Heat Mass Tran., 48 (8) (2012), pp. 1425-1435
CrossRefView Record in ScopusGoogle Scholar
[16]
B.T. Jiang, F.Y. Zhao
Combination of support vector regression and artificial neural networks for prediction of critical heat flux
Int. J. Heat Mass Tran., 62 (2013), pp. 481-494
ArticleDownload PDFView Record in ScopusGoogle Scholar
[17]
M. Ma, J. Lu, G. Tryggvason
Using statistical learning to close two-fluid multiphase flow equations for a simple bubbly system
Phys. Fluids, 27 (9) (2015), Article 092101
CrossRefView Record in ScopusGoogle Scholar
[18]
W. Chang, X. Chu, A.F.B.S. Fareed, S. Pandey, J. Luo, B. Weigand, E. Laurien
Heat transfer prediction of supercritical water with artificial neural networks
Appl. Therm. Eng., 131 (2018), pp. 815-824
ArticleDownload PDFView Record in ScopusGoogle Scholar
[19]
P. Amani, K. Vajravelu
Intelligent modeling of rheological and thermophysical properties of green covalently functionalized graphene nanofluids containing nanoplatelets
Int. J. Heat Mass Tran., 120 (2018), pp. 95-105
ArticleDownload PDFView Record in ScopusGoogle Scholar
[20]
H.S. Lim, Y.T. Kang
Estimation of finish cooling temperature by artificial neural networks of backpropagation during accelerated control cooling process
Int. J. Heat Mass Tran., 126 (2018), pp. 579-588
ArticleDownload PDFView Record in ScopusGoogle Scholar
[21]
M.H. Ahmadi, A. Tatar, M.A. Nazari, R. Ghasempour, A.J. Chamkha, W.M. Yan
Applicability of connectionist methods to predict thermal resistance of pulsating heat pipes with ethanol by using neural networks
Int. J. Heat Mass Tran., 126 (2018), pp. 1079-1086
ArticleDownload PDFView Record in ScopusGoogle Scholar
[22]
G.M. Hobold, A.K. da Silva
Machine learning classification of boiling regimes with low speed, direct and indirect visualization
Int. J. Heat Mass Tran., 125 (2018), pp. 1296-1309
ArticleDownload PDFView Record in ScopusGoogle Scholar
[23]
H. Wei, S. Zhao, Q. Rong, H. Bao
Predicting the effective thermal conductivities of composite materials and porous media by machine learning methods
Int. J. Heat Mass Tran., 127 (2018), pp. 908-916
ArticleDownload PDFView Record in ScopusGoogle Scholar
[24]
A. Shahsavar, S. Khanmohammadi, A. Karimipour, M. Goodarzi
A novel comprehensive experimental study concerned synthesizes and prepare liquid paraffin-Fe3O4 mixture to develop models for both thermal conductivity & viscosity: a new approach of GMDH type of neural network
Int. J. Heat Mass Tran., 131 (2019), pp. 432-441
ArticleDownload PDFView Record in ScopusGoogle Scholar
[25]
G.M. Hobold, A.K. da Silva
Automatic detection of the onset of film boiling using convolutional neural networks and Bayesian statistics
Int. J. Heat Mass Tran., 134 (2019), pp. 262-270
ArticleDownload PDFView Record in ScopusGoogle Scholar
[26]
A. Zendehboudi, R. Saidur, I.M. Mahbubul, S.H. Hosseini
Data-driven methods for estimating the effective thermal conductivity of nanofluids: a comprehensive review
Int. J. Heat Mass Tran., 131 (2019), pp. 1211-1231
ArticleDownload PDFView Record in ScopusGoogle Scholar
[27]
Y. Li, H. Wang, X. Deng
Image-based reconstruction for a 3D-PFHS heat transfer problem by ReConNN
Int. J. Heat Mass Tran., 134 (2019), pp. 656-667
ArticleDownload PDFCrossRefView Record in ScopusGoogle Scholar
[28]
L. Yang, W. Dai, Y. Rao, M.K. Chyu
Optimization of the hole distribution of an effusively cooled surface facing non-uniform incoming temperature using deep learning approaches
Int. J. Heat Mass Tran., 145 (2019), p. 118749
ArticleDownload PDFView Record in ScopusGoogle Scholar
[29]
A. Baghban, M. Kahani, M.A. Nazari, M.H. Ahmadi, W.M. Yan
Sensitivity analysis and application of machine learning methods to predict the heat transfer performance of CNT/water nanofluid flows through coils
Int. J. Heat Mass Tran., 128 (2019), pp. 825-835
ArticleDownload PDFView Record in ScopusGoogle Scholar
[30]
G.M. Hobold, A.K. da Silva
Visualization-based nucleate boiling heat flux quantification using machine learning
Int. J. Heat Mass Tran., 134 (2019), pp. 511-520
ArticleDownload PDFView Record in ScopusGoogle Scholar
[31]
X. Zhao, K. Shirvan, R.K. Salko, F. Guo
On the prediction of critical heat flux using a physics-informed machine learning-aided framework
Appl. Therm. Eng., 164 (2020), p. 114540
ArticleDownload PDFView Record in ScopusGoogle Scholar
[32]
Y. Liu, T.N. Dinh, Y. Sato, B. Niceno
Data-driven modeling for boiling heat transfer: using deep neural networks and high-fidelity simulation results
Appl. Therm. Eng., 144 (2018), pp. 305-320
ArticleDownload PDFView Record in ScopusGoogle Scholar
[33]
M. Ravichandran, M. Bucci
Online, quasi-real-time analysis of high-resolution, infrared, boiling heat transfer investigations using artificial neural networks
Appl. Therm. Eng., 163 (2019), p. 114357
ArticleDownload PDFView Record in ScopusGoogle Scholar
[34]
M. He, Y. Lee
Application of deep belief network for critical heat flux prediction on microstructure surfaces
Nucl. Technol. (2019), pp. 1-17
View Record in ScopusGoogle Scholar
[35]
Y. Lv, Y. Duan, W. Kang, Z. Li, F.Y. Wang
Traffic flow prediction with big data: a deep learning approach
IEEE Trans. Intell. Transport. Syst., 16 (2) (2014), pp. 865-873
Google Scholar
[36]
G. Litjens, T. Kooi, B.E. Bejnordi, A.A.A. Setio, F. Ciompi, M. Ghafoorian, J.A.W.M. Laak, B. Ginneken, C.I. Sánchez
A survey on deep learning in medical image analysis
Med. Image Anal., 42 (2017), pp. 60-88
ArticleDownload PDFView Record in ScopusGoogle Scholar
[37]
K. Lan, D.T. Wang, S. Fong, L.S. Liu, K.K. Wong, N. Dey
A survey of data mining and deep learning in bioinformatics
J. Med. Syst., 42 (8) (2018), p. 139
View Record in ScopusGoogle Scholar
[38]
A. Kamilaris, F.X. Prenafeta-Boldú
Deep learning in agriculture: a survey
Comput. Electron. Agric., 147 (2018), pp. 70-90
ArticleDownload PDFView Record in ScopusGoogle Scholar
[39]
L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, M. Pietikäinen
Deep learning for generic object detection: a survey
Int. J. Comput. Vis., 128 (2) (2020), pp. 261-318
CrossRefView Record in ScopusGoogle Scholar
[40]
K. Simonyan, A. Zisserman
Very Deep Convolutional Networks for Large-Scale Image Recognition, arXiv Preprint arXiv:1409
(2014), p. 1556
View Record in ScopusGoogle Scholar
[41]
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich
Going deeper with convolutions
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 1-9
CrossRefView Record in ScopusGoogle Scholar
[42]
K. He, X. Zhang, S. Ren, J. Sun
Deep residual learning for image recognition
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016), pp. 770-778
CrossRefView Record in ScopusGoogle Scholar
[43]
J. Hu, L. Shen, G. Sun
Squeeze-and-excitation networks
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018), pp. 7132-7141
CrossRefView Record in ScopusGoogle Scholar
[44]
T. Shimobaba, T. Kakue, T. Ito
Convolutional neural network-based regression for depth prediction in digital holography
IEEE 27th International Symposium on Industrial Electronics (ISIE), Cairns, QLD (2018), pp. 1323-1326
CrossRefView Record in ScopusGoogle Scholar
[45]
J. Du, Y. Xu
Hierarchical deep neural network for multivariate regression
Pattern Recogn., 63 (2017), pp. 149-157
ArticleDownload PDFView Record in ScopusGoogle Scholar
[46]
A. Ameri, M.A. Akhaee, E. Scheme, K. Englehart
Regression convolutional neural network for improved simultaneous EMG control
J. Neural. Eng., 16 (2019), Article 036015
CrossRefView Record in ScopusGoogle Scholar
[47]
F. Khademi, S.M. Jamal, N. Deshpande, S. Londhe
Predicting strength of recycled aggregate concrete using artificial neural network, adaptive neuro-fuzzy inference system and multiple linear regression
International Journal of Sustainable Built Environment, 5 (2016), pp. 355-369
ArticleDownload PDFView Record in ScopusGoogle Scholar
[48]
S. Kim, P.Y. Lu, S. Mukherjee, M. Gilbert, L. Jing, V. Čeperić, M. Soljačić
Integration of neural network-based symbolic regression in deep learning for scientific discovery
IEEE Transactions on Neural Networks and Learning Systems (2020)
Google Scholar
[49]
H. Tayara, K.G. Soo, K.T. Chong
Vehicle detection and counting in high-resolution aerial images using convolutional regression neural network
IEEE Access, 6 (2018), pp. 2220-2230
CrossRefGoogle Scholar
[50]
G. Sateesh Babu, P. Zhao, X.L. Li
Deep convolutional neural network based regression approach for estimation of remaining useful life
Database Systems for Advanced Applications (2016), pp. 214-228
CrossRefGoogle Scholar
[51]
A. Komeilibirjandi, A.H. Raffiee, A. Maleki, M. Alhuyi Nazari, M. Safdari Shadloo
Thermal conductivity prediction of nanofluids containing CuO nanoparticles by using correlation and artificial neural network
J. Therm. Anal. Calorim., 139 (4) (2020), pp. 2679-2689
CrossRefView Record in ScopusGoogle Scholar
[52]
G.E. Hinton
Deep belief networks
Scholarpedia, 4 (5) (2009), p. 5497
Google Scholar
[53]
G.E. Hinton, P. Dayan, B.J. Frey, R.M. Neal
The “wake-sleep” algorithm for unsupervised neural networks
Science, 268 (5214) (1995), pp. 1158-1161
View Record in ScopusGoogle Scholar
[54]
G.E. Hinton
Training products of experts by minimizing contrastive divergence
Neural Comput., 14 (8) (2002), pp. 1771-1800
CrossRefView Record in ScopusGoogle Scholar
[55]
G.E. Hinton, S. Osindero, Y.W. Teh
A fast learning algorithm for deep belief nets
Neural Comput., 18 (7) (2006), pp. 1527-1554
CrossRefView Record in ScopusGoogle Scholar
[56]
Y. Liu, N.T. Dinh, R.C. Smith, X. Sun
Uncertainty quantification of two-phase flow and boiling heat transfer simulations through a data-driven modular Bayesian approach
Int. J. Heat Mass Tran., 138 (2019), pp. 1096-1116
ArticleDownload PDFView Record in ScopusGoogle Scholar
[57]
R. Salakhutdinov, A. Mnih, G.E. Hinton
Restricted Boltzmann machines for collaborative filtering
Proceedings of the 24th International Conference on Machine Learning (2007)
Google Scholar
[58]
R. Kohavi
A study of cross-validation and bootstrap for accuracy estimation and model selection
Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. San Mateo, CA: Morgan Kaufmann, 2 (12) (1995), pp. 1137-1143
View Record in ScopusGoogle Scholar
[59]
S. Arlot, A. Celisse
A survey of cross-validation procedures for model selection
Stat. Surv., 4 (2010), pp. 40-79
CrossRefView Record in ScopusGoogle Scholar
[60]
J. Larsen, L.K. Hansen, C. Svarer, M. Ohlsson
Design and regularization of neural networks: the optimal use of a validation set, in Neural Networks for Signal Processing VI
Proceedings of the 1996 IEEE Signal Processing Society Workshop, IEEE. (1996), pp. 62-71
CrossRefView Record in ScopusGoogle Scholar
[61]
A. Ng
Machine Learning Yearning: Technical Strategy for Ai Engineers in the Era of Deep Learning
(2019), pp. 25-26
https://www.mlyearning.org
View Record in ScopusGoogle Scholar
[62]
T. Hastie, R. Tibshirani, J. Friedman
The Elements of Statistical Learning: Data Mining, Inference, and Prediction
Springer Science & Business Media (2009), pp. 219-260
CrossRefView Record in ScopusGoogle Scholar
[63]
C. Xia, W. Hu, Z. Guo
Natural convective boiling in vertical rectangular narrow channels
Exp. Therm. Fluid Sci., 12 (3) (1996), pp. 313-324
ArticleDownload PDFView Record in ScopusGoogle Scholar
[64]
Y. Kim, K. Suh
One-dimensional critical heat flux concerning surface orientation and gap size effects
Nucl. Eng. Des., 226 (3) (2003), pp. 277-292
ArticleDownload PDFCrossRefView Record in ScopusGoogle Scholar
[65]
F.W. Staub
The void fraction in subcooled boiling-prediction of vapour volumetric fraction
J. Heat Tran., 90 (1968), pp. 151-157
CrossRefView Record in ScopusGoogle Scholar
[66]
F. Tachibana, M. Akiyama, H. Kawamura
Non-hydrodynamic aspects of pool boiling burnout
J. Nucl. Sci. Technol., 4 (3) (1967), pp. 121-130
CrossRefView Record in ScopusGoogle Scholar
[67]
G. Guglielmini, E. Nannei
On the effect of heating wall thickness on pool boiling burnout
Int. J. Heat Mass Tran., 19 (9) (1976), pp. 1073-1075
ArticleDownload PDFView Record in ScopusGoogle Scholar