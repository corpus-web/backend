<title>Improved block-Jacobi parallel algorithm for the SN nodal method with unstructured mesh</title>
<author>1,LiangQiao,2,YouqiZheng,3,HongchunWu,4,YongpingWang,5,XiananDu</author>
<Affiliation>1,School of Nuclear Science and Technology, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China</Affiliation>
<year>2021</year>
<Jounral>Progress in Nuclear Energy</Journal>
<Publishing_house>ELSEVIER</Publishing_house>
<Text_Collector>XiaFan，HEU</Text_Collector>
<DOI>10.1016/j.pnucene.2021.103629</DOI>
<URL>https://www.sciencedirect.com/science/article/pii/S0149197021000019</URL>
Improved block-Jacobi parallel algorithm for the SN nodal method with unstructured mesh
LiangQiao,YouqiZheng,HongchunWu,YongpingWang,XiananDu
School of Nuclear Science and Technology, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China
<Section>Highlights</Section>
Block-Jacobi parallel algorithm was applied in SN nodal transport solver with unstructured mesh.
•
Iteration degradation of Block-Jacobi parallel algorithm was mitigated by predicting the fluxes at interfaces of subdomains.
•
In inner group scattering source iteration, the prediction can be used to reduced the number of iterations.
<Section>Abstract<Section/>
The substantial time required for solving the neutron transport equation can be reduced through parallel calculation. The block-Jacobi algorithm is a common method for parallel neutron transport calculation in an unstructured mesh; however, iteration degradation is an inevitable problem that limits the application of this algorithm by reducing the parallel efficiency. In this study, we applied the block-Jacobi algorithm to the SN nodal method with a triangular-z mesh and proposed an improvement to achieve high parallel efficiency. The interface prediction (IP) method was developed to prevent iteration degradation. This method is based on extrapolating the interface information instead of using the information from the preceding iteration at the interfaces in sub-domains. Meanwhile, the prediction was used recursively to accelerate the self-group scattering source iteration in the entire space; this process is referred to as the inner iteration prediction method (IIP). These two methods effectively prevent iteration degradation and reduce time required for self-group scattering source iteration. A more stable and improved parallel performance was thus achieved.
Keywords:Parallel calculation;SN method;Unstructured mesh;Block-Jacobi parallel algorithm
<Section>1. Introduction</Section>
Neutron transport based on an unstructured mesh plays an important role in modeling an advanced reactor, which is usually designed to be an irregular core with complex assemblies. Parallel calculation is a promising method of reducing the computational cost during reactor design. However, parallel calculation with an unstructured mesh is difficult for the SN transport method with a sweeping calculation scheme as the information needs to be passed from upstream meshes to current meshes while solving equations at each mesh. Significant work has been conducted on parallelizing the sweeping calculation in the SN transport code.
A scheduling algorithm was used to perform the parallel calculation. In this method, a list of mesh priorities should be established before calculation to ensure that the calculation of meshes at the subdomain interfaces and the sending of the messages to other CPUs are started as soon as possible. Thereby, more CPUs can be simultaneously invoked. This is known as an NP-complete scheduling problem, for which the optimal result is rarely obtained. Only an acceptable solution can be obtained under approximations.
In addition, a dynamic sweeping algorithm—referred to as PSD-b (parallel sweep algorithm by directions and buffering) (Colomer et al., 2013)—was proposed as an alternative to other methods reported in the literature. During the sweeping calculation, the sequences of meshes depend on the information arrival, instead of using pre-established sweeping sequences. A list of solvable meshes in all directions were arranged when their upstream mesh information arrived. Meanwhile, a buffer was used to reduce the number of communications.
The block-Jacobi algorithm is another method that has been widely employed in parallel calculations (Yavuz and Larsen 1989) (Rosa et al. 2009) (Rosa et al. 2014) (Odry et al., 2017). Whole meshes were divided into several non-overlapping blocks where interface meshes will use previous iteration values and the inner meshes remain as serial calculation schemes. Therefore, all blocks can achieve parallel calculation. However, the degradation of the iteration scheme decreases the parallel efficiency. To address this defect, the diffusion synthesis acceleration (DSA) method (Warsa et al., 2005) was developed to reduce the number of iterations. However, the DSA method is only suitable for a diffusive problem while transport synthesis acceleration is time-consuming and unstable.
Dynamic sweeping and scheduling algorithms both maintain the same iteration scheme as source iteration iterating in the entire space, using updated fluxes as in the Gauss-Seidel iteration scheme. Therefore, the iteration numbers of these two parallel algorithms are equal to the serial calculation. This contrasts with the block-Jacobi method, which sweeps locally over a sub-domain, changing the iteration scheme into a Jacobi iteration at the sub-domain interfaces via previous iteration fluxes. For communication between processors, the scheduling method and dynamic sweeping method require a large number of sending and receiving operations to be performed during each sweeping calculation. Reducing the communication operations by buffering a set of messages increases the CPU idle time. Therefore, the number of communications and CPU idle time are difficult to reduce simultaneously. The block-Jacobi method requires communication of the interface information after each iteration. This could be more suitable for parallel calculation owing to its inherent concurrence, depending on whether the iteration degradation can be addressed.
In previous studies, we proposed a new nodal SN method with a triangular-z mesh. A three-dimensional neutron transport code—DNTR—was developed (Lu and Wu 2007) and was incorporated in to the SARAX fast reactor code system as the main solver (Zheng et al., 2018a,b) (Zheng et al., 2018a,b). Owing to the large number of triangular-z meshes and energy groups in the fast reactor core calculation, the computational cost was unacceptably large for engineering design applications. Parallel calculation with several CPUs becomes necessary for obtaining an ideal solution.
In this paper, we compared three common methods for parallel SN neutron transport calculation, with an unstructured mesh, and proposed a new method to improve the parallel efficiency. Section 2 presents the block-Jacobi method applied in DNTR, which can effectively overcome the degradation of the original block-Jacobi method. Section 3 presents the numerical results of these parallel algorithms and Section 4 concludes this article.
<Section>2. Theory</Section>
2.1. Nodal SN method with triangular-z mesh
The nodal SN method applies a transverse integration method to derive three 1-D equations in the radial plane and one in the axial plane. Area coordinates were used to transform an arbitrary triangle into a regular triangle, as shown in Fig. 1. Under the transformed coordinate, the neutron transport equation in direction m is expressed as:
(1)@
where μm, ηm, and ξmare cosines of direction m relative to the x-, y- and z-directions, −23≤x≤13, −ys(x)≤y≤ys(x), and ys(x)=x+2/33. The fission source and scattering source are both included in Q.
Fig. 1. Regular triangle.
After integrating Eq. (1) along the y- and z-dimensions, a one-dimensional equation along the x-direction is obtained. Because of the symmetry of the u-, v- and x-directions, equations along the u- or v-directions will have the same forms as those in the x-direction. Together with a one-dimensional equation in the z-direction, four equations will be associated with radial leakage and axial leakage by using the transvers integration method. 2nd order polynomials are used to represent the nodal spatial distribution and 1st order to represent the axial distributions.
Two cases need to be considered when solving the equation in each mesh, as shown in Fig. 2. There are two incoming surfaces and one outgoing surface along the m-direction in triangle 1. The average nodal flux and outgoing surface flux can only be obtained if two incoming surfaces are known. In contrast, triangle 2 has one incoming surface and two outgoing surfaces along the m-direction. These two outgoing surface fluxes can be solved by combining the transverse integrated one-dimensional equation and the binary quadratic polynomial in the nodal distribution. Therefore, all the meshes should be arranged in a sequence where each mesh is solved after its upstream meshes have been solved.
Fig. 2. Two cases of calculations.
The sweeping calculation demands that all meshes must be calculated according to the dependence sequences along all directions. For the example shown in Fig. 3, numbered meshes were arranged in direction m, with two known boundary conditions along the upper and right sides. All meshes were ranked in the sweeping order shown in Fig. 4, under the principle that upstream meshes were calculated first. The calculation starts from the top and proceeds to the bottom of the schedule. Meshes at the same height can be solved in an arbitrary order. After generating sweeping schedules in all directions, the discrete SN nodal transport equations in all meshes can be efficiently solved via the sweeping calculation method.
Fig. 3. Triangle meshes.
Fig. 4. Sweeping order.
The source iteration method is commonly applied to solve the transport equation with a fission source. To illustrate the source iteration, the discrete transport equations in all meshes can be arranged as follows:
(2)Ωm∇Ψgm+Σt,gΨgm=Qs,g→g+Qf,g+∑g′≠gQs,g′−g
where the subscript m is the discrete direction index, g is the index of the energy group, Ωm represents the vector of direction m, Ψgm indicates the vector the average angular fluxes of all meshes in direction m in group g, Σt,gis the vector of the macro total cross sections in group g, Qs,g→g is the vector of self-group scattering neutron sources in group g, and Qf,g and Qs,g′→g are the vectors of the fission neutron source in group g and the scattering neutron source from other groups. An isotropic scattering source was adopted here.
Two levels of source iterations were required to solve the transport equation. The first is the iteration of the outer-group scattering and fission source, which is referred to as the outer iteration. The second is the inner iteration of the self-group scattering source due to the updated angular flux after sweeping calculations in all directions. The inner iteration can be expressed as follows:
(3)AΨgm,n+1=Σs,g−gSΨgm,n+Qout
where A is the streaming plus total interaction operator, equal to Ωm∇+Σt,g, the superscript n indicates the nth iteration, the matrix S is the integration of angular flux in all directions, and Qout indicates the fission and scattering sources from other groups, fixed during the inner iteration. In Eq. (3), if all meshes are swept in dependence order, matrix A becomes a lower triangle matrix as every mesh is only related to its up-stream meshes.
2.2. Parallel schemes with unstructured mesh
The spatial, angular and energy domains for the transport calculation can be decomposed and calculated in parallel. Comparatively, when some energy groups are solved simultaneously, severe iteration degradation will increase the number of iterations. If some directions in the angular domain are distributed into different CPUs, full space communications are performed to obtain the scalar fluxes during self-group scattering source iteration, aggravating the communication burden. Therefore, in this study, only the parallel calculation in the spatial domain is investigated for the distributed parallel computation of the multi-group SN nodal transport method.
Meshes in space can be divided into several non-overlapping domains that are distributed into different CPUs. To balance the computation tasks and lower the communication cost, the number of meshes should be assigned as equal as possible, with a minimized number of cutting edges. Some developed methods in domain decomposition can be found in the METIS software package (Lasalle et al., 2015), which is applied in the SARAX code to obtain domain decomposition of triangular meshes in the radial plane.
An example is shown to illustrate domain decomposition and parallel calculation. Along the dashed lines in Fig. 5, the whole mesh can be divided into two parts. These dashed lines are referred to as interface boundaries in this article. When two CPUs are invoked to calculate these two domains as the sweeping order in Fig. 6, CPU2 must remain idle until CPU1 solves mesh 8 or 6 and passes the outgoing surface angular fluxes to CPU2. The idle time of CPU2 finally decreases the parallel efficiency. To invoke as many CPUs as possible to calculate simultaneously, three common algorithms were developed to explore the parallel potential of sweeping calculations in unstructured meshes.
Fig. 5. Mesh decomposition.
Fig. 6. Sweeping order decomposition.
One method is based on the scheduling algorithm developed by Pautz (2002). A scheduling algorithm was applied to determine a sweeping order with the minimized idle time of all CPUs by overlapping calculation and communication. In Fig. 6, two branches are shown for the meshes calculated by CPU1, i.e., a long chain consisting of 1-2-3-4-5-6 and a short chain consisting of 1-2-7-8. If we ignore the time cost of communication and assume that the time required to solve each mesh is the same, CPU2 will idle for six mesh calculation steps when CPU1 first calculates along the long chain. However, CPU2 will only idle for four steps when the short chain is calculated first by CPU1. A higher priority will be assigned to meshes at an interface boundary located at a higher position in the sweeping order, as well as to their upstream meshes.
Another method abandons a pre-established sweeping order and adopts a dynamic sweeping scheme instead (Colomer et al., 2013). This parallel algorithm will assign a list of solvable meshes by judging the arrival of information concerning incoming surfaces, after a previous list of solvable meshes has finished the calculation. Owing to the independence of sweeping along different directions, the list of solvable meshes can contian meshes in different directions to improve the parallel efficiency.
The sweeping scheduling algorithm and dynamic sweeping algorithm hold the iteration scheme as in serial. Therefore, the CPU idle time and communication between different CPUs inevitably reduces the parallel efficiency. Performing communication after solving each mesh at the interface boundary would cause a large number of communications. Sending and receiving messages after collecting a certain number of meshes would not significantly improve the parallel efficiency as the CPU idle time will increase. In addition to these two methods, another algorithm named the block-Jacobi method accomplishes parallel calculation from another perspective.
The block-Jacobi method can easily accomplish the parallel calculation of Eq. (3). The most suitable iteration method to solve equations set with a lower triangle coefficient matrix, as in Eq. (3), is the Gaussian elimination shown in Eq. (4). D is the diagonal part of A and L is the lower part of A.
(4)Ψgm,n+1=D−1(Qout+Σs,g−gSΨgm,n−LΨgm,n+1)
By dividing the whole matrix into several blocks where the interface boundaries use previous iteration values, the equation set can be written as Eq. (5),
(5)Ψgm,n+1=D−1(Σs,g−gSΨgm,n+Qout+LbΨgm,n+LiΨgm,n+1)
where L = Li + Lb, Lb only contains the part at the interface boundary, and Li contains the part at the inner blocks. During the (n+1)th iteration, the inner meshes of each block are calculated using the (n+1)th results. The iteration scheme at the boundary meshes is degraded to the Jacobi iteration owing to the use of the nth results. Therefore, parallel calculation can be performed for all blocks using the nth results as interface boundary conditions during the (n+1)th iteration. However, the degradation of the iteration reduces the parallel efficiency.
Previously, research has been conducted to study the iteration degradation of the block-Jacobi method via Fourier analysis (Azmy et al. 2015). The results have shown that the convergence is affected by the scattering ratio, cell size and optical thickness, which are related to the spectrum radius of the method. The convergence robustness of this algorithm reduces due to the thin cells and high scattering ratio (Anistratov and Azmy 2015). Therefore, further work is needed to study the iteration degradation of the block-Jacobi parallel algorithm.
2.3. Improvement of the block-Jacobi parallel with interface prediction
If the (n+1)th iteration results can be predicted accurately or even approximately, the parallel efficiency of the block-Jacobi method will increase. To predict the iteration results at the interface boundary, the relation of the nth and (n+1)th iteration results was deduced from Eq. (4). The difference between the nth and (n+1)th iteration results is shown in Eq. (6), consisting of variations in the self-group scattering source and leakage from upstream meshes. The difference in leakage from upstream meshes in two contiguous iterations was assumed to be small so that the iteration difference was mainly changed by the self-group scattering source, as shown in Eq. (7).
(6)Ψgm,n+1−Ψgm,n=D−1(Σs,g−gSΨgm,n−Σs,g−gSΨgm,n−1)−D−1(LΨgm,n+1−LΨgm,n)
(7)Ψgm,n+1−Ψgm,n≈D−1(Σs,g−gSΨgm,n−Σs,g−gSΨgm,n−1)
Because diagonal elements in matrix D are equal to the total cross sections in all meshes, the (n+1)th iteration values can be estimated through Eq. (8) by the nth and (n−1)th source terms.
(8)Ψgm,n+1≈Ψgm,n+Σt,g−1Σs,g−gS(Ψgm,n−Ψgm,n−1)
The Block-Jacobi algorithm utilizes the previous iteration result to prompt all cores to calculate simultaneously, which causes iteration degradation. This degradation can be reduced by employing Eq. (8) to predict the interface values in each block; this is referred to herein as the interface prediction (IP) method. Eq. (8) can also be used recursively in predicting the iteration values to accelerate the self-group scattering source iteration; herein, this method is referred to as inner iteration prediction (IIP) method.
The IP method is easy to apply to block-Jacobi parallel calculations. One only needs to add the correction values at the interface boundaries before starting sub-domain sweeping. The correction values are obtained from the previous and updated scattering sources according to Eq. (8). The pseudo-code of the in-group scattering source block-Jacobi iteration with the IP method is:
Do n = 1, max iteration number
Update in-group scattering source by Ψgm,n,
Wait until non-blocking communication finished at all CPUs,
If (n > 1) then:
Set interface flux by Ψgm,n=Ψgm,n+Σt,g−1Σs,g−gS(Ψgm,n−Ψgm,n−1)
Else:
Set interface flux by Ψgm,n
End if
Solve Ψgm,n+1=D−1(Σs,g−gSΨgm,n+Qout+LbΨgm,n+LiΨgm,n+1),
Non-blocking send and receive interface fluxes to adjoint sub-domains,
Check iteration error, error=||Ψgn+1−Ψgn||2/||Ψgn||2
End do
Non-blocking communications are preferred owing to the calculation overlapping with the communication. For the IIP method, the pseudo-code is:
Do n = 1, max iteration number N
Update in-group scattering source by Ψgm,n,
If (n = = 3) then
Do iip = 1, max IIP number NIIP
Predict all meshes flux by Ψgm,n+iip+Σt,g−1Σs,g−gS(Ψgm,n+iip−Ψgm,n+iip−1)
Update in-group scattering source by Ψgm,n+iip,
End do
Ψgm,n=Ψgm,n+NIIP
End if
Wait until non-blocking communication finished at all CPUs,
If (n > 1 and n/= 3) then:
Set interface flux by Ψgm,n=Ψgm,n+Σt,g−1Σs,g−gS(Ψgm,n−Ψgm,n−1)
Else:
Set interface flux by Ψgm,n
End if
Solve Ψgm,n+1=D−1(Σs,g−gSΨgm,n+Qout+LbΨgm,n+LiΨgm,n+1),
Non-blocking send and receive interface fluxes to adjoint sub-domains,
Check iteration error, error=||Ψgn+1−Ψgn||2/||Ψgn||2
End do
Because the IIP requires relatively steady fluxes between the two iterations, the method can be applied during the 3rd iteration. To avoid repeated predictions by the IIP and IP methods, the IP should be removed when the IIP calculation starts.
<Section>3. Numerical results</Section>
To test the parallel performance, three numerical benchmarks of a sodium-cooled fast reactor—the JOYO64 (Yokoyama, 2006), MET1000, and MOX3600 (OECD/NEA 2015) benchmarks—were calculated. These cores were chosen to consider the differences between the different reactor cores size. Apart from these regular hexagonal lattices assembled cores, a radiation test core with several sub-channels was also calculated with the SARAX code to test the parallel performance in an unstructured mesh.
The spatial decomposition for parallel calculations was performed using the open-source graph partition package METIS (Karypis and Kumar 1998). The computational environment was based on an Ubuntu16.04 LTS system with two Intel Xeon® Gold 6132 2.60 GHz CPUs having 14 cores each. For regular hexagonal cores, the problems were calculated at the S4 order using 33 energy group cross-sections with transport correction. The hexagonal cores were drawn as shown in Fig. 7, where each hexagonal area is divided into six triangular meshes. The radiation test core, with complex geometry shown in Fig. 8, was meshed with unstructured triangular prisms. Cross-sections condensed into seven energy groups with transport correction were provided for the core calculation. The iteration criteria of the outer iterations were 1.0E-5 for the convergence of the fission source and 1.0E-5 for keff simultaneously in the following tests. The inner iteration criteria were either 5.0E-6 for the convergence of flux or three times iterations as the default.
Fig. 7. Triangular-z Mesh (Each hexagonal assembly was divided into six triangle meshes in JOYO64, MET1000 and MOX3600).
Fig. 8. Radiation test core.
3.1. Comparison between different parallel methods
To choose a suitable parallel method for the SN nodal transport solver, the JOYO benchmark was first calculated. The performances of the three parallel methods were compared based on the same 2382 triangle meshes in the radial plane, where the entire space was decomposed into subdomains according to the number of CPUs. Each mesh sent and received messages with all 29 axial layers together to reduce the number of communications. With a fixed mesh size, the parallel efficiencies of the three parallel methods were compared using different numbers of CPUs, as shown in Fig. 9. The parallel efficiency (PE) was calculated as follows:
(9)PE=100×SN=100×TserialN⋅TN,parallel
where Tserial is the real-time cost in serial calculation, TN, parallel is the real-time cost in parallel calculation when using N CPUs, and S is the speed-up ratio. The block-Jacobi method obtained the highest parallel efficiency, ranging from 64 to 102%. When four CPUs were invoked, the cache hit ratio was higher because the memory was split into smaller parts so that the calculations in each CPU became more than four times faster. Therefore, the parallel efficiency can exceed 100%. Second, the performance of the sweeping scheduling method is approximately 5–8% higher than that of the dynamic sweeping method. The mesh size was so limited that the communication and idle time of sweeping scheduling and dynamic sweeping methods increased dramatically with the number of cores. Roughly 41% of the time was wasted in waiting and communicating when using 28 cores. From these comparisons, the block-Jacobi method is more suitable for the nodal transport method with tens of thousands of meshes.
Fig. 9. Parallel efficiency of three methods.
3.2. Effect of interface prediction for block-Jacobi method
The problem faced by the block-Jacobi method is iteration degradation, which reduces parallel efficiency. To compensate for the defect of the parallel iteration scheme, the interface prediction (IP) method was applied to the block-Jacobi method.
Three regular hexagonal cores, JOYO64, MET1000, and MOX3600, and one radiation test Core with unstructured meshes were calculated using different numbers of cores at the same iteration criteria. The number of meshes in the radial plane is 2382, 2274, 4902 and 5210 in the JOYO64, MET1000, MOX3600, and radiation test core, respectively. The parallel efficiencies of the block-Jacobi method are shown in Fig. 10; these are approximately 49–70% when using 28 cores. The IP and IIP methods require more inner iterations to predict the flux and thus the number of inner iterations increased from 3 to 5. In such a condition, the iteration degradation becomes more significant as the self-scattering source fully converges. The highest parallel efficiencies are located in radiation test core, which contains the largest number of meshes. In most cases, higher parallel efficiencies were obtained as the number of meshes increased, except for MET1000. Increased numbers of sweeping calculations were introduced due to iteration degradation, as listed in Table 1, Table 2, Table 3, Table 4. The sweeping calculation increments were 8–20% in JOYO64, 10–38% in MET1000, 3–5% in MOX3600, and 2–7% in the radiation test core; the parallel efficiencies of the block-Jacobi method were very unstable owing to this degradation. The most severe case affected by degradation was the calculation in MET1000, when two cores were used; i.e., a parallel efficiency of only approximately 70%.
Fig. 10. Parallel Efficiency of block-Jacobi.
Table 1. Block-Jacobi Parallel Calculation in JOYO64.
Table 2. Block-Jacobi Parallel Calculation in MET1000.
Table 3. Block-Jacobi Parallel Calculation in MOX3600.
Table 4. Block-Jacobi Parallel calculation in radiation test core.
Therefore, the IP method was applied to overcome the degradation of the block-Jacobi method. Before sweeping started, the fixed fluxes by the IP method at the interface boundaries of all sub-domains were used to predict the next iteration values, sustaining the iteration scheme as a serial calculation. The parallel efficiencies in Fig. 11 demonstrate the effectiveness of the IP method, where parallel performances were always better than those in the block-Jacobi method, especially in JOYO64 and MET1000, which were seriously affected by iteration degradation. The details of parallel performance of the IP method listed in Table 5, Table 6, Table 7, Table 8 show that the increment numbers of sweeping calculations decreased, down to 1–6% in JOYO64, 2–6% in MET1000, 1–2% in MOX3600, and 0–4% in radiation test core. Consequently, the parallel efficiencies of MET1000 increased from 70 to 92% when two cores were used. Simultaneously, the error of keff was reduced from 2.6 to 0.7 pcm.
Fig. 11. Parallel Efficiency of block-Jacobi with IP method.
Table 5. Block-Jacobi Parallel Calculation in JOYO64 with IP method.
Table 6. Block-Jacobi Parallel Calculation in MET1000 with IP method.
Table 7. Block-Jacobi Parallel Calculation in MOX3600 with IP method.
Table 8. Block-Jacobi Parallel Calculation in Radiation Test Core with IP method.
Conversely, the relation of the two iterations can be deduced recursively to predict the inner iteration results in the entire space. The inner iteration prediction method was applied 13 times recursively, together with five inner iterations, starting at the third inner iteration to obtain the previous two scattering sources as 18 inner iterations are sufficient for the calculation. As can be seen from Table 9, the errors between 18 inner iterations and 5 inner iterations along with 13 IIP were less than 1 pcm. Parallel calculation results applied with the IIP method are shown in Table 10, Table 11, Table 12, Table 13, including the speed-up ratios compared with those in the block-Jacobi method. Calculations were accelerated by three times to reduce the number of inner iterations.
Table 9. Error (pcm) of keff after Applying IIP Method in Serial Calculation.
Table 10. Block-Jacobi Parallel Calculation in JOYO64 with IIP Method.
Table 11. Block-Jacobi Parallel Calculation in MET1000 with IIP Method.
Table 12. Block-Jacobi Parallel Calculation in MOX3600 with IIP Method.
Table 13. Block-Jacobi Parallel Calculation in Radiation Test Core with IIP Method.
<Section>4. Conclusion</Section>
Compared with other parallel methods for the nodal SN method with an unstructured mesh, the block-Jacobi method is preferred because of its easy implementation and high parallel efficiency. However, the degradation of the iteration scheme would reduce the parallel efficiency in the nodal SN method. In this paper, we proposed a new method to improve the parallel efficiency of the block-Jacobi method, referred to as the interface predictor (IP) method. According to the relation between two adjacent iteration values, such degradation can be mitigated by predicting the fluxes at the interface boundaries. Furthermore, we extended this relationship to accelerate the self-group scattering source iteration, referred to as the inner iteration predictor (IIP) method.
Several numerical tests were performed to show the effect of the improved block-Jacobi method with interface prediction. The sweeping calculation increments were decreased, especially in JOYO64 and MET1000, down from 8 – 20% to 1–6% in JOYO64 and 10–38% to 2–6% in MET1000. For the 2 cores calculation in MET1000, the parallel efficiency increased from 70 to 92%, proving the effectiveness of the IP method. In addition, all cases benefited from this improvement. Another application is the inner iteration prediction method, which reduces the number of inner iterations and calculation time. By replacing 18 inner iterations with 5 inner iterations and 13 IIP calculation, a speed increase by a factor of approximately 3 was achieved in each case. All results indicated higher parallel efficiencies after using the new method with a triangular-z mesh in the neutron transport solution.
<Section>Credit author statement</Section>
Liang Qiao: Conceptualization, Methodology, Software, Writing – original draft. Youqi Zheng: Supervision, Writing – review & editing. Hongchun Wu: Supervision, Yongping Wang: Supervision, Resources. Xianan Du: Supervision, Resources
<Section>Declaration of competing interest</Section>
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
<Section>Acknowledgements</Section>
This work was partially supported by the National Key R&D Program of China (Grant No. YFB) and the National Natural Science Foundation of China (Grant No. 11775170).
<Section>References</Section>
Anistratov and Azmy, 2015
Dmitriy Y. Anistratov, Yousry Y. Azmy
Iterative stability analysis of spatial domain decomposition based on block Jacobi algorithm for the diamond-difference scheme
J. Comput. Phys., 297 (2015), pp. 462-479, 10.1016/j.jcp.2015.05.033
ArticleDownload PDFView Record in ScopusGoogle Scholar
Azmy et al., 2015
Yousry Y. Azmy, Dmitriy Anistratov, R. Joseph Zerr
numerical and analytical studies of the spectrum of parallel block Jacobi iterations for solving the weighted diamond difference form of the SN equations. Mathematics and computations, supercomputing in nuclear applications and Monte Carlo international conference
M and C+SNA+MC, 1 (2015)
Google Scholar
Colomer et al., 2013
G. Colomer, R. Borrell, F.X. Trias, I. Rodríguez
Parallel algorithms for Sn transport sweeps on unstructured meshes
J. Comput. Phys., 232 (1) (2013), pp. 118-135, 10.1016/j.jcp.2012.07.009
ArticleDownload PDFView Record in ScopusGoogle Scholar
Karypis and Kumar., 1998
George Karypis, Vipin Kumar
A fast and high quality multilevel scheme for partitioning irregular graphs
SIAM J. Sci. Comput., 20 (1) (1998), pp. 359-392, 10.1137/S1064827595287997
View Record in ScopusGoogle Scholar
Lasalle et al., 2015
Dominique Lasalle, Md Mostofa, Patwary Ali, Nadathur Satish, Sundaram Narayanan, Karypis George, Pradeep Dubey
Improving Graph Partitioning for Modern Graphs and Architectures
(2015), 10.1145/2833179.2833188
Google Scholar
Lu and Wu., 2007
Haoliang Lu, Hongchun Wu
A nodal SN transport method for three-dimensional triangular- z geometry
Nucl. Eng. Des., 237 (2007), pp. 830-839, 10.1016/j.nucengdes.2006.10.025
ArticleDownload PDFView Record in ScopusGoogle Scholar
Odry et al., 2017
N. Odry, J.J. Lautard, J.F. Vidal, G. Rimpault
Coarse mesh rebalance acceleration applied to an iterative domain decomposition method on unstructured mesh
Nucl. Sci. Eng., 187 (3) (2017), pp. 240-253, 10.1080/00295639.2017.1320891
CrossRefView Record in ScopusGoogle Scholar
OECD/NEA, 2015
OECD/NEA. 2015. “Benchmark for neutronic analysis of sodium-cooled fast reactor cores with various fuel types and core Sizes.”.
Google Scholar
Pautz, 2002
Shawn D. Pautz
An algorithm for parallel Sn sweeps on unstructured meshes
Nucl. Sci. Eng., 140 (2) (2002), pp. 111-136, 10.13182/NSE02-1
CrossRefView Record in ScopusGoogle Scholar
Rosa et al., 2014
Massimiliano Rosa, James S. Warsa, Michael Perks
A cellwise block-gauss-seidel iterative method for multigroup SN transport on a hybrid parallel computer architecture
Nucl. Sci. Eng., 174 (3) (2014), pp. 209-226, 10.13182/nse12-57
Google Scholar
Rosa et al., 2009
Rosa, Massimiliano, James S. Warsa, and Timothy M. Kelley. 2009. “Fourier Analysis of Cell-Wise Block-Jacobi Splitting in Two-Dimensional Geometry.” American Nuclear Society - International Conference on Mathematics, Computational Methods and Reactor Physics 2009, M and C 2009, 1110–21.
Google Scholar
Warsa et al., 2005
J.S. Warsa, K. Thompson, J. Morel, J. Chang, K. Budge, M. Benzi
Preconditioning a parallel, inexact block-Jacobi splitting of the SN algorithm
Proceedings from the 5LC, 5 (2005), pp. 229-232
View Record in ScopusGoogle Scholar
Yavuz and Larsen, 1989
Musa Yavuz, Edward W. Larsen
Spatial domain decomposition for neutron transport problems
Transport Theor. Stat. Phys., 18 (2) (1989), pp. 205-219, 10.1080/00411458908204321
CrossRefView Record in ScopusGoogle Scholar
Yokoyama, 2006
Yokoyama et al., 2006. “Japan’s Experimental Fast Reactor JOYO MK-I Core: Sodium-Cooled Uranium-Plutonium Mixed Oxide Fueled Fast Core Surrounded by UO2 blanket.” JOYO-LMFR-RSR-001, NEA/NSC/Doc(2006), OECD/NEA (2006).
Google Scholar
Zheng et al., 2018a
Youqi Zheng, Xianan Du, Zhitao Xu, Shencheng Zhou, Yong Liu, Chenghui Wan, Longfei Xu
SARAX: a new code for fast reactor analysis Part I: methods
Nucl. Eng. Des., 340 (October) (2018), pp. 421-430, 10.1016/j.nucengdes.2018.10.008
ArticleDownload PDFView Record in ScopusGoogle Scholar
Zheng et al., 2018b
Youqi Zheng, Liang Qiao, Zi’an Zhai, Xianan Du, Zhitao Xu
SARAX: a new code for fast reactor analysis Part II: verification, validation and uncertainty quantification
Nucl. Eng. Des., 331 (February) (2018), pp. 41-53, 10.1016/j.nucengdes.2018.02.033
ArticleDownload PDFView Record in ScopusGoogle Scholar
View Abstract