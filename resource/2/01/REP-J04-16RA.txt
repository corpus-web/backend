<title>A Movie Recommendation System Based on Differential Privacy Protection</title>
<author>Min Li   ,1 Yingming Zeng   ,2 Yue Guo   ,1 and Yun Guo   1 </author>
<Affiliation>1 College of Cyber Science, Nankai  University, Tianjin 300017, China2Hangtian INC, Beijing 100140, China</Affiliation>
<year>2020</year>
<Jounral> Science and Technology of Nuclear Installation</Journal>
<Publishing_house>Hindawi</Publishing_house>
<Text_Collector>田苗，BFSU</Text_Collector>
<DOI>10.1155/2020/6611463</DOI>
<URL>https://doi.org/10.1155/2020/6611463</URL>
In the past decades, the ever-increasing popularity of the Internet has led to an explosive growth of information, which has consequently led to the emergence of recommendation systems. A series of cloud-based encryption measures have been adopted in the current recommendation systems to protect users’ privacy. However, there are still many other privacy attacks on the local devices. ,erefore, this paper studies the encryption interference of applying a diﬀerential privacy protection scheme on the data in the user’s local devices under the assumption of an untrusted server. A dynamic privacy budget allocation method is proposed based on a localized diﬀerential privacy protection scheme while taking the speciﬁc application scene of movie recommendation into  consideration.  What  is  more,  an  improved  user-based  collaborative  ﬁltering  algorithm,  which  adopts  a  matrix-based similarity calculation method instead of the traditional vector-based method when computing the user similarity, is proposed. Finally, it was proved by experimental results that the diﬀerential privacy-based movie recommendation system  (DP-MRE) proposed in this paper could not only protect the privacy of users but also ensure the accuracy of recommendations.
1. Introduction
With the development of information technology, tons of data are piled up on the Internet and users have many ways to access these data. For the users, what they spend most of their time on is no longer where to get information, but to ﬁnd out what they are really interested in among numerous information. ,erefore, the recommendation system came into being as an inevitable product of this era of big data. However,  a  key  factor  that  usually  inﬂuences  the  perfor- mance of recommendation systems is whether the amount of user data is enough or not and that may lead to a high risk of privacy leakage. In 2013, LG Corporation was charged for illegal collection of user data via smart TVs, which reﬂects the increasing awareness of privacy protection among users. What is more, IoT  devices such as WiFi ﬁngerprint which are frequently used in  our  daily life  are  also facing many kinds of security attacks [1, 2]. However, most of the existing recommendation systems [3–6] are developed based on the assumption  of  trusted  servers.  In  most  commonly  used
collaborative ﬁltering algorithms, a trusted server collects all user  data  and  makes  user  behavior  analysis  to  give  out personalized recommendations.
,e application of diﬀerential privacy protection scheme in recommendation systems was ﬁrst proposed by McSherry et al.  [7, 8]. In their  scheme, the server is responsible for encrypting user data, and random noise is added to each step of  aggregation  in  the  recommendation  system.  In  such privacy protection schemes, only the circumstances that user data were published to a third-party from a trusted server were considered. However, other circumstances, such that when user data are uploaded from the local device to the cloud, attackers may eavesdrop on the transmission channel and  launch  a  Man-in-the-Middle  (MITM)  attack  or  the attackers may directly hack into the  cloud  server  and  get access to  sensitive user  data,  are not taken into consider- ation. ,erefore, we reach our research question that how to apply  diﬀerential  privacy  protection  on  users’  local  data under the basic assumption of an untrusted server. In this paper, existing diﬀerential privacy protection schemes and commonly used recommendation algorithms are reviewed, and  the  application  of localized  diﬀerential  privacy  pro- tection  scheme  in  recommendation  systems  to  solve  the security issue in recommendation algorithms is investigated. ,e  main  contributions  of this  work  are  summarized  as follows:
(i) A  privacy  budget  allocation  scheme  that  can  dy- namically allocate privacy budget is proposed based on  the  localized  diﬀerential  privacy protection.  In this  allocation  scheme,  users’  behaviors  such  as movie watching records are allocated to the nodes in the privacy preﬁx tree with equal probability. After that, Laplace noise is added according to the privacy budget  allocated to  each  node.  ,is  scheme  could avoid  the  extreme  circumstances  of unevenly  dis- tributed  privacy  budget  and  added  noise.  In  the meantime, this allocation scheme could also ensure the   security   of  users’   private   data,   as   well   as guaranteeing  the  accuracy  of recommendation  re- sults  by  recording  the  combinatorial  sequences  of user behavior.
(ii) ,e   traditional   user-based   collaborative   ﬁltering recommendation  algorithm  is  improved by taking the speciﬁc application scene of movie recommen- dation  into  consideration.  During  the  process  of calculating user similarity to ﬁnd out a similar group of the target users, a matrix-based method is pro- posed    to    replace    the    traditional    vector-based method.  More  speciﬁcally,  after  the  privacy preﬁx tree is generated, we construct a user-interest matrix E according to users’ movie watching records and the characteristics of combinatorial sequences, then apply the user-based  collaborative ﬁltering  recom- mendation algorithm with matrix E to calculate the similarity  between  users  and  ﬁnd  out  the  similar group  of the  target  user,  and  ﬁnally  give  out  the recommendation results.
2. Theoretical Basis
2.1.  Diﬀerential Privacy.  In  2006,  Cynthia  Dwork,  Frank McSherry, Kobbi Nissim, and Adam D. Smith introduced the  concept  of diﬀerential  privacy  [8–12],  which  assumes that the attackers are able to access all information except the target information and makes it hard for attackers to access users’ privacy via diﬀerence calculation. For the calculation result of the dataset, whether a single record is in the dataset or  not  has  a  negligible  impact  on  the  result.  ,e  basic deﬁnitions and properties of diﬀerential privacy involved in this paper are as follows.
Assume that datasets D1  and D2  have the same property structure, the symmetric diﬀerence between them is denoted as D1 ΔD2 , and the number of records in D1 ΔD2  is denoted as  |D1 ΔD2 |.  If  |D1 ΔD2 |  1,  we  say  that  D1   and  D2   are adjacent datasets.
Deﬁnition 1  (ε-diﬀerential privacy). Let ε be a positive real number and M be a random algorithm that takes a dataset asinput. Let M(x) denote the result obtained from a query of random  algorithm M and R be  a  subset  of M(x). ,e  al- gorithm M is said to provide ε-diﬀerential privacy if, for all adjacent dataset pairs of D1  and D2  that diﬀer on a single element and all subsets R of M(x), the following equation is satisﬁed:@1
Deﬁnition    2  (global    sensitivity).    For    query    function f: D ⟶ Rd , where D is a dataset and Rd is a d-dimensional vector  of real  numbers  representing  the  query  result,  the global sensitivity off over all adjacent dataset pairs of D1 and D2  is described by@2
Global   sensitivity  describes  the  maximum   range   of changes when a query function is performed on a pair of adjacent datasets. It has nothing to do with the dataset, but it is only determined by the query function itself. ,e global sensitivity of the counting query is  1.
Property 1  (Sequential composition). Assume that there are n independent algorithms M1 , M2 , . . . , MN  whose privacy guarantees are ε 1 , ε2 , . . . , εn , respectively. ,en for the same dataset            D,            the            composite            algorithm M(M1 (D), M2 (D), . . . , Mn(D))   is    (1 εi )-diﬀerentially private.
Deﬁnition    3  (,e    Laplace    mechanism).    ,e    Laplace mechanism adds Laplace noise to the original query outputs to realize ε-diﬀerential privacy. ,e noise is from Laplace distribution Lap (σ) that can be expressed by the following probability  density function with  mean value  0  and  scale parameter:@3
2.2. Privacy Preﬁx Tree.  ,e movie recommendation system based on diﬀerential privacy protection that we proposed in this paper combines users’ movie watching records with the characteristic  structure  of preﬁx  tree  [13]  to  construct  a privacy preﬁx tree (DP-tree), which can be considered as an improved preﬁx tree, and its structure is shown in Figure 1. In Figure 1, Prior is a pointer point to the parent node; Value stores the value; Num is the number of times that this value shows; Depth is the depth of value; Child[i] is an array of pointers that point to the child nodes, and EndNum stores the number of times that the current node is the end of each path. ,e genres of all the movies that a user has watched are recorded and abstracted to a privacy preﬁx tree with a root node  denoted  as  Root,  in  which  each  node  represents  a genre. In the privacy preﬁx tree, each branch is actually a sequence of the combination of diﬀerent tags that represent diﬀerent movie genres,  and  each  sequence  is  started with node Root. ,e identical subsequences are merged and the
Figure 1: Data structure of a privacy preﬁx tree.
number of times that the subsequence shows is accumulated. Finally, the frequency that each genre of movie is watching as  well  as  the  frequency that  each  movie  genre  sequence shows is also recorded.
Based on the construction of the privacy preﬁx tree, the movie  recommendation  system  proposed  in  this  paper decomposes the record of user behavior, allocates privacy budget  dynamically  for  the  privacy  preﬁx  tree,  and  adds Laplace  noise that  satisﬁes  the  Laplace  distribution.  After that, a user-interest matrix E is constructed according to the appearance  frequency  of  diﬀerent  movie  genres  and  the movie genre sequences that we get from the privacy preﬁx tree. Finally, matrix similarity is calculated to ﬁnd out the similar  user  group  of  the  target  user,  and  a  user-based collaborative  ﬁltering  algorithm  is  adopted  to  give  out  a recommendation of movies.
3. Design of the Differential Privacy Protection Scheme
3.1. Principal Steps in Diﬀerential Privacy Protection Scheme.
,ere are two principal steps when designing a diﬀerential privacy protection scheme: ﬁrstly, select appropriate privacy budget parameters and allocate a proper privacy budget for the protected data; secondly, add some noise interference to the protected data.
For the noise addition of the counting query, the Laplace mechanism  is  adopted  to  add  interference  to  the  privacy data, and the size of noise is closely related to the result of privacy budget allocation. More precisely, the privacy budget ε is inversely proportional to the  size  of the added noise. ,erefore, the privacy budget ε not only determines the level of  diﬀerential  privacy  protection  but  also  inﬂuences  the addition  of noise  interference;  that  is  why  ε  is  the  core parameter in diﬀerential privacy protection scheme. In this paper, we will mainly focus on how to allocate the privacy budget appropriately.
For  the  movie  recommendation  system  based  on  dif- ferential  privacy  protection,  ﬁrstly,  a  privacy  preﬁx  tree movie  genre  is  constructed  according  to  users’  watching history.  Movie  genres  and  sequences  that  appear  more frequently in the privacy preﬁx tree are more likely to arouse users’  interest,  and  they  also  have  a  higher  possibility  of being attacked. In order to prevent the privacy budget from being exhausted, we usually allocate more privacy budgets for the data that are commonly used. However, the tradi- tional  privacy budget  allocation  method which  evenly  al- locates the privacy budget to each node or each layer of the privacy preﬁx tree will lead to unreasonable addition of noise
interference.       What       is       more,       limited       privacy budget allocation for commonly used data may lead to quick exhaustion  of the total budget, which will undermine the protection of users’ privacy. ,erefore, the problem of how to allocate the privacy budget reasonably is worth further investigation. In this paper, we proposed a scheme based on preﬁx tree allocation that can allocate the privacy budget ε dynamically and reasonably according to the frequency of data use.
3.2. Preﬁx Tree Privacy Budget Allocation Scheme.  ,e ﬁlm recommendation  system  based  on  diﬀerential  privacy  in- troduced in this paper is based on the tree structure for data protection and encryption. Figure 2 shows the structure of user  information  based  on  the  preﬁx  tree  structure;  the genres are extracted as a movie feature and a privacy preﬁx tree is constructed based on the preﬁx tree structure. Spe- ciﬁcally  speaking,  the  genres  (types)  are  extracted  from users’  watching  records  and  stored  in  sequences  in  the substructure of a tree, where each path represents a certain combination of movie types and then records the showing frequency of each child node as well as the frequency of them appearing as leaf nodes. In order to reasonably allocate the privacy budget, we assign the privacy budget for each node in the privacy preﬁx tree proportionally. In particular, the root node R is abstract and does not represent a real movie type, so it will not consume any privacy budget. All other nodes in subtrees need to be assigned a privacy budget.
Instead of storing the movie type directly in the preﬁx tree, the corresponding letter representation  of the movie type is stored, as shown in Table 1.
Table 2 shows the data stored in each node in the privacy preﬁx tree structure shown in Figure 2.
As  shown in  Figure 3  and Table 2, the ﬁrst path rep- resents that the times (counts) of user watching movies that are tagged with a are  10, b is 5, and the end number is 2,
which  means that  the  user has watched  3  movies  that  are
depicted by sequence <a, b>, and similarly, we can tell that he or she has also watched 2 movies that are depicted by sequence <a, b, c>.
As shown in Figure 3, assuming that the total privacy budget of the tree is ε, start with the ﬁrst level of this tree; the frequencies  of movie  types  a,  d2,  and f  are  10,  6,  and  4, respectively. ,erefore, the total privacy budget allocation proportion of the subtree with node a as its root node should be (10/20)ε; thus, the dashed box shown in Figure 3 should totally    be    assigned    0.5ε    privacy    budget.    Similarly, εb   (0.5 ∗ 0.5 ∗ 0.6)/2ε. When a movie type appears in dif- ferent  sequences, the privacy budget of it equals the total
Figure  2:  User  information  diagram  based  on  the  preﬁx  tree structure.
Table 1: Mapping table of movie genres to tags.
Table 2: DP-tree data structure.
sum of the allocated privacy budget in each sequence. For example,    the    privacy    budget    of   movie    type    d    is εd   εd1  + εd2    0. 125ε + 0. 15ε  0.275ε.       According       to Property  1,  the  sequential  composition  property  of  the diﬀerential privacy protection, it can be concluded that@4
It   can  be   seen   that,   compared   with   other   privacy budget allocation methods [14–18], the method of allocating privacy budget is based  on the value  of each node in the preﬁx tree, instead of just allocating uniformly according to the level structure. ,is allocation method can allocate the privacy budget reasonably and dynamically in the case that big diﬀerences exist among structures of the subtrees, and it also eliminates the requirement of artiﬁcially adjusting the value of privacy budget allocation.
3.3.  Preﬁx  Tree  Privacy  Budget Allocation Algorithm.  ,e privacy budget allocation algorithm based on the preﬁx tree is  shown  as  follows.  TMovie  stores  the  result  of privacy budget allocation of movie type nodes; DP-tree movie type node v and its privacy εv are stored as <v, εv > in the queue set TQueue; Pv is the statistical frequency of the current node v being watched by users; GetTop (LinkQueue Q, string r, andIn the above algorithm, the TMovie and TQueue sets are initialized to be empty after inputting the privacy budget ε, and the preﬁxed preﬁx tree and root node R are constructed. ,en, add the current node and its privacy budget to TQueue (when R  is not the root),  and  compare the weight of the current node with its parent node. If their weights are equal, assign half of the current privacy budget for both of them. Otherwise, compare the current node with its brother nodes and  assign  half of parent  nodes’  privacy  budget  to  them according to their weight ratio. Repeat this process for each child node of the current node.
4. Design of DP-MRE
4.1. Overall Framework of DP-MRE.  Figure 4 is the overall frame diagram of the movie recommendation system based on diﬀerential privacy protection, where the overall system is composed of ﬁve components. Firstly, users’ private data are collected  on  their  local  devices,  and  then  a  preﬁx  tree  is constructed based on the collected data to dynamically al- locate the privacy budget. Next, noise interference that obeys Laplace distribution is added, and then the users’ data after being interfered with as well as public data are used together as the input of recommendation system and ﬁnally give out movie  recommendations.  ,e  detailed  meaning  of  each component in Figure 4 is as follows:
Public data refer to the public information related to users’ private data from internal or external resources. We chose the MovieLens  1M dataset, which contains
100 million ratings from 6,000 users on nearly 4,000 movies. ,is  dataset will be used as  an  experimental dataset and test dataset for experimental veriﬁcation in this paper.
User data refer to the historical data of users’ behavior
collected from their personal devices. In this paper, we used the historical records of movies watched by users, such as the frequency of a user watching a certain type of movie, as well as users’ ratings on these movies. What is more, this part of data is not interfered with.
Privacy quantiﬁcation refers to the process that con- structs the privacy preﬁx tree according to users’ be- havior records and allocates privacy budget according to the appearing times and frequencies of each node in the privacy preﬁx tree that we proposed in this paper.
Data perturbation refers to the process that adds noise which obeys Laplace distribution to each node in the privacy preﬁx tree according to its privacy budget, in order to interfere with the original data to ensure the security  of  users’  private  data  while  preserving  the eﬀectiveness of data. In other words, the interfered data should satisfy two necessary conditions: being secure enough  to  protect  users’  privacy  and  being  eﬀective enough  to  give  out  accurate  recommendation  in the subsequent recommendation stage.
ﬂoat e) represents the dequeue function of header element (Algorithm 1).
Figure 3: Privacy budget allocation scheme based on the preﬁx tree.
ALGORITHM   1: Privacy budget allocation algorithm.
Recommendation refers to the ﬁnal stage of our DP- MRE system design, in which an untrusted third-party server obtains the data after perturbation, that is, after adding Laplace noise, and then uses these data to build a user-interest matrix according to user’s preference on diﬀerent  types  of movie.  Next,  similarity  calculation based on the multidimensional matrix is performed to ﬁnd   out   similar   user   groups,   and   a   user-based
collaborative ﬁltering algorithm is adopted to give out a ﬁnal recommendation for users.
4.2. User-Based Collaborative Filtering Algorithm.  ,e user- based   collaborative   ﬁltering   recommendation   algorithm [19–21] is usually composed of two parts: (1) to calculate the user similarity; (2) to recommend the interested contents of similar user groups to the target user.
Figure 4: Frame diagram of the movie recommendation system based on diﬀerential privacy protection.
,is paper extends the traditional method of computing vector-based similarity to matrix-based similarity and fur- ther combines the watching frequency of movie types as well as the combinatorial sequence of movie types. ,e speciﬁc method is to construct an N ∗ N user-interest matrix E with movie type as both horizontal and vertical quantities. For example,  assume that  a∼ n  represent  movie types  and the user-interest matrix E is constructed as follows:@5
where  the  diagonal  of  the  matrix,  that  is,  the  set  P  {paa , pbb , . . . , pnn }, represents users’ rating scores on movie type a ∼ n; other quantities qmn represent users’ rating score on     certain     movie     type     sequences.     For     example, paa    3, pbb   2, qab   2 indicate that the user has an interest score of 3 for type a movies, 2 for type b movies, and 2 for <a,b> sequence.
As  shown in  Figure 3, the privacy preﬁx tree is  con- structed from user A’s movie watching record. According to the values  of each node in the preﬁx tree  and the  se- quence relationship between movie types in Figure 3, the user-interest matrix of user A can be constructed as follows:@6
After constructing the user-interest matrix, the similarity between users can be obtained via matrix similarity calcu- lation.  In this paper, the  correlation coeﬃcient  is used to evaluate  the  similarity  of  two  matrices.  ,e  correlation coeﬃcient  is  an  indicator  used  to  measure  the  statistical relationship between two variables, and it is a ratio, which can also be regarded as a special form of covariance after the standardization that eliminates the impact of the variation of amplitude. ,e correlation coeﬃcient could be either pos- itive  or negative, which represents the direction  of corre- lation between two variables but does not change the degree of similarity. In other words, the degree of similarity between two variables is reﬂected by the absolute value of the cor- relation coeﬃcient. ,e correlation coeﬃcient used in this paper is calculated as follows:@7
where A  mean(A), B  mean (B), matrix A and B are two matrices with the same size, A and B represent the mean value  matrix  of A  and  B,  respectively,  and  r  denotes  the correlation coeﬃcient which ranges in  [1, +1]. It indicates that matrices A and B share high similarity when the ab- solute value of r is close to  1, and when r is close to 0, it
indicates that matrices A and B are less similar.
,e similarity of the rating scores on movie type (a∼c) between  UserA  and  UserB∼ UserE  is  calculated  using  the method described above in this section, and the results are as shown in Table 3.
According  to  the  calculation  results  based  on  matrix similarity in Table 3, the similarity between UserA and UserE is the highest.  However,  if we  change to  use the  Pearson correlation  coeﬃcient  to  evaluate  the  similarity  between users, although the structure of the user-interest matrix of UserA and UserE shares the highest similarity, the common rating items of UserA and UserC will lead to the calculation result  of the  similarity  between  UserA  and  UserC  being exactly  1,  which  is  not  consistent with  the  real  situation. However, in the matrix-based similarity calculation method we proposed, the similarity between  UserA and  UserE is a little higher than that between UserA and UserD. ,erefore, both the  absolute value  and the  quantity  structure  of the matrices are taken into account in the method we proposed based on matrix similarity.
What is more, if we change to use the Euclidean distance to  evaluate  the  similarity  between  users,  if there  are  no common rating items between two users, the  similarity it gives out would be relatively low even if the structure and value  are  highly  similar  to  each  other.  For  example,  in Table 3, the similarity between UserA and UserB, UserC, and UserE is all relatively low. When calculating the similarity of matrices, we can easily notice that actually UserA and UserE have  high  similarity,  and  their  similarities  to  UserB  and UserC are also higher than the results given by Euclidean distance calculation.
Assuming that the number of users who need person- alized  recommendation  is  u  and the  similar  group  of the target user is K, use S(u,K) to denote the process of selecting items that user u interested in from similar group K, denote the interest rating score of user v to itemj as rvj and similarity between the interest of user u and user v as wuv , and denote the user group who are interested in item j as N(j). ,en, the interest rating score of user u to item j should be given by equation (8):@8
Table 3: User similarity calculated based on matrix similarity.
After calculating p  (u,j), compare the value p  (u,j) be- tween diﬀerent users. If two users have a similar value of p (u, j),  it  indicates  that  they  share  a  similar  interest  in  a particular item. ,en, the recommendation results could be given out by sorting the values from largest to smallest and selecting the highest-ranked items.
4.3. Analysis of Privacy Security.  In this section, the privacy security of the DP-MRE algorithm proposed in this paper is analyzed based on diﬀerential privacy protection. Let D1 and D2  be  the  adjacent  dataset  (i.e.,  d  (D1 ,  D2 )  1),  f(Di ) denotes the category set of users’ private data, C denotes the size of the public movie set, j denotes the users’ private data, and z(j) is the size of the Laplace noise added to movie type j. From the deﬁnition of diﬀerential privacy, we can know that, for   arbitrary  r  (r1 , . . . , rc ) ∈ Range(DP −  MRE),   if  the algorithm DP - MRE satisﬁes@9
or if the algorithm DP-MRE satisﬁes
@10
then  we  can  conclude  that  the  algorithm  DP-MRE satisﬁes the ε-diﬀerential privacy protection.
According  to  the  diﬀerential  privacy  protection  pro- posed  in  this  paper,  the  diﬀerential  privacy  protection  is carried  out  on  users’ local private  devices,  so  the privacy protection  analysis  only  focuses  on  the  steps  of privacy budget  allocation  and  noise  addition,  while  there  is  no privacy leakage problem  in the user  similarity calculation and   recommendation   steps.   ,erefore,   privacy   security analysis can be performed in the privacy budget allocation and noise addition steps as follows:
@11
In the ﬁrst step, according to the sequential composition property of diﬀerence privacy, the noise is  added to each category set independently; thus, the diﬀerence in privacy remains unchanged.  Furthermore, the  second  step can be derived  from  the  added  Laplace  noise  and  triangle  in- equality. ,erefore, we have proved that the DP-MRE al- gorithm satisﬁes Inequality 11.
5. Experimental Results and Analysis
5.1. Privacy Budget Allocation.  ,e key point in the appli- cation  of  diﬀerential  privacy  protection  algorithm  is  to preserve users’ privacy as well as the eﬀectiveness of data in the meantime. On one hand, users’ privacy is ensured by the diﬀerential privacy protection mechanism, which is realized by adding the noise satisfying Laplace distribution to users’ personal data. On the other hand, the eﬀectiveness means the property of data that it can still be analyzed and pro- cessed after being protected by a diﬀerential privacy scheme, and the analysis results should be relatively accurate. At the same time, to allocate the privacy budget reasonably should also be taken  into  consideration when  designing  a  diﬀer- ential privacy protection scheme.
In order to evaluate the eﬀectiveness of the preﬁx tree privacy budget allocation method proposed in this paper, the query error of each node in the tree structure is analyzed, and it is compared with the traditional allocation method which  allocates  the  privacy  budget  uniformly  or  propor- tionally according to arithmetic or geometric series. Mean square error is adopted to evaluate the query error. Assume that   the   accurate   value   of  a   set   of  data   is   given   by (a1 , a2 , . . . , an )  and  the   approximate  value   is  given  by (a, a, . . . , a). ,en, the mean square error (MSE) is given by equation (12).
@12
MovieLens 1M dataset, which contains 6,000 user ratings on nearly 4,000 ﬁlms, was used and we designed a query for the   Movies   dataset   and   repeated   the   query   n   times (n  10, 20, . . . , 1000) to obtain the mean square error value generated by these n queries. In order to get a more accurate
result  and  to  avoid  the  extreme  circumstance  that  the randomness of noise may lead to, the calculation of mean square error is repeated d rounds (d  100); for each round, the  mean  square  error  is  denoted  as  MS  (i  1, 2, . . . , d). ,us, we  could  get the  average  of the  mean  square  error MSE. ,e greater value of MSE reﬂects the larger noise and correspondingly infers a lower accuracy of query results. ,e calculation method of (i  1, 2, . . . , d) and MSE is shown in equation (11) and (12).@13
Denote the query deﬁned on Movies dataset as f, xj  is the result of the j-th query on f, and yj is the corresponding noise result.
As can be seen from Figure 5, under repeated attacks, the errors generated by all privacy budget allocation schemes are increasing. ,e traditional allocation method which evenly allocates privacy budget to each layer generates the largest error, which indicates that this method produces the largest
error in the case of uneven distribution of tree structure. In
the cases when the number of queries is relatively small, the error between privacy budget allocation schemes based on arithmetic  diﬀerence  and  arithmetic  ratio  is  not  much diﬀerent  from  that  based  on  the  preﬁx  tree  structure. However, with the increase of the number of queries, the noise error generated under the privacy budget allocation based on the preﬁx tree is lower than other methods. ,e results indicate that when the number of queries is relatively small, all privacy budget allocation schemes produce rela- tively similar errors, except the scheme that evenly allocates privacy budget based on layers. However, when the number of queries  is  large  enough,  the  privacy  budget  allocation scheme based on the preﬁx tree performs better than all the other schemes.
5.2. Performance of DP-MRE.  In order to reﬂect the impact of diﬀerential privacy on the recommendation quality of the recommendation  system  (DP-MRE)  in  this  paper,  we  use precision and recall to evaluate the performance of the rec- ommendation system. Precision and recall are two indicators that are commonly used to evaluate the eﬃciency and quality of information retrieval systems with chaotic data. Both of these two indicators range from 0 to 1. ,e closer their value is to 1, the higher the quality of the system is, in other words, the higher the accuracy of the results given out by the information retrieval  system  is.  Precision  is  deﬁned  according  to  the prediction results, which indicates how many of the samples whose  predictions  are  positive  are  truly  positive,  whereas recall  is  deﬁned  according  to  our  original  samples,  which indicates how many positive samples are predicted correctly as positive. ,e deﬁnition of precision and recall in a rec- ommendation system is shown as follows:@14
In  order  to  objectively  analyze  the  feasibility  and  ef- fectiveness in the ﬁlm recommendation system of DP-MRE algorithm based on diﬀerential privacy protection proposed in this paper,  it  is  compared with the  S-DPDP  algorithm based on diﬀerential privacy protection proposed by Shen et al. We  set the diﬀerence privacy parameter ε  as an in- dependent  variable,  took  diﬀerent  values  for  the  privacy budget parameter in the experiment, and controlled a single variable to compare multiple recommendation algorithms. In addition, in order to more intuitively reﬂect the impact of privacy  protection  on  the  overall  recommendation  algo- rithm, this paper also added the data recommendation al- gorithm  Baseline  without  privacy  protection  scheme  into comparison.  ,erefore,  two  algorithms  with  diﬀerential privacy  protection  scheme,  S-DPDP  and  DP-MRE  algo- rithm, as well as an algorithm without privacy protection are taken into comparison.
Figure 6  shows the impact of diﬀerential privacy pro- tection  on  the  precision  of the  recommendation  system. From the  experimental  results, we  could  see that,  for the recommendation  system  without  privacy  protection,  the precision  of the  user-based  collaborative  ﬁltering  recom- mendation  system  is  about  0.53,  and  diﬀerential  privacy protection algorithms DP-MRE and S-DPDP indeed cause a certain degree of loss in recommendation precision. When the diﬀerential privacy parameter ε is close to 1, the precision of DP-MRE and S-DPDP algorithm recommended is about 0.51.  With  the  increase  of  the  privacy  parameter  ε,  the precision  of DP-MRE  and  S-DPDP  algorithms  gradually increases  to  that  of  Baseline  algorithm.  Compared  with S-DPDP, DP-MRE has a smaller loss of precision, since DP- MRE allocates the privacy budget according to the DP-tree structure, which maintains the type combination sequence and frequency characteristics of the movies watched by users and distributes the Laplace noise reasonably, therefore re- ducing  the  loss  of recommended  quality  caused by  noise addition. However, S-DPDP adopted an iterative algorithm to  add  noise,  which  blurs  the  similarity  between  users. ,erefore, from the perspective of recommendation quality loss,  DP-MRE  performs  better  than  S-DPDP  algorithm, whereas  DP-MRE  has  a  higher  time  complexity  in  the privacy budget allocation process, which aﬀects the overall system eﬃciency.
Figure 7  shows the impact of diﬀerential privacy pro- tection on the recall rate of the recommendation systems. From the  experimental  results, we  could  see that,  for the recommendation  system  without  privacy  protection,  the user-based  collaborative  ﬁltering  recommendation  system has  a  recall  rate  of around  0.51,  and  diﬀerential  privacy protection algorithms DP-MRE and S-DPDP also cause a certain  degree  of recommendation  quality loss.  However, with the increase of the privacy parameter ε, the recall rate gradually  increases  to  that  of Baseline  algorithm.  In  the
Figure 5: Error MSE caused by repeated query under each privacy budget allocation scheme.
Figure 6: Impact of diﬀerential privacy protection on the precision of recommendation systems.Figure 7: Impact of diﬀerential privacy protection on the recall rate of recommendation systems.
recommendation  results,  higher  precision  and  recall  rate indicate a higher recommendation system. According to the experimental  results,  the  recall  rate  of  DP-MRE  is  very similar to  that  of S-DPDP;  especially when the  dataset  is relatively large, the recall rate of these two recommendation algorithms is basically the same, whereas the recall rate of DP-MRE is slightly higher than that of S-DPDP algorithm.
6. Conclusion
In this paper, we mainly introduced how to apply the dif- ferential  privacy  protection  scheme  in  a  movie  recom- mendation  system  to  protect  users’  privacy  during  the recommendation process, while in the meantime, ensuring the recommendation performance will not suﬀer too much loss. In conclusion, the scheme proposed in this paper ﬁrstly
adds noise to local sensitive data in a dynamic manner to ensure users’ privacy, then sends the data with added noise to the server for similarity calculation, and ﬁnally gives out movie recommendation via user-based collaborative ﬁlter- ing algorithm. ,e experimental results have shown that this scheme could achieve a considerable balance in the trade-oﬀ between  preserving  users’  privacy  and  ensuring  the  per- formance of recommendation system. A meaningful attempt of combining diﬀerential privacy and recommendation al- gorithm has been made in our research. However, there are still a lot of open issues that are worth to be investigated in both ﬁelds of diﬀerential privacy and recommendation al- gorithms [22]. What is more, the application of diﬀerential privacy  in  recommendation  algorithms  other  than  user- based collaborative ﬁltering algorithm will be further studied in our future research.
Data Availability
All data are owned by third parties. ,e dataset used in this paper  is  the  MovieLens   1M  from  https://grouplens.org/ datasets/movielens/1 m/.
Disclosure
An  earlier version  of this  paper was presented  at the  In- ternational  Symposium  on  Security  and  Privacy in  Social Networks and Big Data (Social Sec 2020).
Conflicts of Interest
,e authors declare that they do not have any commercial or associative  interest  that  represents  conﬂicts  of interest  in connection with the work submitted.
Acknowledgments
,is work was supported by the National Natural Science Foundation  of  China  (Grant  no.  61672300)  and  the  In- dustrial Internet Innovation and Development Project of the Ministry of Industry and Information Technology of PRC (Grant no. TC190H3WM).
